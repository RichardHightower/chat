metadata:
  version: 1.0.0
  timestamp: 2025-05-28 08:48:00.999027
  description: YAML snapshot of /Users/richardhightower/src/chat
  generator: yaml-project
  generator_version: 0.1.0
  author:
  tags:
  custom:
config:
  supported_extensions:
    .py: python
    .sh: bash
    .sql: sql
    .toml: toml
  forbidden_dirs:
    - __pycache__
    - node_modules
    - dist
    - cdk.out
    - env
    - venv
    - .venv
    - .idea
    - build
    - .git
    - .svn
    - .hg
    - .DS_Store
    - .vs
    - .vscode
    - target
    - bin
    - obj
    - out
    - Debug
    - Release
    - tmp
    - .tox
    - .pytest_cache
    - __MACOSX
  include_pattern:
  exclude_pattern:
  outfile: /Users/richardhightower/src/chat/project.yaml
  log_level: INFO
  max_file_size: 204800
  config_dir: .yamlproject
  chunking_enabled: false
  chunk_size: 1048576
  temp_dir:
  backup_enabled: false
  backup_dir:
  metadata_fields:
    - extension
    - size_bytes
    - language
  yaml_format:
    indent: 2
    width: 120
tests:
content:
  files:
    pyproject.toml:
      content: |
        [project]
        name = "chat-app"
        version = "0.1.0"
        description = "A Streamlit chat app using LiteLLM and OpenAI"
        authors = [
            {name = "Rick Hightower",email = "richardhightower@gmail.com"}
        ]
        readme = "docs/README.md"
        requires-python = ">=3.10,<3.13"
        dependencies = [
            "streamlit (>=1.45.1,<2.0.0)",
            "litellm (>=1.70.0,<2.0.0)",
            "python-dotenv (>=1.0.1,<2.0.0)",
            "requests (>=2.31.0,<3.0.0)",
            "boto3 (>=1.30.0,<2.0.0)",
            "vector-rag @ file:///Users/richardhightower/src/vector-rag",
            "sqlalchemy (>=2.0.36,<3.0.0)",
            "pgvector (>=0.3.6,<0.4.0)",
            "psycopg2-binary (>=2.9.10,<3.0.0)",
            "sentence-transformers (>=2.2.2,<3.0.0)",
            "torch (>=2.2.0,<3.0.0)"
        ]

        [[tool.poetry.packages]]
        include = "chat"
        from = "src"

        [build-system]
        requires = ["poetry-core>=2.0.0,<3.0.0"]
        build-backend = "poetry.core.masonry.api"
      metadata:
        extension: .toml
        size_bytes: 844
        language: toml
    test/test_search_ui.py:
      content: |-
        """
        Test script for the new search UI functionality.
        Tests BM25, semantic, and hybrid search methods.
        """

        import sys
        import os
        sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

        from chat.rag.rag_service import RAGService
        from vector_rag.model import ChunkResults


        def test_search_methods():
            """Test all search methods with the RAG service."""
            
            # Initialize RAG service
            rag_service = RAGService()
            
            # Create or get a test project
            project = rag_service.get_or_create_project(
                name="Test Search UI",
                description="Testing BM25 and hybrid search"
            )
            
            print(f"Using project: {project.name} (ID: {project.id})")
            
            # Add some test documents if needed
            test_docs = [
                {
                    "filename": "postgres_bm25.md",
                    "content": """
                    PostgreSQL BM25 Implementation Guide
                    
                    BM25 (Best Matching 25) is implemented in PostgreSQL using the full-text search
                    capabilities. The tsvector column stores preprocessed searchable text, while
                    ts_rank_cd() provides ranking similar to BM25.
                    
                    Key components:
                    - tsvector: Stores tokenized and normalized text
                    - tsquery: Represents search queries
                    - GIN index: Accelerates full-text searches
                    - ts_rank_cd(): Cover density ranking function
                    """
                },
                {
                    "filename": "vector_search.md", 
                    "content": """
                    Vector Search and Semantic Similarity
                    
                    Vector search uses embeddings to find semantically similar content. Unlike
                    keyword search, it understands context and meaning. The process involves:
                    
                    1. Convert text to embeddings using models like sentence-transformers
                    2. Store embeddings in a vector database
                    3. Use cosine similarity to find related content
                    4. Return results ranked by similarity score
                    """
                },
                {
                    "filename": "hybrid_search.md",
                    "content": """
                    Hybrid Search: Combining Vector and BM25
                    
                    Hybrid search combines the strengths of both semantic search and keyword
                    matching. It's ideal for technical documentation where exact terms matter
                    but context is also important.
                    
                    Benefits:
                    - Captures exact technical terms with BM25
                    - Understands semantic meaning with vectors
                    - Configurable weights for different use cases
                    - Better recall and precision than either method alone
                    """
                }
            ]
            
            # Add documents if they don't exist
            existing_files = {f.name for f in rag_service.list_files(project.id)}
            
            for doc in test_docs:
                if doc["filename"] not in existing_files:
                    print(f"Adding document: {doc['filename']}")
                    rag_service.add_document(
                        project_id=project.id,
                        filename=doc["filename"],
                        content=doc["content"],
                        metadata={"type": "documentation"}
                    )
            
            # Test different search methods
            test_queries = [
                ("PostgreSQL tsvector implementation", "Technical keyword search"),
                ("how to find similar documents", "Semantic understanding query"),
                ("BM25 ranking algorithm", "Mixed technical and conceptual")
            ]
            
            print("\n" + "="*60)
            
            for query, description in test_queries:
                print(f"\nQuery: '{query}' ({description})")
                print("-" * 60)
                
                # Test semantic search
                print("\n1. Semantic Search Results:")
                semantic_results = rag_service.api.search_text(
                    project_id=project.id,
                    query_text=query,
                    page_size=3,
                    similarity_threshold=0.5
                )
                
                if semantic_results and semantic_results.results:
                    for i, result in enumerate(semantic_results.results):
                        print(f"   [{i+1}] Score: {result.score:.3f} - {result.chunk.file_name}")
                        print(f"       Preview: {result.chunk.content[:100]}...")
                else:
                    print("   No results found")
                
                # Test BM25 search
                print("\n2. BM25 (Keyword) Search Results:")
                bm25_results = rag_service.search_bm25(
                    project_id=project.id,
                    query=query,
                    top_k=3,
                    rank_threshold=0.0
                )
                
                if bm25_results and bm25_results.results:
                    for i, result in enumerate(bm25_results.results):
                        print(f"   [{i+1}] Rank: {result.score:.3f} - {result.chunk.file_name}")
                        print(f"       Preview: {result.chunk.content[:100]}...")
                else:
                    print("   No results found")
                
                # Test hybrid search
                print("\n3. Hybrid Search Results (50/50 weights):")
                hybrid_results = rag_service.search_hybrid(
                    project_id=project.id,
                    query=query,
                    top_k=3,
                    vector_weight=0.5,
                    bm25_weight=0.5
                )
                
                if hybrid_results and hybrid_results.results:
                    for i, result in enumerate(hybrid_results.results):
                        print(f"   [{i+1}] Score: {result.score:.3f} - {result.chunk.file_name}")
                        # Check for individual scores in metadata
                        if "_scores" in result.chunk.metadata:
                            scores = result.chunk.metadata["_scores"]
                            print(f"       (Vector: {scores.get('vector', 0):.3f}, BM25: {scores.get('bm25', 0):.3f})")
                        print(f"       Preview: {result.chunk.content[:100]}...")
                else:
                    print("   No results found")
            
            print("\n" + "="*60)
            print("\nSearch method testing complete!")
            
            # Test the UI integration
            print("\n\nTo test the UI:")
            print("1. Run: poetry run streamlit run src/chat/app.py")
            print("2. Go to the 'RAG' tab and select the 'Test Search UI' project")
            print("3. Go to the 'Search' tab")
            print("4. Toggle 'Show search in main area'")
            print("5. Try different search methods and queries")
            print("\nFeatures to test:")
            print("- Semantic search with similarity threshold")
            print("- BM25 keyword search")
            print("- Hybrid search with adjustable weights")
            print("- Metadata tree view in expandable sections")
            print("- 'Send to Chat' functionality")
            print("- Tabular results view with sortable columns")


        if __name__ == "__main__":
            test_search_methods()
      metadata:
        extension: .py
        size_bytes: 6609
        language: python
    test/test_all_providers_streaming.py:
      content: |-
        """
        Test script to verify streaming functionality for all providers.
        """

        import os
        import asyncio
        from dotenv import load_dotenv
        import logging

        # Use the application's logger
        from src.chat.util.logging_util import logger

        # Import provider classes
        from src.chat.ai.anthropic import AnthropicProvider
        from src.chat.ai.google_gemini import GoogleGeminiProvider
        from src.chat.ai.open_ai import OpenAIProvider
        from src.chat.ai.perplexity import PerplexityProvider
        from src.chat.ai.ollama import OllamaProvider

        # Load environment variables
        load_dotenv()

        async def test_provider_streaming(provider_class, model_name, prompt):
            """Test streaming with a specific provider."""
            print(f"\n\n{'=' * 50}")
            print(f"Testing {provider_class.__name__} with model {model_name}")
            print(f"{'=' * 50}")
            
            try:
                # Initialize provider
                provider = provider_class(model=model_name)
                print(f"Provider initialized: {provider.__class__.__name__}")
                
                print(f"\nPrompt: '{prompt}'\n")
                print("Response:")
                
                # Define callback to print chunks
                def print_chunk(text):
                    print(text, end="", flush=True)
                
                # Get the streaming generator
                generator = provider.generate_completion_stream(
                    prompt=prompt,
                    callback=print_chunk
                )
                
                # Manually iterate over the generator
                full_text = ""
                while True:
                    try:
                        chunk = await anext(generator)
                        full_text += chunk
                    except StopAsyncIteration:
                        break
                
                print("\n\n" + "-" * 40)
                print(f"Streaming completed. Total length: {len(full_text)} characters")
                
            except Exception as e:
                print(f"\n\nError testing {provider_class.__name__}: {str(e)}")

        async def main():
            """Run streaming tests for all providers."""
            # Test prompt
            prompt = "Write a short poem about streaming data, one line at a time."
            
            # Test each provider
            await test_provider_streaming(AnthropicProvider, "claude-3-7-sonnet-latest", prompt)
            await test_provider_streaming(GoogleGeminiProvider, "gemini-2-flash", prompt)
            await test_provider_streaming(OpenAIProvider, "gpt-4o-2024-08-06", prompt)
            await test_provider_streaming(PerplexityProvider, "sonar-pro", prompt)
            
            # For Ollama, we need to ensure the server is running and a model is available
            # Uncomment this line if Ollama is set up
            # await test_provider_streaming(OllamaProvider, "llama3.3:latest", prompt)

        if __name__ == "__main__":
            asyncio.run(main())
      metadata:
        extension: .py
        size_bytes: 2614
        language: python
    test/test_stream_openai.py:
      content: |-
        """
        Test specifically for OpenAI streaming using our utility functions.
        """

        import os
        import asyncio
        from dotenv import load_dotenv
        import litellm
        from src.chat.util.streaming_util import stream_response
        from src.chat.ai.open_ai import OpenAIProvider
        import logging

        # Set up logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

        # Load environment variables
        load_dotenv()

        async def test_openai_streaming():
            """Test streaming with OpenAI provider."""
            # Ensure API key is set
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                print("Error: OPENAI_API_KEY not found in environment variables")
                return
            
            os.environ["OPENAI_API_KEY"] = api_key
            
            # Initialize provider
            provider = OpenAIProvider(api_key=api_key, model="gpt-4o-2024-08-06")
            
            # Test prompt
            prompt = "Write a short poem about code streaming, with each line building on the previous one."
            
            print(f"\nTest prompt: '{prompt}'\n")
            print("Response:")
            
            # Define callback for printing chunks
            def print_chunk(chunk):
                print(chunk, end="", flush=True)
            
            # Run streaming test directly with provider
            full_text = ""
            try:
                # Get the generator
                generator = provider.generate_completion_stream(
                    prompt=prompt,
                    output_format="text",
                    callback=print_chunk
                )
                
                # Manually iterate over the generator
                while True:
                    try:
                        chunk = await anext(generator)
                        full_text += chunk
                    except StopAsyncIteration:
                        break
                
                print("\n\n" + "-" * 40)
                print(f"Streaming completed. Total length: {len(full_text)} characters")
                
            except Exception as e:
                print(f"\n\nError during OpenAI streaming: {str(e)}")

        if __name__ == "__main__":
            asyncio.run(test_openai_streaming())
      metadata:
        extension: .py
        size_bytes: 1925
        language: python
    test/test_stream_openai_json.py:
      content: |-
        """
        Test for OpenAI streaming with JSON output format.
        """

        import os
        import asyncio
        from dotenv import load_dotenv
        import litellm
        from src.chat.util.streaming_util import stream_response
        from src.chat.ai.open_ai import OpenAIProvider
        import logging
        import json

        # Set up logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

        # Load environment variables
        load_dotenv()

        async def test_openai_json_streaming():
            """Test streaming with OpenAI provider using JSON output format."""
            # Ensure API key is set
            api_key = os.getenv("OPENAI_API_KEY")
            if not api_key:
                print("Error: OPENAI_API_KEY not found in environment variables")
                return
            
            os.environ["OPENAI_API_KEY"] = api_key
            
            # Initialize provider
            provider = OpenAIProvider(api_key=api_key, model="gpt-4o-2024-08-06")
            
            # Test prompt with JSON request
            prompt = "Generate a list of 3 programming languages with the following properties: name, year_created, and primary_use_case. Format the output as JSON."
            
            print(f"\nTest prompt: '{prompt}'\n")
            print("Response:")
            
            # Define callback for printing chunks
            def print_chunk(chunk):
                print(chunk, end="", flush=True)
            
            # Run streaming test with JSON format
            full_text = ""
            try:
                # Get the generator
                generator = provider.generate_completion_stream(
                    prompt=prompt,
                    output_format="json_object",  # Request JSON format
                    callback=print_chunk
                )
                
                # Manually iterate over the generator
                while True:
                    try:
                        chunk = await anext(generator)
                        full_text += chunk
                    except StopAsyncIteration:
                        break
                
                print("\n\n" + "-" * 40)
                print(f"Streaming completed. Total length: {len(full_text)} characters")
                
                # Validate JSON
                try:
                    json_result = json.loads(full_text)
                    print("\nJSON parsed successfully:")
                    print(json.dumps(json_result, indent=2))
                except json.JSONDecodeError as e:
                    print(f"\nFailed to parse as JSON: {e}")
                
            except Exception as e:
                print(f"\n\nError during OpenAI JSON streaming: {str(e)}")

        if __name__ == "__main__":
            asyncio.run(test_openai_json_streaming())
      metadata:
        extension: .py
        size_bytes: 2350
        language: python
    test/test_streaming.py:
      content: |-
        """
        Test script for streaming functionality with Anthropic Claude.

        This script tests the streaming functionality independently from the main application.
        """

        import os
        import asyncio
        import src.chat.ai.anthropic
        from dotenv import load_dotenv

        # Load environment variables from .env file
        load_dotenv()

        async def test_streaming():
            """Test the streaming functionality with Claude API."""
            # Get API key from environment
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                print("Error: ANTHROPIC_API_KEY not found in environment variables.")
                return

            # Initialize the Anthropic client
            client = anthropic.Anthropic(api_key=api_key)
            
            # Test prompt
            prompt = "Write a short poem about streaming data, one line at a time."
            
            print(f"Sending prompt to Claude: {prompt}")
            print("\nResponse:")
            
            # Set up streaming
            full_response = ""
            try:
                with client.messages.stream(
                    messages=[{"role": "user", "content": prompt}],
                    model="claude-3-5-sonnet-20240620",
                    max_tokens=1000,
                    temperature=0.7,
                    system="You are a helpful assistant."
                ) as stream:
                    # Process each chunk as it arrives
                    for chunk in stream:
                        if chunk.type == "content_block_delta" and chunk.delta.type == "text":
                            # Extract text from the chunk
                            text_chunk = chunk.delta.text
                            if text_chunk:
                                full_response += text_chunk
                                # Print each chunk with a cursor
                                print(text_chunk, end="", flush=True)
                                
                    # Print a newline at the end
                    print("\n")
                    print("-" * 50)
                    print(f"Complete response ({len(full_response)} characters):")
                    print(full_response)
                    
            except Exception as e:
                print(f"\n\nError during streaming: {str(e)}")

        if __name__ == "__main__":
            asyncio.run(test_streaming())
      metadata:
        extension: .py
        size_bytes: 2042
        language: python
    test/test_stream_anthropic.py:
      content: |-
        """
        Simplified test for Anthropic streaming using our utility functions.
        """

        import os
        import asyncio
        from dotenv import load_dotenv
        import litellm
        from src.chat.util.streaming_util import stream_response
        import logging

        # Set up logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)

        # Load environment variables
        load_dotenv()

        async def simple_stream_test():
            """Test streaming with our utility function."""
            # Ensure API key is set
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                print("Error: ANTHROPIC_API_KEY not found in environment variables")
                return
            
            os.environ["ANTHROPIC_API_KEY"] = api_key
            
            # Set up the client
            client = litellm
            
            # Test messages
            messages = [
                {"role": "user", "content": "Write a short poem about data streaming, line by line."}
            ]
            
            # Stream options
            stream_options = {
                "model": "anthropic/claude-3-7-sonnet-latest",
                "max_tokens": 1000,
                "temperature": 0.7,
                "system": "You are a helpful assistant."
            }
            
            print(f"Testing streaming with model: {stream_options['model']}")
            
            # Define callback for printing chunks
            def print_chunk(chunk):
                print(chunk, end="", flush=True)
            
            # Run streaming test
            full_text = ""
            try:
                async for chunk in stream_response(
                    client=client,
                    messages=messages,
                    stream_options=stream_options,
                    callback=print_chunk
                ):
                    full_text += chunk
                
                print("\n\n" + "-" * 40)
                print(f"Streaming completed. Total length: {len(full_text)} characters")
                
            except Exception as e:
                print(f"\n\nError occurred: {str(e)}")

        if __name__ == "__main__":
            asyncio.run(simple_stream_test())
      metadata:
        extension: .py
        size_bytes: 1828
        language: python
    test/test_litellm_streaming.py:
      content: |-
        """
        Test script for streaming functionality with LiteLLM.

        This script tests the streaming functionality independently from the main application.
        """

        import os
        import asyncio
        import litellm
        from dotenv import load_dotenv
        import logging

        # Configure logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        logger = logging.getLogger(__name__)

        # Load environment variables from .env file
        load_dotenv()

        async def test_streaming():
            """Test the streaming functionality with LiteLLM."""
            # Get API key from environment
            api_key = os.getenv("ANTHROPIC_API_KEY")
            if not api_key:
                print("Error: ANTHROPIC_API_KEY not found in environment variables.")
                return

            # Set up the environment variable for LiteLLM
            os.environ["ANTHROPIC_API_KEY"] = api_key
            
            print("Testing streaming functionality with LiteLLM")
            try:
                ver = getattr(litellm, "__version__", "unknown")
                print(f"LiteLLM version: {ver}")
            except:
                print("Could not determine LiteLLM version")
            
            # Test prompt
            prompt = "Write a short poem about streaming data, one line at a time."
            
            # Set up messages
            messages = [
                {"role": "user", "content": prompt}
            ]
            
            # Model to use
            model = "anthropic/claude-3-7-sonnet-latest"
            
            print(f"Sending prompt to {model} via LiteLLM: {prompt}")
            print("\nResponse:")
            
            # Set up streaming
            full_response = ""
            try:
                response = await litellm.acompletion(
                    model=model,
                    messages=messages,
                    max_tokens=1000,
                    temperature=0.7,
                    stream=True
                )
                
                async for chunk in response:
                    if hasattr(chunk, 'choices') and chunk.choices:
                        delta = chunk.choices[0].delta
                        
                        # Extract content from delta
                        if hasattr(delta, 'content') and delta.content:
                            text_chunk = delta.content
                            full_response += text_chunk
                            
                            # Print each chunk
                            print(text_chunk, end="", flush=True)
                
                # Print a newline at the end
                print("\n")
                print("-" * 50)
                print(f"Complete response ({len(full_response)} characters):")
                print(full_response)
                    
            except Exception as e:
                print(f"\n\nError during streaming: {str(e)}")

        if __name__ == "__main__":
            asyncio.run(test_streaming())
      metadata:
        extension: .py
        size_bytes: 2518
        language: python
    test/test_bedrock.py:
      content: |-
        """
        Test script for AWS Bedrock LLM provider.
        """

        import os
        import asyncio
        from dotenv import load_dotenv
        import logging

        # Configure logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        logger = logging.getLogger(__name__)

        # Import the AWS Bedrock provider
        from src.chat.ai.bedrock import BedrockProvider

        # Load environment variables from .env file
        load_dotenv()

        async def test_bedrock_provider():
            """Test the AWS Bedrock provider with Claude model."""
            # Check if AWS credentials are set
            aws_access_key = os.getenv("AWS_ACCESS_KEY_ID")
            aws_secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")
            aws_session_token = os.getenv("AWS_SESSION_TOKEN")
            
            if not aws_access_key or not aws_secret_key:
                print("Error: AWS credentials not found in environment variables.")
                print("Please set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in your .env file.")
                return
                
            # Log if using session token
            if aws_session_token:
                print("Using temporary credentials with AWS session token")
            else:
                print("Using long-term credentials (no session token provided)")
            
            # Default model - Claude 3 Sonnet
            model = "anthropic.claude-3-sonnet-20240229-v1:0"
            
            try:
                # Initialize the provider
                provider = BedrockProvider(model=model)
                print(f"Provider initialized with model: {model}")
                
                # Test prompt
                prompt = "Write a short paragraph explaining what AWS Bedrock is and its benefits."
                print(f"\nSending prompt: '{prompt}'\n")
                
                # Get completion
                response = await provider.generate_completion(prompt=prompt)
                print("Response:")
                print("-" * 50)
                print(response)
                print("-" * 50)
                
                # Test streaming (optional)
                print("\nTesting streaming response with the same prompt...")
                
                # Define callback to print chunks
                def print_chunk(text):
                    print(text, end="", flush=True)
                
                # Get streaming generator
                print("\nStreaming response:")
                print("-" * 50)
                
                generator = provider.generate_completion_stream(
                    prompt=prompt,
                    callback=print_chunk
                )
                
                # Manually iterate over the generator
                full_text = ""
                while True:
                    try:
                        chunk = await anext(generator)
                        full_text += chunk
                    except StopAsyncIteration:
                        break
                
                print("\n" + "-" * 50)
                print(f"Streaming completed. Total length: {len(full_text)} characters")
                
            except Exception as e:
                print(f"\n\nError testing AWS Bedrock provider: {str(e)}")

        if __name__ == "__main__":
            asyncio.run(test_bedrock_provider())
      metadata:
        extension: .py
        size_bytes: 2836
        language: python
    test/test_bedrock_access.py:
      content: |-
        """
        Test script to check AWS Bedrock model access.
        """

        import os
        import boto3
        import json
        from dotenv import load_dotenv
        import logging
        from botocore.exceptions import ClientError

        # Configure logging
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        logger = logging.getLogger(__name__)

        # Load environment variables from .env file
        load_dotenv()

        def check_bedrock_access():
            """Check which AWS Bedrock models are accessible with current credentials."""
            # Check if AWS credentials are set
            aws_access_key = os.getenv("AWS_ACCESS_KEY_ID")
            aws_secret_key = os.getenv("AWS_SECRET_ACCESS_KEY")
            aws_session_token = os.getenv("AWS_SESSION_TOKEN")
            aws_region = os.getenv("AWS_REGION", "us-east-1")
            
            if not aws_access_key or not aws_secret_key:
                print("Error: AWS credentials not found in environment variables.")
                print("Please set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in your .env file.")
                return
            
            # Log credential information
            print(f"AWS Region: {aws_region}")
            print(f"Using Access Key ID: {aws_access_key[:4]}...{aws_access_key[-4:] if len(aws_access_key) > 8 else ''}")
            if aws_session_token:
                print("Using temporary credentials with session token")
            
            try:
                # Create boto3 clients for Bedrock services
                bedrock_client_kwargs = {
                    "region_name": aws_region,
                    "aws_access_key_id": aws_access_key,
                    "aws_secret_access_key": aws_secret_key
                }
                
                # Add session token if available
                if aws_session_token:
                    bedrock_client_kwargs["aws_session_token"] = aws_session_token
                
                # Create clients for both the management API and runtime API
                bedrock_mgmt = boto3.client("bedrock", **bedrock_client_kwargs)
                bedrock_runtime = boto3.client("bedrock-runtime", **bedrock_client_kwargs)
                
                print("\n=== Testing AWS Bedrock IAM Permissions ===")
                
                # Test management API permissions
                try:
                    print("\nTesting ListFoundationModels permission:")
                    response = bedrock_mgmt.list_foundation_models()
                    print("‚úÖ Success - You have permission to list foundation models")
                except ClientError as e:
                    print(f"‚ùå Failed - {e.response['Error']['Message']}")
                    print("You may need 'bedrock:ListFoundationModels' permission in your IAM policy")
                
                # List available model IDs
                try:
                    print("\n=== Available Bedrock Models ===")
                    response = bedrock_mgmt.list_foundation_models()
                    models = response.get("modelSummaries", [])
                    
                    if not models:
                        print("No models found. You need to request access to models in the AWS Bedrock console.")
                        return
                    
                    print(f"Found {len(models)} accessible models")
                    
                    # Group models by provider
                    providers = {}
                    for model in models:
                        model_id = model["modelId"]
                        provider = model_id.split(".")[0] if "." in model_id else "other"
                        if provider not in providers:
                            providers[provider] = []
                        providers[provider].append(model)
                    
                    # Print models by provider
                    for provider, provider_models in providers.items():
                        print(f"\n{provider.upper()} ({len(provider_models)} models):")
                        for model in provider_models:
                            model_id = model["modelId"]
                            
                            # Check if the model has throughputType information available
                            # Note: Not all models return this information
                            model_throughput = model.get("inferenceTypesSupported", [])
                            if model_throughput:
                                supports_on_demand = "ON_DEMAND" in model_throughput
                                throughput_info = "‚úÖ On-demand supported" if supports_on_demand else "‚ö†Ô∏è Requires provisioned throughput"
                                print(f"  - {model_id} ({throughput_info})")
                            else:
                                # If no throughput info, just print the model ID
                                print(f"  - {model_id}")
                            
                    # Test access to a specific model
                    print("\n=== Testing Model Access ===")
                    
                    # Try to access a Claude model if available
                    claude_models = [m for m in models if "anthropic.claude" in m["modelId"]]
                    if claude_models:
                        test_model = claude_models[0]["modelId"]
                        print(f"Testing access to {test_model}...")
                        
                        try:
                            # For Claude models, use the appropriate API based on the model version
                            if "claude-3" in test_model.lower():
                                # Claude 3 models use the converse API
                                request_id = "test-" + os.urandom(4).hex()
                                response = bedrock_runtime.converse(
                                    modelId=test_model,
                                    messages=[
                                        {"role": "user", "content": "Hello, can you say hi?"}
                                    ],
                                    conversationId=request_id
                                )
                            else:
                                # Older Claude models use InvokeModel
                                prompt = "Human: Hello, can you say hi?\nAssistant:"
                                body = json.dumps({"prompt": prompt, "max_tokens_to_sample": 50})
                                response = bedrock_runtime.invoke_model(
                                    modelId=test_model,
                                    contentType="application/json",
                                    body=body
                                )
                            
                            print(f"‚úÖ Success! Model {test_model} is accessible")
                            
                        except ClientError as e:
                            error_msg = e.response.get('Error', {}).get('Message', str(e))
                            print(f"‚ùå Failed - {error_msg}")
                            print(f"You don't have access to use {test_model}")
                            print("Check the 'Model access' section in the AWS Bedrock console")
                    else:
                        print("No Claude models found to test")
                        
                    # Try a different provider if Claude isn't available
                    if not claude_models:
                        if len(models) > 0:
                            test_model = models[0]["modelId"]
                            print(f"Testing access to {test_model}...")
                            try:
                                # Try generic InvokeModel API
                                # Note: different models have different input formats, so this may fail
                                body = json.dumps({"prompt": "Hello", "max_tokens": 50})
                                response = bedrock_runtime.invoke_model(
                                    modelId=test_model,
                                    contentType="application/json",
                                    body=body
                                )
                                print(f"‚úÖ Success! Model {test_model} is accessible")
                            except ClientError as e:
                                error_msg = e.response.get('Error', {}).get('Message', str(e))
                                print(f"‚ùå Failed - {error_msg}")
                                print(f"You don't have access to use {test_model} or the request format was incorrect")
                
                except ClientError as e:
                    print(f"Error listing models: {e.response['Error']['Message']}")
            
            except Exception as e:
                print(f"Error checking AWS Bedrock access: {str(e)}")

        if __name__ == "__main__":
            check_bedrock_access()
      metadata:
        extension: .py
        size_bytes: 7845
        language: python
    db/init/init.sql:
      content: |-
        -- Enable pgvector extension
        CREATE EXTENSION IF NOT EXISTS vector;

        -- Create the vector column with the correct dimension
        -- This will be used as a template for all vector columns
        CREATE TABLE IF NOT EXISTS vector_dimension_template (
            embedding vector(384)
        );

        -- Add full-text search support to chunks table

        -- Add tsvector column with generated value
        ALTER TABLE chunks
        ADD COLUMN IF NOT EXISTS content_tsv tsvector
        GENERATED ALWAYS AS (to_tsvector('english', content)) STORED;

        -- Create GIN index for efficient full-text search
        CREATE INDEX IF NOT EXISTS idx_chunks_content_tsv
        ON chunks USING GIN (content_tsv);

        -- Analyze table to update statistics for query planner
        ANALYZE chunks;
      metadata:
        extension: .sql
        size_bytes: 696
        language: sql
    src/chat/__init__.py:
      content: ''
      metadata:
        extension: .py
        size_bytes: 0
        language: python
    src/chat/app.py:
      content: |
        """
        Multi-Provider Chat Application

        This is the main application file that ties together all components of the chat application.
        It uses Streamlit for the UI and supports multiple LLM providers.
        """

        import streamlit as st
        import os
        import sys
        from dotenv import load_dotenv

        # Add src directory to Python path if not already there
        src_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
        if src_path not in sys.path:
            sys.path.insert(0, src_path)

        # Import utility modules
        from chat.util.logging_util import logger as llm_logger

        # Import application modules
        from chat.conversation.conversation_storage import ConversationStorage
        from chat.ai.provider_manager import initialize_provider, get_available_providers
        from chat.ui.conversation_manager import get_conversation, initialize_conversation_id
        from chat.ui.sidebar import (
            render_provider_settings,
            render_conversation_management,
            render_current_conversation_details
        )
        from chat.ui.chat import (
            display_chat_messages,
            handle_user_input,
            initialize_chat_history
        )


        def setup_environment():
            """Load environment variables and configure logging."""
            # Load environment variables
            if load_dotenv():
                llm_logger.info(".env file loaded successfully.")
            else:
                llm_logger.warning(".env file not found. Relying on pre-set environment variables.")


        def setup_page():
            """Configure the Streamlit page settings."""
            st.set_page_config(
                page_title="Multi-Provider Chat App",
                page_icon="ü§ñ",
                layout="wide"
            )
            
            # App title and description
            st.title("ü§ñ Multi-Provider Chat App")
            st.caption("Chat with multiple LLM providers using Streamlit and LiteLLM.")


        @st.cache_resource
        def get_conversation_storage():
            """Initialize and retrieve the conversation storage."""
            storage_dir = os.environ.get("CONVERSATION_STORAGE_DIR", "conversations")
            return ConversationStorage(storage_dir)


        def main():
            """Main application function."""
            # Setup environment and page
            setup_environment()
            setup_page()

            # Get available providers
            providers = get_available_providers()

            # Get conversation storage
            conversation_storage = get_conversation_storage()

            # Create tabs in the sidebar
            sidebar_tabs = st.sidebar.tabs(["Chat Settings", "RAG", "Search"])

            from chat.rag.torch_fix import apply_torch_fix
            apply_torch_fix()

            # Initialize RAG service if not already done
            if "rag_service" not in st.session_state:
                try:
                    from chat.rag.rag_service import RAGService
                    st.session_state.rag_service = RAGService()
                except Exception as e:
                    st.sidebar.error(f"Error initializing RAG service: {str(e)}")
                    st.session_state.rag_service = None
                    st.session_state.rag_enabled = False

            # Render standard sidebar components in the Chat Settings tab
            with sidebar_tabs[0]:
                # Provider settings
                selected_provider, selected_model, temperature, use_streaming = render_provider_settings(providers)

                # Conversation management
                render_conversation_management(conversation_storage, selected_provider, selected_model)

                # Current conversation details
                render_current_conversation_details(conversation_storage, selected_provider, selected_model)

            # Render RAG sidebar components in the RAG tab
            with sidebar_tabs[1]:
                # Import and use the RAG sidebar
                from chat.ui.sidebar_rag import render_rag_sidebar
                render_rag_sidebar()
            
            # Render Search interface in the Search tab
            with sidebar_tabs[2]:
                # Debug information
                has_rag_service = "rag_service" in st.session_state and st.session_state.rag_service is not None
                has_rag_project = "rag_project" in st.session_state and st.session_state.rag_project is not None
                has_project_id = "current_rag_project_id" in st.session_state and st.session_state.current_rag_project_id is not None
                
                # Show debug info
                with st.expander("Debug Info", expanded=False):
                    st.write(f"RAG Service exists: {has_rag_service}")
                    st.write(f"RAG Project exists: {has_rag_project}")
                    st.write(f"Project ID exists: {has_project_id}")
                    if has_project_id:
                        st.write(f"Project ID: {st.session_state.current_rag_project_id}")
                    st.write("Session state keys:", list(st.session_state.keys()))
                
                if has_rag_service and has_project_id:
                    # Try to get the project if we only have the ID
                    if not has_rag_project and st.session_state.rag_service:
                        try:
                            projects = st.session_state.rag_service.list_projects()
                            project = next((p for p in projects if p.id == st.session_state.current_rag_project_id), None)
                            if project:
                                st.session_state.rag_project = project
                                has_rag_project = True
                        except Exception as e:
                            st.error(f"Error loading project: {str(e)}")
                
                if has_rag_service and has_rag_project:
                    try:
                        from chat.ui.search_interface import render_search_interface, render_search_statistics
                    except ImportError as e:
                        st.error(f"Failed to import search interface: {e}")
                        render_search_interface = None
                        render_search_statistics = None
                    
                    # Add a toggle to show search in main area
                    show_search_main = st.checkbox(
                        "Show search in main area",
                        value=st.session_state.get("show_search_main", False),
                        help="Toggle between chat and search interface in the main area"
                    )
                    st.session_state.show_search_main = show_search_main
                    
                    if show_search_main:
                        st.info("Search interface is now shown in the main area")
                    
                    # Show search statistics in sidebar
                    if render_search_statistics:
                        render_search_statistics(
                            st.session_state.rag_service,
                            st.session_state.rag_project.id
                        )
                else:
                    st.warning("Please select a project in the RAG tab first")
                    if not has_rag_service:
                        st.error("RAG service is not initialized")
                    if not has_project_id:
                        st.info("No project ID found in session state")
                    if has_project_id and not has_rag_project:
                        st.info(f"Project ID {st.session_state.current_rag_project_id} exists but project object not loaded")

            # Initialize provider (using selections from first tab)
            llm_provider, error_message = initialize_provider(selected_provider, selected_model)

            # Display error message if provider initialization failed
            if error_message:
                st.error(error_message)
                st.sidebar.error(f"Provider failed: {error_message}")

            # Check if we should show search interface or chat
            if st.session_state.get("show_search_main", False) and st.session_state.get("rag_service") and st.session_state.get("rag_project"):
                # Show search interface in main area
                try:
                    from chat.ui.search_interface import render_search_interface
                    render_search_interface(
                        st.session_state.rag_service,
                        st.session_state.rag_project.id
                    )
                except ImportError as e:
                    st.error(f"Failed to import search interface: {e}")
                    st.info("Falling back to chat interface")
            else:
                # Show chat interface (default)
                # Initialize chat history
                initialize_chat_history(selected_provider, selected_model)

                # Initialize conversation
                initialize_conversation_id()
                conversation = get_conversation(conversation_storage)

                # Display existing chat messages
                display_chat_messages(st.session_state.messages)

                # Handle user input
                handle_user_input(
                    llm_provider=llm_provider,
                    conversation=conversation,
                    conversation_storage=conversation_storage,
                    selected_provider=selected_provider,
                    selected_model=selected_model,
                    temperature=temperature,
                    use_streaming=use_streaming
                )

        if __name__ == "__main__":
            main()
      metadata:
        extension: .py
        size_bytes: 8450
        language: python
    src/chat/ui/sidebar.py:
      content: |-
        """
        Sidebar UI components for the chat application.

        This module contains functions for rendering the sidebar UI components
        including provider selection, model selection, temperature controls,
        and conversation management.
        """

        import streamlit as st
        import uuid
        from datetime import datetime
        import os
        from typing import Dict, Any, Tuple

        from chat.conversation.conversation import MessageType
        from chat.conversation.conversation_storage import ConversationStorage
        from chat.util.logging_util import logger as llm_logger


        def render_provider_settings(providers: Dict[str, Dict[str, Any]]) -> Tuple[str, str, float, bool]:
            """
            Render the provider settings section in the sidebar.
            
            Args:
                providers: Dictionary of available providers and their models
                
            Returns:
                Tuple containing (selected_provider, selected_model, temperature, use_streaming)
            """
            st.header("Provider Settings")
            
            # Provider selection
            selected_provider = st.selectbox("Select Provider", list(providers.keys()))
            
            # Model selection for the chosen provider
            provider_info = providers[selected_provider]
            selected_model = st.selectbox("Select Model", provider_info["models"])
            
            # Temperature slider
            temperature = st.slider("Temperature", min_value=0.0, max_value=1.0, value=0.7, step=0.1)
            
            # Streaming option - default to enabled for all providers
            # All providers now support streaming
            use_streaming = st.checkbox("Enable streaming responses", 
                                      value=True,
                                      help="When enabled, responses will stream in real-time instead of waiting for the complete response.")
            
            # Provider-specific settings
            if selected_provider == "Ollama":
                render_ollama_settings(selected_model)
            elif selected_provider == "AWS Bedrock":
                render_bedrock_settings()

            return selected_provider, selected_model, temperature, use_streaming


        def render_bedrock_settings():
            """Render AWS Bedrock-specific settings."""
            st.subheader("AWS Bedrock Settings")
            
            # Get current AWS settings from environment variables
            current_region = os.environ.get("AWS_REGION", "us-east-1")
            
            # Allow the user to select a region
            aws_regions = [
                "us-east-1", "us-east-2", "us-west-1", "us-west-2", 
                "eu-west-1", "eu-central-1", "ap-northeast-1", "ap-southeast-1", 
                "ap-southeast-2", "ap-south-1"
            ]
            
            selected_region = st.selectbox("AWS Region", aws_regions, 
                                          index=aws_regions.index(current_region) if current_region in aws_regions else 0)
            
            # Update the environment variable if it has changed
            if selected_region != current_region:
                os.environ["AWS_REGION"] = selected_region
                llm_logger.info(f"Updated AWS Region to: {selected_region}")
            
            # Check AWS credentials
            aws_access_key = os.environ.get("AWS_ACCESS_KEY_ID", "")
            aws_secret_key = os.environ.get("AWS_SECRET_ACCESS_KEY", "")
            aws_session_token = os.environ.get("AWS_SESSION_TOKEN", "")
            
            if not aws_access_key or not aws_secret_key:
                st.warning("‚ö†Ô∏è AWS credentials not configured. Please add AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY to your .env file.")
            else:
                # Show masked credentials
                st.success("‚úÖ AWS credentials configured")
                st.text(f"Access Key ID: {aws_access_key[:4]}...{aws_access_key[-4:] if len(aws_access_key) > 8 else ''}")
                
                # Show session token info if present
                if aws_session_token:
                    st.info("üîë Using temporary credentials with session token")
                    # Show a very short preview of the token for confirmation
                    token_preview = aws_session_token[:4] + "..." + aws_session_token[-4:] if len(aws_session_token) > 8 else ""
                    st.text(f"Session Token: {token_preview}")
                
            # Add a button to check available models
            if st.button("Check Available Bedrock Models"):
                try:
                    import boto3
                    
                    # Create a Bedrock client
                    bedrock_service_client = boto3.client(
                        "bedrock",
                        region_name=selected_region,
                        aws_access_key_id=aws_access_key,
                        aws_secret_access_key=aws_secret_key,
                        **({"aws_session_token": aws_session_token} if aws_session_token else {})
                    )
                    
                    try:
                        # Try to list models
                        response = bedrock_service_client.list_foundation_models()
                        accessible_models = [model["modelId"] for model in response.get("modelSummaries", [])]
                        
                        if accessible_models:
                            st.success(f"‚úÖ Found {len(accessible_models)} accessible models in your AWS account")
                            
                            # Display models by provider
                            provider_models = {}
                            for model_id in accessible_models:
                                provider = model_id.split(".")[0] if "." in model_id else "other"
                                if provider not in provider_models:
                                    provider_models[provider] = []
                                provider_models[provider].append(model_id)
                            
                            for provider, models in provider_models.items():
                                with st.expander(f"{provider.capitalize()} Models ({len(models)})"):
                                    st.write("\n".join([f"- `{model}`" for model in models]))
                        else:
                            st.warning("No models found. You need to request access to models in the AWS Bedrock console.")
                    
                    except Exception as e:
                        if "AccessDeniedException" in str(e):
                            st.error("Access denied. Your IAM user/role needs 'bedrock:ListFoundationModels' permission.")
                        else:
                            st.error(f"Error listing models: {str(e)}")
                            
                except Exception as e:
                    st.error(f"Error checking models: {str(e)}")
            
            # Information about AWS Bedrock
            with st.expander("About AWS Bedrock"):
                st.markdown("""
                **Amazon Bedrock** is a fully managed service that makes high-performing foundation models (FMs) from leading 
                AI companies available through a unified API.
                
                Available models include:
                - Claude (from Anthropic)
                - Llama 3 (from Meta)
                - Titan (from Amazon)
                - Command (from Cohere)
                
                To use AWS Bedrock with this chat app:
                1. Make sure you have AWS credentials configured in your .env file
                2. Ensure your AWS account has access to the selected models
                3. Choose the appropriate region where the models are available
                
                **Important Requirements:**
                
                1. **Request Model Access**
                   - Go to AWS Management Console ‚Üí Amazon Bedrock ‚Üí Model access
                   - Request access to the models you want to use
                   - Some models are approved immediately, others may require a waiting period
                
                2. **On-Demand vs. Provisioned Throughput**
                   - This app uses "on-demand throughput" (pay-as-you-go usage)
                   - Some newer models (like some Claude 3.7 variants) only work with "provisioned throughput"
                   - Provisioned throughput requires creating an inference profile in the AWS console
                   - Stick with models that support on-demand throughput unless you set up an inference profile
                
                3. **Model Versions Matter**
                   - Use the exact model ID that matches what's available in your AWS account
                   - Use the "Check Available Bedrock Models" button above to verify your available models
                
                For more information, visit [AWS Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)
                """)


        def render_ollama_settings(selected_model: str = ""):
            """Render Ollama-specific settings."""
            st.subheader("Ollama Settings")

            # Get the current base URL
            current_base_url = os.environ.get("OLLAMA_BASE_URL", "http://localhost:11434")

            # Allow the user to change the base URL
            ollama_base_url = st.text_input("Ollama API Base URL", value=current_base_url)

            # Update the environment variable if it has changed
            if ollama_base_url != current_base_url:
                os.environ["OLLAMA_BASE_URL"] = ollama_base_url
                llm_logger.info(f"Updated Ollama base URL to: {ollama_base_url}")

            # Model-specific settings
            if selected_model:
                st.subheader(f"Model: {selected_model}")

                # Show different settings based on model size
                is_large_model = any(size in selected_model for size in ["70b", "72b"])
                is_medium_model = any(size in selected_model for size in ["27b", "32b"])

                if is_large_model:
                    st.warning(f"‚ö†Ô∏è {selected_model} is a very large model that requires significant RAM (40-45GB). Responses may take longer and context length is limited.")

                    # Context size settings for large models
                    context_size = st.slider("Context Tokens", min_value=256, max_value=4096, value=2048, step=256,
                              help="Maximum number of tokens to generate. Lower values reduce memory usage.", key="context_slider_large")
                    st.session_state.ollama_context_size = context_size

                elif is_medium_model:
                    st.warning(f"‚ö†Ô∏è {selected_model} is a large model that requires significant RAM (15-20GB). Responses may take longer.")

                    # Context size settings for medium models
                    context_size = st.slider("Context Tokens", min_value=512, max_value=6144, value=2560, step=512,
                              help="Maximum number of tokens to generate. Lower values reduce memory usage.", key="context_slider_medium")
                    st.session_state.ollama_context_size = context_size

                elif "llama4:scout" in selected_model:
                    st.success(f"‚úÖ {selected_model} is Meta's newest model, optimized for efficiency and performance.")

                    # Scout-specific settings
                    st.info("üí° Llama 4 Scout is designed for efficient operation with good performance.")
                    context_size = st.slider("Context Tokens", min_value=1024, max_value=8192, value=4096, step=1024,
                              help="Maximum number of tokens to generate.", key="context_slider_scout")
                    st.session_state.ollama_context_size = context_size

                else:
                    st.success(f"‚úÖ {selected_model} is optimized for your system and should run efficiently.")

                # Add model-specific notes based on model name
                if "deepseek" in selected_model:
                    st.info("üí° DeepSeek models excel at reasoning tasks and problem-solving.")
                elif "gemma" in selected_model:
                    st.info("üí° Gemma models are Google's efficient models with good instruction following.")
                elif "qwen" in selected_model:
                    st.info("üí° Qwen models have strong multilingual capabilities.")

            # Option to check Ollama status
            if st.button("Check Ollama Status"):
                try:
                    import requests
                    try:
                        response = requests.get(f"{ollama_base_url}/api/version", timeout=5)
                        if response.status_code == 200:
                            data = response.json()
                            st.success(f"Ollama is running! Version: {data.get('version', 'unknown')}")

                            # Also check for loaded models
                            try:
                                models_response = requests.get(f"{ollama_base_url}/api/tags", timeout=5)
                                if models_response.status_code == 200:
                                    models_data = models_response.json()
                                    model_count = len(models_data.get("models", []))
                                    st.info(f"Found {model_count} models available on your Ollama server.")
                                else:
                                    st.warning("Could not retrieve model information.")
                            except Exception as e:
                                st.warning(f"Could not check available models: {e}")

                        else:
                            st.error(f"Ollama returned status code {response.status_code}")
                    except requests.exceptions.ConnectionError:
                        st.error("Could not connect to Ollama. Please check that it's running and the URL is correct.")
                    except Exception as e:
                        st.error(f"Error checking Ollama status: {e}")
                except ImportError:
                    st.error("Requests library not available. Cannot check Ollama status.")

            # Information about Ollama
            with st.expander("About Ollama"):
                st.markdown("""
                **Ollama** lets you run open-source large language models locally on your machine.
                
                To use Ollama with this chat app:
                1. Install Ollama from [ollama.ai](https://ollama.ai)
                2. Ensure the Ollama server is running
                3. Set the correct base URL above (default: http://localhost:11434)
                
                Currently installed models:
                - gemma3:27b (Google's 27B parameter model)
                - qwen3:32b (Alibaba's newer 32B parameter model)
                - qwen:72b (Alibaba's 72B parameter multilingual model)
                - deepseek-r1:70b (70B parameter specialized reasoning model)
                - llama3.3:latest (Meta's Llama 3.3 model)
                - llama4:scout (Meta's newest Llama 4 Scout model)
                
                To install additional models, use: `ollama pull MODEL_NAME`
                """)


        def render_conversation_management(
            conversation_storage: ConversationStorage,
            selected_provider: str,
            selected_model: str
        ) -> None:
            """
            Render the conversation management section in the sidebar.

            Args:
                conversation_storage: Instance of ConversationStorage
                selected_provider: Currently selected provider name
                selected_model: Currently selected model name
            """
            st.header("Conversation Management")

            # Context maintenance option
            maintain_context = st.checkbox(
                "Maintain Conversation Context",
                value=True,
                help="When enabled, the app will send the entire conversation history to the LLM."
            )

            # Store the context maintenance setting in session state
            st.session_state.maintain_context = maintain_context

            # Load existing conversations
            st.subheader("Saved Conversations")
            try:
                # Create storage directory if it doesn't exist
                os.makedirs(os.environ.get("CONVERSATION_STORAGE_DIR", "conversations"), exist_ok=True)

                conversation_list = conversation_storage.list_conversations()

                if conversation_list:
                    # Create a selectbox of saved conversations
                    conversation_options = ["Current"] + [
                        f"{c['title']} ({datetime.fromisoformat(c['updated_at']).strftime('%Y-%m-%d %H:%M')})"
                        for c in conversation_list
                    ]
                    selected_conversation = st.selectbox("Select Conversation", conversation_options)

                    # Load the selected conversation
                    if selected_conversation != "Current" and conversation_list:
                        # Find the selected conversation index
                        selected_idx = conversation_options.index(selected_conversation) - 1  # Adjust for "Current"

                        if st.button("Load Selected Conversation"):
                            # Get the conversation ID
                            conversation_id = conversation_list[selected_idx]["id"]

                            # Load the conversation
                            st.session_state.conversation_id = conversation_id
                            # Clear the current conversation object to force reload
                            if "conversation_obj" in st.session_state:
                                del st.session_state.conversation_obj

                            # Update messages in session state from the loaded conversation
                            loaded_conversation = conversation_storage.load_conversation(conversation_id)
                            if loaded_conversation:
                                st.session_state.messages = [
                                    {"role": "user" if msg.message_type == MessageType.INPUT else "assistant",
                                     "content": msg.content}
                                    for msg in loaded_conversation.messages
                                ]
                                st.success(f"Loaded conversation: {loaded_conversation.title or conversation_id[:8]}")
                                st.rerun()
                else:
                    st.info("No saved conversations found. Start chatting to create one!")
            except Exception as e:
                llm_logger.error(f"Error loading conversation list: {e}", exc_info=True)
                st.error("Could not load saved conversations")

            # Conversation actions
            st.subheader("Conversation Actions")
            col1, col2 = st.columns(2)

            with col1:
                if st.button("New Conversation"):
                    # Create a new conversation
                    st.session_state.conversation_id = str(uuid.uuid4())
                    if "conversation_obj" in st.session_state:
                        del st.session_state.conversation_obj

                    # Clear messages
                    st.session_state.messages = [
                        {"role": "assistant",
                         "content": f"Hello! I'm using {selected_provider}'s {selected_model}. How can I help you today?"}
                    ]
                    st.rerun()

            with col2:
                if st.button("Save Conversation"):
                    # Get the current conversation
                    from chat.ui.conversation_manager import get_conversation
                    current_conversation = get_conversation(conversation_storage)

                    # Generate a title if none exists
                    if not current_conversation.title:
                        current_conversation.title = conversation_storage.generate_conversation_title(current_conversation)

                    # Save the conversation
                    if conversation_storage.save_conversation(current_conversation):
                        st.success(f"Conversation saved: {current_conversation.title or current_conversation.id[:8]}")
                    else:
                        st.error("Failed to save conversation")


        def render_current_conversation_details(conversation_storage: ConversationStorage,
                                                selected_provider: str,
                                                selected_model: str) -> None:
            """
            Render the current conversation details and management options.

            Args:
                conversation_storage: Instance of ConversationStorage
                selected_provider: Currently selected provider name
                selected_model: Currently selected model name
            """
            st.subheader("Current Conversation")

            try:
                # Get current conversation
                from chat.ui.conversation_manager import get_conversation
                conversation = get_conversation(conversation_storage)

                # Edit conversation title
                current_title = conversation.title or conversation_storage.generate_conversation_title(conversation)
                new_title = st.text_input("Conversation Title", value=current_title)

                if new_title != current_title:
                    conversation.title = new_title
                    try:
                        if conversation_storage.save_conversation(conversation):
                            st.success("Title updated")
                        else:
                            st.error("Failed to update title")
                    except Exception as e:
                        llm_logger.error(f"Error updating conversation title: {e}", exc_info=True)
                        st.error("Error saving title")

                # Delete conversation button
                if st.button("Delete Current Conversation"):
                    try:
                        if conversation_storage.delete_conversation(st.session_state.conversation_id):
                            # Create a new conversation
                            st.session_state.conversation_id = str(uuid.uuid4())
                            if "conversation_obj" in st.session_state:
                                del st.session_state.conversation_obj

                            # Clear messages
                            st.session_state.messages = [
                                {"role": "assistant",
                                 "content": f"Previous conversation deleted. I'm ready for a new conversation using {selected_provider}'s {selected_model}!"}
                            ]
                            st.success("Conversation deleted")
                            st.rerun()
                        else:
                            st.error("Failed to delete conversation")
                    except Exception as e:
                        llm_logger.error(f"Error deleting conversation: {e}", exc_info=True)
                        st.error("Error deleting conversation")

                # Export conversation
                if st.button("Export Conversation"):
                    try:
                        # Create a download link for the conversation history
                        conversation_data = "\n".join([
                            f"[{msg['role']}] ({datetime.now().strftime('%Y-%m-%d %H:%M')}): {msg['content']}"
                            for msg in st.session_state.messages
                        ])

                        title_slug = current_title.replace(" ", "_").lower()[:30]
                        st.download_button(
                            label="Download Conversation",
                            data=conversation_data,
                            file_name=f"{title_slug}_{st.session_state.conversation_id[:8]}.txt",
                            mime="text/plain"
                        )
                    except Exception as e:
                        llm_logger.error(f"Error exporting conversation: {e}", exc_info=True)
                        st.error("Error generating export")

                # Display conversation statistics
                st.text(f"ID: {conversation.id[:8]}...")
                st.text(f"Messages: {len(conversation.messages)}")
                st.text(f"Created: {conversation.created_at.strftime('%Y-%m-%d %H:%M')}")
                st.text(f"Updated: {conversation.updated_at.strftime('%Y-%m-%d %H:%M')}")

            except Exception as e:
                llm_logger.error(f"Error in conversation management sidebar: {e}", exc_info=True)
                st.error("Error loading conversation details")
                st.text("Try starting a new conversation.")
      metadata:
        extension: .py
        size_bytes: 22127
        language: python
    src/chat/ui/__init__.py:
      content: |2+

      metadata:
        extension: .py
        size_bytes: 1
        language: python
    src/chat/ui/chat.py:
      content: |-
        """
        Chat UI components for the chat application.

        This module contains functions for rendering the chat UI components
        including message display, input handling, and response generation.
        """
        import logging

        import streamlit as st
        import asyncio
        from typing import Dict, Optional, List, Union, Any

        from chat.conversation.conversation import Conversation, MessageType
        from chat.conversation.conversation_storage import ConversationStorage
        from chat.ai.llm_provider import LLMProvider
        from chat.util.logging_util import logger as llm_logger
        from chat.ui.chat_utils import handle_message_with_streaming
        from chat.rag.rag_chat import get_rag_context_for_prompt, format_rag_response


        def display_chat_messages(messages: List[Dict[str, str]]) -> None:
            """
            Display the chat message history.
            
            Args:
                messages: List of message dictionaries with 'role' and 'content' keys
            """
            for message in messages:
                with st.chat_message(message["role"]):
                    st.markdown(message["content"])


        def handle_user_input(
            llm_provider: Optional[LLMProvider],
            conversation: Optional[Conversation],
            conversation_storage: ConversationStorage,
            selected_provider: str,
            selected_model: str,
            temperature: float,
            system_prompt: str = "You are a helpful and concise chat assistant designed for providing accurate and relevant information.",
            use_streaming: bool = False
        ) -> None:
            """
            Handle user input, generate responses, and update the conversation.
            
            Args:
                llm_provider: The initialized LLM provider
                conversation: The current conversation object
                conversation_storage: Instance of ConversationStorage
                selected_provider: Currently selected provider name
                selected_model: Currently selected model name
                temperature: Temperature setting for response generation
                system_prompt: System prompt to use for the LLM
                use_streaming: Whether to use streaming mode for response generation
            """
            # Get user input from chat input
            if prompt := st.chat_input("Your message..."):
                # Add user message to chat history and display it
                st.session_state.messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt)

                logging.info(f"PROMPT: {prompt}")
                # Check for RAG context
                enhanced_prompt = get_rag_context_for_prompt(prompt)
                logging.info(f"ENHANCED_PROMPT: {enhanced_prompt}")


                # Use enhanced prompt if available, otherwise use original prompt
                final_prompt = enhanced_prompt if enhanced_prompt else prompt

                # If RAG is enabled and enhanced prompt is available, indicate that RAG is being used
                if enhanced_prompt:
                    with st.chat_message("system"):
                        st.info("Using document context to enhance response...")
                
                # Generate and display assistant's response
                if not llm_provider:
                    with st.chat_message("assistant"):
                        st.error("LLM Provider is not available. Cannot process messages.")
                    if not any(m["role"] == "assistant" and "Provider is not available" in m["content"] for m in
                               st.session_state.messages):
                        st.session_state.messages.append({"role": "assistant",
                                                         "content": "LLM Provider is not available due to an initialization error. Please check the logs."})
                    return

                # Prepare generation options
                generation_options = {
                    "system_prompt": system_prompt,
                    "temperature": temperature,
                }
                
                llm_logger.info(f"User prompt for {selected_provider}: {prompt}")
                
                # Decide whether to use conversation context
                maintain_context = st.session_state.get("maintain_context", True)
                active_conversation = conversation if maintain_context else None
                
                # Check if we should use streaming mode for any provider that supports it
                should_stream = (use_streaming and 
                                 hasattr(llm_provider, 'generate_completion_stream'))
                
                # Handle streaming separately (it creates its own chat message context)
                if should_stream:
                    llm_logger.info(f"Using streaming mode for {selected_provider}")
                    
                    try:
                        # Handle streaming
                        handle_message_with_streaming(
                            provider=llm_provider,
                            prompt=final_prompt,
                            conversation=active_conversation,
                            conversation_storage=conversation_storage,
                            options=generation_options
                        )
                    except Exception as e:
                        llm_logger.error(f"Error in streaming mode: {e}", exc_info=True)
                        # Fallback to non-streaming mode on error
                        with st.chat_message("assistant"):
                            st.error(f"Streaming failed: {str(e)}. Trying standard response...")
                            should_stream = False
                
                # Non-streaming path
                if not should_stream:
                    with st.chat_message("assistant"):
                        message_placeholder = st.empty()
                        message_placeholder.markdown("‚è≥ Thinking...")
                        
                        try:
                            # Use standard non-streaming approach
                            full_response_content = asyncio.run(
                                llm_provider.generate_completion(
                                    prompt=final_prompt,
                                    output_format="text",
                                    options=generation_options,
                                    conversation=active_conversation
                                )
                            )
                            
                            # Update UI after completion
                            llm_logger.info(f"{selected_provider} response received, length: {len(full_response_content or '')}")
                            
                            if not full_response_content:
                                full_response_content = "I received an empty response. Could you try rephrasing?"
                                llm_logger.warning("LLM returned an empty response.")

                            # Format response to highlight citations if RAG was used
                            if enhanced_prompt:
                                full_response_content = format_rag_response(full_response_content)
                            
                            # Display the response
                            message_placeholder.markdown(full_response_content)
                            
                            # Add assistant's response to session state
                            st.session_state.messages.append({"role": "assistant", "content": full_response_content})
                            
                            # Update conversation if needed
                            if active_conversation:
                                if not maintain_context:  # Only needed if not already updated via context
                                    active_conversation.add_message(full_response_content, MessageType.OUTPUT)
                                
                                # Auto-save the conversation
                                try:
                                    if conversation_storage.save_conversation(active_conversation):
                                        llm_logger.info(f"Auto-saved conversation {active_conversation.id}")
                                    else:
                                        llm_logger.warning(f"Failed to auto-save conversation {active_conversation.id}")
                                except Exception as e:
                                    llm_logger.error(f"Error during auto-save of conversation: {e}", exc_info=True)
                                
                        except ValueError as ve:
                            error_msg = f"Input/Output Error: {ve}"
                            llm_logger.error(error_msg, exc_info=True)
                            message_placeholder.error(error_msg)
                            st.session_state.messages.append({"role": "assistant", "content": f"I encountered an issue with the data format: {ve}"})
                            
                        except ImportError as ie:
                            error_msg = f"Import error: {ie}. Ensure all dependencies for the LLM provider are installed."
                            llm_logger.error(error_msg, exc_info=True)
                            message_placeholder.error(error_msg)
                            st.session_state.messages.append({"role": "assistant", "content": "There's a configuration problem with the LLM provider."})
                            
                        except Exception as e:
                            error_msg = f"Sorry, an unexpected error occurred: {type(e).__name__} - {e}"
                            llm_logger.error(error_msg, exc_info=True)
                            message_placeholder.error(error_msg)
                            st.session_state.messages.append({"role": "assistant", "content": "I ran into an unexpected problem trying to respond."})

        def initialize_chat_history(selected_provider: str, selected_model: str) -> None:
            """
            Initialize the chat history in session state if it doesn't exist.
            Update provider information if the provider has changed.
            
            Args:
                selected_provider: Currently selected provider name
                selected_model: Currently selected model name
            """
            # Initialize chat history if it doesn't exist
            if "messages" not in st.session_state:
                st.session_state.messages = [
                    {"role": "assistant",
                     "content": f"Hello! I'm using {selected_provider}'s {selected_model}. How can I help you today?"}
                ]
            # If provider changed, add a system message
            elif "current_provider" in st.session_state and st.session_state.current_provider != selected_provider:
                st.session_state.messages.append(
                    {"role": "assistant",
                     "content": f"I've switched to {selected_provider}'s {selected_model}. How can I help you?"}
                )
            
            # Update current provider
            st.session_state.current_provider = selected_provider
      metadata:
        extension: .py
        size_bytes: 10031
        language: python
    src/chat/ui/chat_utils.py:
      content: |-
        """
        Utility functions for chat UI components.

        This module contains utility functions specifically for chat UI operations.
        """

        import streamlit as st
        import asyncio
        from typing import Optional, Dict, Any, Callable

        from chat.ai.llm_provider import LLMProvider
        from chat.conversation.conversation import Conversation, MessageType
        from chat.conversation.conversation_storage import ConversationStorage
        from chat.util.logging_util import logger as llm_logger

        # Alias the logger for convenient access
        logger = llm_logger


        def handle_message_with_streaming(
            provider: LLMProvider,
            prompt: str,
            conversation: Optional[Conversation],
            conversation_storage: Optional[ConversationStorage],
            options: Dict[str, Any]
        ):
            """
            Handle streaming message display in the chat UI.
            
            Args:
                provider: The LLM provider instance
                prompt: User's input prompt
                conversation: Current conversation object
                conversation_storage: Instance of ConversationStorage
                options: Options for the provider (system_prompt, temperature, etc.)
            """
            # Add user message to session state
            st.session_state.messages.append({"role": "user", "content": prompt})
            # Note: We don't display the user message here because it should already
            # be displayed in the main chat UI before this function is called
            
            # Create placeholder for assistant response
            with st.chat_message("assistant"):
                message_placeholder = st.empty()
                message_placeholder.markdown("‚è≥ Thinking...")
                streaming_indicator = st.empty()
                full_response = ""
                
                # Function to update UI with each chunk
                def update_message_placeholder(chunk: str):
                    nonlocal full_response
                    full_response += chunk
                    # Add blinking cursor to show it's still streaming
                    message_placeholder.markdown(full_response + "‚ñå")
                
                try:
                    # Start streaming response
                    async def stream_response():
                        nonlocal full_response
                        try:
                            maintain_context = st.session_state.get("maintain_context", True)
                            active_conversation = conversation if maintain_context else None
                            
                            # Show streaming indicator 
                            streaming_indicator.markdown("*Streaming response...*")
                            
                            # Get the async generator
                            llm_logger.info(f"Starting streaming with provider {provider.__class__.__name__}")
                            generator = provider.generate_completion_stream(
                                prompt=prompt,
                                output_format="text",
                                options=options,
                                conversation=active_conversation,
                                callback=update_message_placeholder
                            )
                            llm_logger.info(f"Got generator of type: {type(generator)}")
                            
                            # Manually iterate over the generator
                            while True:
                                try:
                                    chunk = await anext(generator)
                                    # Processing is handled by callback
                                except StopAsyncIteration:
                                    break
                            
                            # Clear streaming indicator when done
                            streaming_indicator.empty()
                            
                            # Remove cursor and display final message
                            if full_response:
                                message_placeholder.markdown(full_response)
                            return full_response
                        except Exception as e:
                            logger.error(f"Error in stream_response: {e}", exc_info=True)
                            raise
                    
                    # Run the streaming in asyncio
                    try:
                        full_response = asyncio.run(stream_response())
                    except Exception as e:
                        logger.error(f"Error running stream_response: {e}", exc_info=True)
                        raise
                    
                    # Add to session state, making sure we avoid duplicate entries
                    # Check if the message was already added (can happen if this is interrupted and rerun)
                    if not st.session_state.messages or st.session_state.messages[-1]["content"] != full_response:
                        st.session_state.messages.append({"role": "assistant", "content": full_response})
                    
                    # If not using the conversation object for context (which would have been updated automatically),
                    # we need to manually add the response to the conversation
                    if not st.session_state.get("maintain_context", True) and conversation:
                        conversation.add_message(full_response, MessageType.OUTPUT)
                    
                    # Save conversation
                    if conversation_storage and conversation:
                        try:
                            if conversation and conversation_storage.save_conversation(conversation):
                                llm_logger.info(f"Auto-saved conversation {conversation.id}")
                            elif conversation:
                                llm_logger.warning(f"Failed to auto-save conversation {conversation.id}")
                        except Exception as e:
                            llm_logger.error(f"Error during auto-save of conversation: {e}", exc_info=True)
                        
                except Exception as e:
                    error_msg = f"Error during streaming: {str(e)}"
                    llm_logger.error(error_msg, exc_info=True)
                    message_placeholder.error(error_msg)
                    streaming_indicator.empty()  # Clear streaming indicator on error
                    
                    # Add error message to session state if not already done
                    if not st.session_state.messages or "Error during streaming" not in st.session_state.messages[-1]["content"]:
                        st.session_state.messages.append({"role": "assistant", "content": f"Error: {str(e)}"})
                        
                    # Try to fall back to non-streaming if possible
                    try:
                        llm_logger.info("Attempting fallback to non-streaming mode")
                        message_placeholder.markdown("‚è≥ Trying non-streaming mode as fallback...")
                        
                        # Use standard non-streaming approach as fallback
                        maintain_context = st.session_state.get("maintain_context", True)
                        active_conversation = conversation if maintain_context else None
                        
                        fallback_response = asyncio.run(
                            provider.generate_completion(
                                prompt=prompt,
                                output_format="text",
                                options=options,
                                conversation=active_conversation
                            )
                        )
                        
                        if fallback_response:
                            message_placeholder.markdown(fallback_response)
                            # Update session state with the fallback response
                            st.session_state.messages[-1] = {"role": "assistant", "content": fallback_response}
                            
                            # Update conversation if needed
                            if conversation:
                                conversation.add_message(fallback_response, MessageType.OUTPUT)
                                if conversation_storage:
                                    conversation_storage.save_conversation(conversation)
                                    
                            llm_logger.info("Fallback to non-streaming mode succeeded")
                            return  # Exit function successfully
                            
                    except Exception as fallback_error:
                        # If fallback fails too, just log the error
                        llm_logger.error(f"Fallback to non-streaming also failed: {fallback_error}", exc_info=True)
                        message_placeholder.error(f"Both streaming and non-streaming attempts failed. Please try again.")
      metadata:
        extension: .py
        size_bytes: 8008
        language: python
    src/chat/ui/sidebar_rag.py:
      content: |-
        """
        RAG sidebar components for the chat application.

        This module contains functions for rendering the RAG-specific sidebar components
        including project management, file upload, and RAG settings.
        """

        import streamlit as st
        import os
        from typing import Dict, List, Optional, Tuple

        from chat.rag.rag_service import RAGService
        from chat.util.logging_util import logger as llm_logger


        def initialize_rag_state():
            """Initialize RAG-related session state variables if they don't exist."""
            if "rag_enabled" not in st.session_state:
                st.session_state.rag_enabled = False

            if "rag_service" not in st.session_state:
                try:
                    from chat.rag.rag_service import RAGService
                    st.session_state.rag_service = RAGService()
                except Exception as e:
                    st.error(f"Error initializing RAG service: {str(e)}")
                    st.session_state.rag_service = None
                    st.session_state.rag_enabled = False
                    return

            if "current_rag_project_id" not in st.session_state:
                st.session_state.current_rag_project_id = None

            if "rag_similarity_threshold" not in st.session_state:
                st.session_state.rag_similarity_threshold = 0.7

            if "rag_max_chunks" not in st.session_state:
                st.session_state.rag_max_chunks = 3


        def render_rag_sidebar():
            """Render the RAG sidebar components."""
            st.header("RAG Settings")

            # Initialize RAG session state
            initialize_rag_state()

            # RAG toggle
            st.session_state.rag_enabled = st.checkbox(
                "Enable RAG",
                value=st.session_state.rag_enabled,
                help="When enabled, the chat will use Retrieval Augmented Generation to provide context from your documents."
            )

            if not st.session_state.rag_enabled:
                st.info("Enable RAG to access document management features.")
                return

            # Project management
            render_project_management()

            # Only show file management and settings if a project is selected
            if st.session_state.current_rag_project_id:
                with st.expander("File Management", expanded=True):
                    render_file_management(st.session_state.current_rag_project_id)

                with st.expander("RAG Settings", expanded=True):
                    render_rag_settings()


        def render_project_management():
            """Render project management components."""
            st.subheader("Project Management")

            # Get service instance
            rag_service = st.session_state.rag_service

            # Get list of projects
            projects = rag_service.list_projects()
            
            # If we have a project ID but no project object, try to restore it
            if (st.session_state.get("current_rag_project_id") and 
                not st.session_state.get("rag_project") and 
                projects):
                project_id = st.session_state.current_rag_project_id
                selected_project = next((p for p in projects if p.id == project_id), None)
                if selected_project:
                    st.session_state.rag_project = selected_project

            if projects:
                # Create a dropdown for project selection
                project_options = ["Select a project..."] + [f"{p.name} (ID: {p.id})" for p in projects]
                selected_option = st.selectbox("Select Project", project_options)

                if selected_option != "Select a project...":
                    # Extract project ID from selection
                    project_id = int(selected_option.split("ID: ")[1].rstrip(")"))
                    st.session_state.current_rag_project_id = project_id

                    # Display project info
                    selected_project = next((p for p in projects if p.id == project_id), None)
                    if selected_project:
                        # Store both the project ID and the project object
                        st.session_state.rag_project = selected_project
                        st.success(f"Using project: {selected_project.name}")
                        if selected_project.description:
                            st.info(selected_project.description)

            # Create new project section
            st.markdown("---")
            st.markdown("### Create New Project")

            new_project_name = st.text_input("Project Name", key="new_project_name")
            new_project_desc = st.text_area("Project Description", key="new_project_desc")

            if st.button("Create Project"):
                if new_project_name:
                    try:
                        new_project = rag_service.get_or_create_project(
                            name=new_project_name,
                            description=new_project_desc
                        )
                        st.session_state.current_rag_project_id = new_project.id
                        st.session_state.rag_project = new_project
                        st.success(f"Created project: {new_project.name} (ID: {new_project.id})")
                        st.rerun()
                    except Exception as e:
                        st.error(f"Error creating project: {str(e)}")
                else:
                    st.warning("Please enter a project name.")


        def render_file_management(project_id: int):
            """Render file management components for a project."""
            rag_service = st.session_state.rag_service

            # File upload section
            st.markdown("### Upload Document")

            # Create columns for upload components
            col1, col2 = st.columns([3, 1])

            with col1:
                uploaded_file = st.file_uploader(
                    "Choose a file",
                    type=["txt", "md", "py", "js", "html", "css", "json", "csv"],
                    key=f"file_uploader_{project_id}"
                )

            with col2:
                # Only show the button when a file is selected
                if uploaded_file is not None:
                    if st.button("Process File", key=f"process_file_{project_id}"):
                        try:
                            # Read file content
                            content = uploaded_file.getvalue().decode("utf-8")

                            # Create metadata
                            metadata = {
                                "filename": uploaded_file.name,
                                "type": uploaded_file.type,
                                "size": uploaded_file.size
                            }

                            # Add file to project
                            file = rag_service.add_document(
                                project_id=project_id,
                                filename=uploaded_file.name,
                                content=content,
                                metadata=metadata
                            )

                            if file:
                                st.success(f"Added file: {file.name}")
                                st.rerun()
                            else:
                                st.error("Failed to add document.")
                        except Exception as e:
                            st.error(f"Error processing file: {str(e)}")

            # List existing files
            st.markdown("### Project Files")

            try:
                files = rag_service.list_files(project_id)

                if not files:
                    st.info("No files in this project.")
                    return

                for file in files:
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.write(f"{file.name}")
                    with col2:
                        if st.button("Remove", key=f"remove_{file.id}_{project_id}"):
                            try:
                                if rag_service.handler.delete_file(file.id):
                                    st.success(f"Removed file: {file.name}")
                                    st.rerun()
                                else:
                                    st.error(f"Failed to remove file: {file.name}")
                            except Exception as e:
                                st.error(f"Error removing file: {str(e)}")
            except Exception as e:
                st.error(f"Error listing files: {str(e)}")


        def render_rag_settings():
            """Render RAG settings components."""
            # Similarity threshold
            st.session_state.rag_similarity_threshold = st.slider(
                "Similarity Threshold",
                min_value=0.0,
                max_value=1.0,
                value=st.session_state.rag_similarity_threshold,
                step=0.05,
                help="Minimum similarity score for a document chunk to be included in context."
            )

            # Max chunks
            st.session_state.rag_max_chunks = st.slider(
                "Max Chunks",
                min_value=1,
                max_value=10,
                value=st.session_state.rag_max_chunks,
                step=1,
                help="Maximum number of document chunks to include in context."
            )
      metadata:
        extension: .py
        size_bytes: 8161
        language: python
    src/chat/ui/conversation_manager.py:
      content: |
        """
        Conversation management module for the chat application.

        This module contains functions for managing conversations, including
        creating, loading, and retrieving conversation objects.
        """

        import streamlit as st
        import uuid


        from chat.conversation.conversation import Conversation, MessageType
        from chat.conversation.conversation_storage import ConversationStorage
        from chat.util.logging_util import logger as llm_logger


        def initialize_conversation_id() -> str:
            """
            Initialize the conversation ID in session state if it doesn't exist.
            
            Returns:
                The current conversation ID
            """
            if "conversation_id" not in st.session_state:
                # Create a new conversation ID
                st.session_state.conversation_id = str(uuid.uuid4())
                llm_logger.info(f"Created new conversation with ID: {st.session_state.conversation_id}")
            
            return st.session_state.conversation_id


        def get_conversation(conversation_storage: ConversationStorage) -> Conversation:
            """
            Create or retrieve the current conversation object.
            
            Args:
                conversation_storage: Instance of ConversationStorage
                
            Returns:
                The current Conversation object
            """
            # Ensure we have a conversation ID
            conversation_id = initialize_conversation_id()
            
            # Create or retrieve the conversation object
            if "conversation_obj" not in st.session_state or st.session_state.get(
                    "current_conversation_id") != conversation_id:
                # Try to load an existing conversation
                conversation = conversation_storage.load_conversation(conversation_id)
                
                if not conversation:
                    # Create a new conversation object
                    conversation = Conversation(id=conversation_id)
                    
                    # Add existing messages from session state if any
                    if "messages" in st.session_state:
                        for msg in st.session_state.messages:
                            msg_type = MessageType.INPUT if msg["role"] == "user" else MessageType.OUTPUT
                            conversation.add_message(msg["content"], msg_type, role=msg["role"])
                
                st.session_state.conversation_obj = conversation
                st.session_state.current_conversation_id = conversation_id
                
                # Generate a title if none exists
                if not conversation.title and conversation.messages:
                    conversation.title = conversation_storage.generate_conversation_title(conversation)
                    # Save the conversation with the new title
                    conversation_storage.save_conversation(conversation)
            
            return st.session_state.conversation_obj


        def sync_conversation_with_messages(conversation: Conversation) -> None:
            """
            Synchronize the conversation object with the messages in session state.
            
            This ensures that the conversation object and the displayed messages are in sync.
            
            Args:
                conversation: The conversation object to synchronize
            """
            if "messages" in st.session_state and conversation:
                # Clear existing messages in the conversation
                conversation.messages.clear()
                
                # Add messages from session state to the conversation
                for msg in st.session_state.messages:
                    msg_type = MessageType.INPUT if msg["role"] == "user" else MessageType.OUTPUT
                    conversation.add_message(msg["content"], msg_type, role=msg["role"])
      metadata:
        extension: .py
        size_bytes: 3394
        language: python
    src/chat/ui/search_interface.py:
      content: |-
        """
        Search interface for RAG documents with support for multiple search methods.
        Provides a tabular view of search results with metadata exploration.
        """

        import streamlit as st
        import pandas as pd
        import json
        import logging
        import time
        from typing import Dict, List, Optional, Tuple
        from datetime import datetime

        from chat.rag.rag_service import RAGService
        from vector_rag.model import ChunkResults

        # Set up logging
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.DEBUG)


        def render_search_interface(rag_service: RAGService, project_id: int):
            """Render the search interface with multiple search method support.
            
            Args:
                rag_service: The RAG service instance
                project_id: Current project ID
            """
            st.header("üìö Document Search")
            
            # Search method selection
            col1, col2 = st.columns([2, 1])
            
            with col1:
                search_method = st.selectbox(
                    "Search Method",
                    ["Semantic Search", "BM25 (Keyword)", "Hybrid Search"],
                    help="""
                    - **Semantic Search**: Finds conceptually similar content
                    - **BM25**: Exact keyword matching (best for technical terms)
                    - **Hybrid**: Combines semantic and keyword search
                    """
                )
            
            with col2:
                # Initialize default weights
                vector_weight = 0.5
                bm25_weight = 0.5
                
                if search_method == "Hybrid Search":
                    st.markdown("**Weight Configuration**")
                    vector_weight = st.slider(
                        "Semantic Weight",
                        min_value=0.0,
                        max_value=1.0,
                        value=0.5,
                        step=0.1,
                        help="Weight for semantic similarity (0-1)"
                    )
                    bm25_weight = 1.0 - vector_weight
                    st.caption(f"BM25 Weight: {bm25_weight:.1f}")
            
            # Search input
            search_query = st.text_input(
                "Search Query",
                placeholder="Enter your search terms...",
                help="Type keywords or natural language queries"
            )
            
            # Initialize default values
            similarity_threshold = 0.7
            rank_threshold = 0.0
            
            # Advanced options expander
            with st.expander("Advanced Options"):
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    page_size = st.number_input(
                        "Results per page",
                        min_value=5,
                        max_value=50,
                        value=10,
                        step=5
                    )
                    
                    # Add performance tip
                    if page_size > 20:
                        st.warning("‚ö†Ô∏è Large result sets may be slow. Consider using smaller page sizes or higher thresholds.")
                
                with col2:
                    if search_method in ["Semantic Search", "Hybrid Search"]:
                        similarity_threshold = st.slider(
                            "Similarity Threshold",
                            min_value=0.0,
                            max_value=1.0,
                            value=0.7,
                            step=0.05,
                            help="Minimum similarity score for semantic search"
                        )
                    
                    if search_method in ["BM25 (Keyword)", "Hybrid Search"]:
                        rank_threshold = st.slider(
                            "BM25 Rank Threshold",
                            min_value=0.0,
                            max_value=1.0,
                            value=0.0,
                            step=0.05,
                            help="Minimum BM25 rank score"
                        )
                
                with col3:
                    show_metadata = st.checkbox("Show Metadata", value=True)
                    show_scores = st.checkbox("Show Scores", value=True)
            
            # Search button
            if st.button("üîç Search", type="primary", disabled=not search_query):
                print(f"\n=== SEARCH BUTTON CLICKED ===")
                print(f"Query: '{search_query}'")
                print(f"Method: {search_method}")
                print(f"Project ID: {project_id}")
                print(f"RAG Service: {rag_service}")
                print(f"=============================\n")
                
                with st.spinner("Searching..."):
                    try:
                        print(f"DEBUG: About to call perform_search...")
                        results = perform_search(
                            rag_service,
                            project_id,
                            search_query,
                            search_method,
                            page_size,
                            similarity_threshold,
                            rank_threshold,
                            vector_weight,
                            bm25_weight
                        )
                        print(f"DEBUG: perform_search returned: {results}")
                        
                        if results:
                            print(f"DEBUG: Displaying {len(results)} results")
                            display_search_results(
                                results,
                                search_method,
                                show_metadata,
                                show_scores
                            )
                        else:
                            print(f"DEBUG: No results to display")
                            st.info("No results found. Try adjusting your search terms or thresholds.")
                            
                    except Exception as e:
                        print(f"ERROR: Exception in search button handler: {str(e)}")
                        import traceback
                        print(f"ERROR: Traceback: {traceback.format_exc()}")
                        st.error(f"Search error: {str(e)}")


        def perform_search(
            rag_service: RAGService,
            project_id: int,
            query: str,
            method: str,
            page_size: int,
            similarity_threshold: float,
            rank_threshold: float,
            vector_weight: float,
            bm25_weight: float
        ) -> Optional[List[Dict]]:
            """Perform search based on selected method.
            
            Returns:
                List of result dictionaries with chunk data and scores
            """
            start_time = time.time()
            print(f"\n=== SEARCH STARTED ===")
            print(f"Method: {method}, Query: '{query}', Project: {project_id}")
            print(f"Threshold: {similarity_threshold}, Page size: {page_size}")
            
            try:
                if method == "Semantic Search":
                    print(f"DEBUG: Starting semantic search...")
                    search_start = time.time()
                    
                    results = rag_service.api.search_text(
                        project_id=project_id,
                        query_text=query,
                        page=1,
                        page_size=page_size,
                        similarity_threshold=similarity_threshold
                    )
                    
                    search_time = time.time() - search_start
                    print(f"DEBUG: Search completed in {search_time:.2f}s, got {len(results.results) if results and results.results else 0} results")
                elif method == "BM25 (Keyword)":
                    print(f"DEBUG: Attempting BM25 search...")
                    results = rag_service.api.search_bm25(
                        project_id=project_id,
                        query_text=query,
                        page=1,
                        page_size=page_size,
                        rank_threshold=rank_threshold
                    )
                    print(f"DEBUG: BM25 search returned: {type(results)}")
                else:  # Hybrid Search
                    print(f"DEBUG: Attempting hybrid search...")
                    results = rag_service.api.search_hybrid(
                        project_id=project_id,
                        query_text=query,
                        page=1,
                        page_size=page_size,
                        vector_weight=vector_weight,
                        bm25_weight=bm25_weight,
                        similarity_threshold=similarity_threshold,
                        rank_threshold=rank_threshold
                    )
                    print(f"DEBUG: Hybrid search returned: {type(results)}")
                
                if not results or not hasattr(results, 'results') or not results.results:
                    print(f"DEBUG: No results found")
                    return None
                
                processing_start = time.time()
                print(f"DEBUG: Processing {len(results.results)} results...")
                
                # Convert results to list of dictionaries
                formatted_results = []
                
                # Check the first result to understand the chunk structure
                if results.results:
                    first_chunk = results.results[0].chunk
                    chunk_attrs = [attr for attr in dir(first_chunk) if not attr.startswith('_')]
                    print(f"DEBUG: Chunk attributes: {chunk_attrs}")
                
                for i, result in enumerate(results.results):
                    try:
                        chunk = result.chunk
                        
                        # Handle different chunk attribute names
                        chunk_id = getattr(chunk, 'id', getattr(chunk, 'chunk_id', i))
                        file_name = getattr(chunk, 'file_name', getattr(chunk, 'filename', 'Unknown'))
                        file_id = getattr(chunk, 'file_id', None)
                        chunk_index = getattr(chunk, 'chunk_index', getattr(chunk, 'index', i))
                        content = getattr(chunk, 'content', str(chunk))
                        metadata = getattr(chunk, 'metadata', getattr(chunk, 'chunk_metadata', {})) or {}
                        
                        formatted_result = {
                            "id": chunk_id,
                            "content": content,
                            "file_name": file_name,
                            "file_id": file_id,
                            "chunk_index": chunk_index,
                            "score": result.score,
                            "metadata": metadata
                        }
                        
                        # Add method-specific scores if available
                        if method == "Hybrid Search" and "_scores" in metadata:
                            formatted_result["vector_score"] = metadata["_scores"].get("vector", 0)
                            formatted_result["bm25_score"] = metadata["_scores"].get("bm25", 0)
                        
                        formatted_results.append(formatted_result)
                        
                    except Exception as result_error:
                        print(f"ERROR: Failed to process result {i+1}: {str(result_error)}")
                        continue
                
                processing_time = time.time() - processing_start
                total_time = time.time() - start_time
                print(f"DEBUG: Processed {len(formatted_results)} results in {processing_time:.2f}s")
                print(f"DEBUG: Total search time: {total_time:.2f}s")
                return formatted_results
                
            except Exception as e:
                print(f"ERROR: Exception in perform_search: {str(e)}")
                print(f"ERROR: Exception type: {type(e)}")
                import traceback
                print(f"ERROR: Traceback: {traceback.format_exc()}")
                st.error(f"Search error: {str(e)}")
                return None


        def display_search_results(
            results: List[Dict],
            search_method: str,
            show_metadata: bool,
            show_scores: bool
        ):
            """Display search results in a tabular format with expandable details."""
            
            st.subheader(f"Search Results ({len(results)} found)")
            
            # Create DataFrame for tabular display
            df_data = []
            for i, result in enumerate(results):
                row = {
                    "Rank": i + 1,
                    "File": result["file_name"],
                    "Chunk": f"#{result['chunk_index']}",
                    "Preview": result["content"][:150] + "..." if len(result["content"]) > 150 else result["content"]
                }
                
                if show_scores:
                    row["Score"] = f"{result['score']:.3f}"
                    if search_method == "Hybrid Search":
                        row["Vector"] = f"{result.get('vector_score', 0):.3f}"
                        row["BM25"] = f"{result.get('bm25_score', 0):.3f}"
                
                df_data.append(row)
            
            # Display as interactive table
            df = pd.DataFrame(df_data)
            
            # Use container for better layout
            with st.container():
                # Display the dataframe
                st.dataframe(
                    df,
                    use_container_width=True,
                    hide_index=True,
                    column_config={
                        "Rank": st.column_config.NumberColumn(width="small"),
                        "File": st.column_config.TextColumn(width="medium"),
                        "Chunk": st.column_config.TextColumn(width="small"),
                        "Preview": st.column_config.TextColumn(width="large"),
                        "Score": st.column_config.NumberColumn(width="small", format="%.3f"),
                        "Vector": st.column_config.NumberColumn(width="small", format="%.3f"),
                        "BM25": st.column_config.NumberColumn(width="small", format="%.3f"),
                    }
                )
            
            # Expandable details for each result
            st.markdown("### Result Details")
            
            for i, result in enumerate(results):
                with st.expander(f"üîç Result {i + 1}: {result['file_name']} - Chunk #{result['chunk_index']}"):
                    col1, col2 = st.columns([3, 1])
                    
                    with col1:
                        st.markdown("**Full Content:**")
                        st.text_area(
                            "Content",
                            value=result["content"],
                            height=200,
                            disabled=True,
                            key=f"content_{i}"
                        )
                    
                    with col2:
                        if show_scores:
                            st.markdown("**Scores:**")
                            st.metric("Overall Score", f"{result['score']:.3f}")
                            
                            if search_method == "Hybrid Search":
                                st.metric("Vector Score", f"{result.get('vector_score', 0):.3f}")
                                st.metric("BM25 Score", f"{result.get('bm25_score', 0):.3f}")
                    
                    if show_metadata and result.get("metadata"):
                        st.markdown("**Metadata:**")
                        display_metadata_tree(result["metadata"], key_prefix=f"meta_{i}")
                    
                    # Action buttons
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        if st.button(f"üìã Copy to Clipboard", key=f"copy_{i}"):
                            # Note: Direct clipboard access requires JavaScript
                            st.info("Content copied! (Note: Use Ctrl+A, Ctrl+C to copy from text area above)")
                    
                    with col2:
                        if st.button(f"üí¨ Send to Chat", key=f"chat_{i}"):
                            # Store in session state for chat to pick up
                            if "selected_chunks" not in st.session_state:
                                st.session_state.selected_chunks = []
                            st.session_state.selected_chunks.append(result)
                            st.success("Added to chat context!")
                    
                    with col3:
                        if st.button(f"üìÑ View Full Document", key=f"doc_{i}"):
                            st.session_state.view_file_id = result["file_id"]
                            st.info("Switch to Files tab to view full document")


        def display_metadata_tree(metadata: Dict, key_prefix: str = "", level: int = 0):
            """Display metadata in a tree structure with collapsible sections."""
            
            for key, value in metadata.items():
                # Skip internal keys
                if key.startswith("_"):
                    continue
                
                indent = "  " * level
                
                if isinstance(value, dict):
                    # Nested dictionary - make it collapsible
                    with st.expander(f"{indent}üìÅ {key}", expanded=False):
                        display_metadata_tree(value, f"{key_prefix}_{key}", level + 1)
                elif isinstance(value, list):
                    # List - display as bullet points
                    st.markdown(f"{indent}**{key}:**")
                    for item in value:
                        st.markdown(f"{indent}  ‚Ä¢ {item}")
                else:
                    # Simple value
                    st.markdown(f"{indent}**{key}:** {value}")


        def render_search_statistics(rag_service: RAGService, project_id: int):
            """Render search statistics and analytics."""
            
            st.markdown("### üìä Search Analytics")
            
            # Get project statistics
            try:
                files = rag_service.list_files(project_id)
                
                col1, col2, col3, col4 = st.columns(4)
                
                with col1:
                    st.metric("Total Files", len(files))
                
                with col2:
                    # This would need to be implemented in the RAG service
                    st.metric("Total Chunks", "N/A")
                
                with col3:
                    st.metric("Avg Chunks/File", "N/A")
                
                with col4:
                    st.metric("Search Methods", "3")
                
            except Exception as e:
                st.error(f"Could not load statistics: {str(e)}")
      metadata:
        extension: .py
        size_bytes: 16182
        language: python
    src/chat/util/streaming_util.py:
      content: |-
        import asyncio
        from typing import AsyncGenerator, Callable, Optional, Dict, Any
        import logging

        # Use the application's logger
        from chat.util.logging_util import logger

        async def stream_response(
            client,
            messages: list,
            stream_options: Dict[str, Any],
            callback: Optional[Callable[[str], None]] = None
        ) -> AsyncGenerator[str, None]:
            """
            Stream a response from any LLM provider using LiteLLM.
            
            Args:
                client: The LiteLLM client
                messages: The messages to send
                stream_options: Additional options for the streaming call
                callback: Optional callback function to update UI
                
            Yields:
                Text chunks as they become available
            """
            try:
                # Set up streaming request with LiteLLM
                full_text = ""
                
                # Extract common options
                model = stream_options.get("model")
                max_tokens = stream_options.get("max_tokens", 4096)
                temperature = stream_options.get("temperature", 0.7)
                
                # Add system message if provided
                if "system" in stream_options and stream_options["system"]:
                    # Check if a system message already exists
                    if not any(msg.get("role") == "system" for msg in messages):
                        messages.insert(0, {"role": "system", "content": stream_options["system"]})
                
                logger.info(f"Starting streaming with model {model}, {len(messages)} messages")
                
                # Prepare parameters for LiteLLM
                completion_params = {
                    "model": model,
                    "messages": messages,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "stream": True  # Important: enable streaming
                }
                
                # Handle response_format if present (for OpenAI models)
                if "response_format" in stream_options:
                    completion_params["response_format"] = stream_options["response_format"]
                    logger.info(f"Using response_format: {stream_options['response_format']}")
                
                # Handle reasoning_effort if present (for some OpenAI models)
                # Note: gpt-4o doesn't support reasoning_effort in streaming mode
                if "reasoning_effort" in stream_options and not model.startswith("gpt-4o"):
                    completion_params["reasoning_effort"] = stream_options["reasoning_effort"]
                    # Add to allowed params for OpenAI
                    completion_params["allowed_openai_params"] = completion_params.get("allowed_openai_params", [])
                    if "reasoning_effort" not in completion_params["allowed_openai_params"]:
                        completion_params["allowed_openai_params"].append("reasoning_effort")
                    logger.info(f"Using reasoning_effort: {stream_options['reasoning_effort']}")
                elif "reasoning_effort" in stream_options:
                    logger.info(f"Skipping reasoning_effort parameter for {model} as it's not supported in streaming mode")
                
                # Use LiteLLM's streaming interface
                logger.info(f"Setting up streaming with LiteLLM for model: {model}")
                response = await client.acompletion(**completion_params)
                logger.info(f"Got streaming response object of type: {type(response)}")
                
                async for chunk in response:
                    if hasattr(chunk, 'choices') and chunk.choices:
                        delta = chunk.choices[0].delta
                        
                        # Extract content from delta
                        if hasattr(delta, 'content') and delta.content:
                            text_chunk = delta.content
                            full_text += text_chunk
                            
                            # Call the callback if provided
                            if callback:
                                callback(text_chunk)
                            
                            # Yield the chunk
                            yield text_chunk
                
                logger.info(f"Streaming completed, total length: {len(full_text)}")
                # In an async generator, we don't return a value
                    
            except Exception as e:
                error_msg = f"Error in LLM streaming: {str(e)}"
                logger.error(error_msg, exc_info=True)
                if callback:
                    callback(f"\nError: {str(e)}")
                yield f"\nError: {str(e)}"
      metadata:
        extension: .py
        size_bytes: 4188
        language: python
    src/chat/util/__init__.py:
      content: ''
      metadata:
        extension: .py
        size_bytes: 0
        language: python
    src/chat/util/logging_util.py:
      content: |+
        import logging

        # --- Logger Setup ---
        # Using standard Python logging
        logger = logging.getLogger(__name__) # Use __name__ for module-level logger
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

      metadata:
        extension: .py
        size_bytes: 251
        language: python
    src/chat/util/json_util.py:
      content: |
        import json
        import re
        from pathlib import Path
        from typing import Optional

        from chat.util.logging_util import logger


        class JsonUtil:
            """Utility class for handling JSON operations."""

            @staticmethod
            def extract_json(raw_json_str: str) -> dict:
                """
                Extract and parse JSON from a string that might contain additional text.

                Args:
                    raw_json_str: The raw string that contains JSON
                Returns:
                    Parsed JSON as a dictionary

                Raises:
                    json.JSONDecodeError: If JSON parsing fails
                """
                try:
                    # First try: direct parsing if it's already valid JSON
                    try:
                        return json.loads(raw_json_str)
                    except json.JSONDecodeError:
                        pass # Continue to next method if this fails

                    # Try removing markdown code block syntax
                    if raw_json_str.startswith('```json') and raw_json_str.endswith('```'):
                        try:
                            # Slice off ```json and ```
                            return json.loads(raw_json_str[7:-3].strip())
                        except json.JSONDecodeError:
                            pass # Continue
                    elif raw_json_str.startswith('```') and raw_json_str.endswith('```'):
                        try:
                            # Slice off ``` and ```
                            return json.loads(raw_json_str[3:-3].strip())
                        except json.JSONDecodeError:
                            pass # Continue


                    # Second try: Extract JSON from code blocks using regex
                    code_block_pattern = r"```(?:json)?\s*([\s\S]*?)\s*```"
                    code_blocks = re.findall(code_block_pattern, raw_json_str)

                    if code_blocks:
                        for block in code_blocks:
                            try:
                                return json.loads(block.strip())
                            except json.JSONDecodeError:
                                continue # Try next block if current one fails

                    # Third try: Special case for responses that start with ```json but might not have closing backticks
                    stripped_raw_str = raw_json_str.strip()
                    if stripped_raw_str.startswith("```json") or stripped_raw_str.startswith("```"):
                        # Extract everything after the opening backticks
                        json_content = re.sub(r"^```(?:json)?\s*", "", stripped_raw_str)
                        # Remove closing backticks if they exist at the very end of the content
                        json_content = re.sub(r"\s*```\s*$", "", json_content)
                        try:
                            return json.loads(json_content.strip())
                        except json.JSONDecodeError:
                            logger.debug("Failed to parse JSON from code block with potentially incomplete delimiters")


                    # Fourth try: Look for JSON object delimiters {} or array delimiters []
                    # Try to find the first '{' and last '}' to extract a JSON object
                    start_idx_obj = raw_json_str.find('{')
                    end_idx_obj = raw_json_str.rfind('}')

                    if start_idx_obj != -1 and end_idx_obj != -1 and end_idx_obj > start_idx_obj:
                        potential_json_obj = raw_json_str[start_idx_obj : end_idx_obj + 1]
                        try:
                            # Validate by trying to parse this substring
                            # More robustly find the actual end of this object
                            brace_count = 0
                            final_end_idx = -1
                            for i, char in enumerate(raw_json_str[start_idx_obj:], start=start_idx_obj):
                                if char == '{':
                                    brace_count += 1
                                elif char == '}':
                                    brace_count -= 1
                                    if brace_count == 0:
                                        final_end_idx = i
                                        break
                            if final_end_idx != -1:
                                json_candidate = raw_json_str[start_idx_obj : final_end_idx + 1]
                                return json.loads(json_candidate)
                        except json.JSONDecodeError:
                            pass # Object parsing failed, try array or give up

                    # Fifth try: Look for JSON array delimiters []
                    start_idx_arr = raw_json_str.find('[')
                    end_idx_arr = raw_json_str.rfind(']')

                    if start_idx_arr != -1 and end_idx_arr != -1 and end_idx_arr > start_idx_arr:
                        potential_json_arr = raw_json_str[start_idx_arr : end_idx_arr + 1]
                        try:
                             # Validate by trying to parse this substring (similar logic for arrays if needed)
                            bracket_count = 0
                            final_end_idx_arr = -1
                            for i, char in enumerate(raw_json_str[start_idx_arr:], start=start_idx_arr):
                                if char == '[':
                                    bracket_count += 1
                                elif char == ']':
                                    bracket_count -= 1
                                    if bracket_count == 0:
                                        final_end_idx_arr = i
                                        break
                            if final_end_idx_arr != -1:
                                json_candidate_arr = raw_json_str[start_idx_arr : final_end_idx_arr + 1]
                                return json.loads(json_candidate_arr)
                        except json.JSONDecodeError:
                            pass # Array parsing failed

                    # If we got here, all parsing attempts failed
                    logger.error(f"Could not find valid JSON in the response: {raw_json_str[:500]}...")
                    raise json.JSONDecodeError("Could not find valid JSON in the response", raw_json_str, 0)

                except Exception as e: # Catch any other unexpected error during extraction
                    logger.error(f"Unexpected error during JSON extraction: {e}", exc_info=True)
                    logger.error(f"Raw response snippet: {raw_json_str[:500]}...")
                    raise # Re-raise the original or a new error

            @staticmethod
            def extract_and_parse_json(raw_json_str: str, debug_file_path: Optional[Path] = None,
                                       debug_file_name: str = "raw_json_response.txt") -> dict:
                """
                Extract and parse JSON from a string that might contain additional text.
                Optionally saves the raw response for debugging.
                """
                if debug_file_path:
                    # Ensure debug_file_path is a Path object if it's a string
                    if isinstance(debug_file_path, str):
                        debug_file_path = Path(debug_file_path)

                    debug_file_path.mkdir(parents=True, exist_ok=True) # Ensure directory exists
                    raw_response_path = debug_file_path / debug_file_name
                    try:
                        with open(raw_response_path, "w", encoding="utf-8") as f:
                            f.write(raw_json_str)
                        logger.info(f"Raw response saved to {raw_response_path}")
                    except Exception as e:
                        logger.error(f"Failed to save raw response to {raw_response_path}: {e}")

                return JsonUtil.extract_json(raw_json_str)

            @staticmethod
            def format_json_schema(schema: dict, indent: int = 2) -> str:
                """
                Format a JSON schema in a more readable way.
                """
                try:
                    formatted_json = json.dumps(schema, indent=indent, sort_keys=False)
                    logger.debug("Successfully formatted JSON schema")
                    return formatted_json
                except Exception as e:
                    logger.error(f"Failed to format JSON schema: {e}")
                    return str(schema) # Fallback
      metadata:
        extension: .py
        size_bytes: 7540
        language: python
    src/chat/rag/rag_service.py:
      content: |-
        """
        RAG (Retrieval Augmented Generation) service for the chat application.
        This service handles document management, text chunking, and semantic search.
        """

        import logging
        import os
        from typing import Dict, List, Optional, Tuple, Union

        from vector_rag.api import VectorRAGAPI
        from vector_rag.config import Config as VectorRAGConfig
        from vector_rag.db import DBFileHandler
        from vector_rag.embeddings import SentenceTransformersEmbedder
        from vector_rag.model import Chunk, ChunkResult, ChunkResults, File, Project

        logger = logging.getLogger(__name__)


        class RAGService:
            """Service for managing RAG functionality."""

            def __init__(self, config_overrides: Optional[Dict] = None):
                """Initialize the RAG service.

                Args:
                    config_overrides: Optional overrides for the vector_rag configuration.
                """
                self.config = VectorRAGConfig(**(config_overrides or {}))
                logger.info(f"Initializing RAG service with config: {self.config.as_dict()}")

                self.api = VectorRAGAPI(config=self.config)
                self.handler = self.api.handler

                # Initialize current project
                self._current_project = None

            @property
            def current_project(self) -> Optional[Project]:
                """Get the currently selected project."""
                return self._current_project

            def get_or_create_project(self, name: str, description: Optional[str] = None) -> Project:
                """Get an existing project or create a new one.

                Args:
                    name: Name of the project
                    description: Optional description of the project

                Returns:
                    Project object
                """
                project = self.handler.get_or_create_project(name, description)
                self._current_project = project
                return project

            def list_projects(self) -> List[Project]:
                """List all available projects.

                Returns:
                    List of projects
                """
                return self.handler.get_projects()

            def add_document(self,
                             project_id: int,
                             filename: str,
                             content: str,
                             metadata: Optional[Dict] = None) -> Optional[File]:
                """Add a document to a project.

                Args:
                    project_id: ID of the project
                    filename: Name of the file
                    content: Content of the file
                    metadata: Optional metadata for the file

                Returns:
                    File object if successful, None otherwise
                """
                import hashlib

                # Create a unique path for the file
                file_path = f"/chat_app/documents/{filename}"

                # Generate CRC
                crc = hashlib.md5(content.encode()).hexdigest()

                # Create file model
                file_model = File(
                    name=filename,
                    path=file_path,
                    crc=crc,
                    content=content,
                    metadata=metadata or {}
                )

                # Add file to project
                return self.handler.add_file(project_id, file_model)

            def search(self,
                       project_id: int,
                       query: str,
                       top_k: int = 5,
                       similarity_threshold: float = 0.7) -> List[ChunkResult]:
                """Search for chunks matching the query.

                Args:
                    project_id: ID of the project to search
                    query: Search query text
                    top_k: Number of results to return
                    similarity_threshold: Minimum similarity score (0.0 to 1.0)

                Returns:
                    List of chunk results, sorted by similarity score
                """
                results = self.handler.search_chunks_by_text(
                    project_id=project_id,
                    query_text=query,
                    page=1,
                    page_size=top_k,
                    similarity_threshold=similarity_threshold
                )

                return results.results if results else []

            def get_context_for_query(self,
                                      project_id: int,
                                      query: str,
                                      max_chunks: int = 5,
                                      max_tokens: int = 1500) -> str:
                """Get context for a query as a formatted string.

                Args:
                    project_id: ID of the project to search
                    query: Search query text
                    max_chunks: Maximum number of chunks to include
                    max_tokens: Approximate maximum number of tokens to include

                Returns:
                    Formatted context string for the query
                """
                results = self.search(
                    project_id=project_id,
                    query=query,
                    top_k=max_chunks
                )

                if not results:
                    return ""

                context_parts = []
                total_length = 0
                approximate_token_ratio = 4  # Approximate characters per token
                max_chars = max_tokens * approximate_token_ratio

                for i, result in enumerate(results):
                    chunk = result.chunk
                    source = f"Document: {chunk.metadata.get('filename', 'Unknown')}"
                    text = chunk.content

                    # Skip if this would exceed our token budget
                    if total_length + len(text) > max_chars:
                        break

                    context_parts.append(f"[{i + 1}] {source}\n{text}\n")
                    total_length += len(text)

                return "\n".join(context_parts)

            def list_files(self, project_id: int) -> List[File]:
                """List all files in a project.

                Args:
                    project_id: ID of the project

                Returns:
                    List of files
                """
                return self.handler.list_files(project_id)
            
            def search_bm25(self,
                            project_id: int,
                            query: str,
                            top_k: int = 10,
                            rank_threshold: float = 0.0) -> ChunkResults:
                """Search using BM25 (keyword/lexical) matching.
                
                Args:
                    project_id: ID of the project to search
                    query: Search query text
                    top_k: Number of results to return
                    rank_threshold: Minimum BM25 rank score
                    
                Returns:
                    ChunkResults object with BM25-ranked results
                """
                return self.api.search_bm25(
                    project_id=project_id,
                    query_text=query,
                    page=1,
                    page_size=top_k,
                    rank_threshold=rank_threshold
                )
            
            def search_hybrid(self,
                              project_id: int,
                              query: str,
                              top_k: int = 10,
                              vector_weight: float = 0.5,
                              bm25_weight: float = 0.5,
                              similarity_threshold: float = 0.0,
                              rank_threshold: float = 0.0) -> ChunkResults:
                """Search using hybrid approach (vector + BM25).
                
                Args:
                    project_id: ID of the project to search
                    query: Search query text
                    top_k: Number of results to return
                    vector_weight: Weight for semantic similarity (0-1)
                    bm25_weight: Weight for BM25 score (0-1)
                    similarity_threshold: Minimum vector similarity score
                    rank_threshold: Minimum BM25 rank score
                    
                Returns:
                    ChunkResults object with hybrid-scored results
                """
                return self.api.search_hybrid(
                    project_id=project_id,
                    query_text=query,
                    page=1,
                    page_size=top_k,
                    vector_weight=vector_weight,
                    bm25_weight=bm25_weight,
                    similarity_threshold=similarity_threshold,
                    rank_threshold=rank_threshold
                )
      metadata:
        extension: .py
        size_bytes: 7547
        language: python
    src/chat/rag/__init__.py:
      content: |-
        """RAG (Retrieval Augmented Generation) module for the chat application."""

        from .rag_service import RAGService

        __all__ = ["RAGService"]
      metadata:
        extension: .py
        size_bytes: 138
        language: python
    src/chat/rag/rag_chat.py:
      content: |-
        """
        RAG integration with chat functionality.

        This module contains functions for integrating RAG with chat components,
        including context enhancement and result formatting.
        """
        import logging

        import streamlit as st
        from typing import Dict, List, Optional

        from chat.rag.rag_service import RAGService
        from chat.util.logging_util import logger as llm_logger


        def get_rag_context_for_prompt(prompt: str) -> Optional[str]:
            """
            Get RAG context for a prompt if RAG is enabled.

            Args:
                prompt: User prompt

            Returns:
                Enhanced prompt with RAG context if enabled, otherwise None
            """
            # Check if RAG is enabled
            if not st.session_state.get("rag_enabled", False):
                return None

            llm_logger.info("Getting RAG context for prompt...")
            
            # First check if user has manually selected chunks from search interface
            if st.session_state.get("selected_chunks"):
                llm_logger.info("Using manually selected chunks from search interface")
                context_parts = []
                for i, chunk_data in enumerate(st.session_state.selected_chunks):
                    filename = chunk_data.get("file_name", "Unknown")
                    content = chunk_data.get("content", "")
                    context_parts.append(f"[Document {i + 1}: {filename}]\n{content}\n")
                
                context_text = "\n".join(context_parts)
                
                # Clear selected chunks after use
                st.session_state.selected_chunks = []
                
                # Create enhanced prompt
                enhanced_prompt = (
                    f"I'll provide you with some context information followed by a question. "
                    f"Please use this context to help answer the question accurately.\n\n"
                    f"CONTEXT:\n{context_text}\n\n"
                    f"QUESTION: {prompt}\n\n"
                    f"Please answer based on the provided context. If the context doesn't contain "
                    f"relevant information, you can rely on your general knowledge, but prioritize "
                    f"the context information when available. Include citations like [Document X] in your answer."
                )
                
                llm_logger.info(f"Enhanced prompt with {len(st.session_state.get('selected_chunks', []))} manually selected chunks")
                return enhanced_prompt
            
            # Otherwise, use automatic search
            # Check if a project is selected
            project_id = st.session_state.get("current_rag_project_id")
            if not project_id:
                return None

            llm_logger.info(f"Using Project ID: {project_id}")

            try:
                # Get RAG service
                rag_service = st.session_state.rag_service

                # Get settings
                similarity_threshold = st.session_state.get("rag_similarity_threshold", 0.7)
                max_chunks = st.session_state.get("rag_max_chunks", 3)

                llm_logger.info(f"Using Similarity Threshold: {similarity_threshold} and Max Chunks: {max_chunks}")

                # Search for relevant context
                results = rag_service.search(
                    project_id=project_id,
                    query=prompt,
                    top_k=max_chunks,
                    similarity_threshold=similarity_threshold
                )

                if not results:
                    llm_logger.info("No relevant context found")
                    return None

                # Format context
                context_parts = []
                for i, result in enumerate(results):
                    chunk = result.chunk
                    metadata = chunk.metadata or {}
                    filename = metadata.get("filename", "Unknown")
                    content = chunk.content

                    context_parts.append(f"[Document {i + 1}: {filename}]\n{content}\n")

                context_text = "\n".join(context_parts)

                # Create enhanced prompt
                enhanced_prompt = (
                    f"I'll provide you with some context information followed by a question. "
                    f"Please use this context to help answer the question accurately.\n\n"
                    f"CONTEXT:\n{context_text}\n\n"
                    f"QUESTION: {prompt}\n\n"
                    f"Please answer based on the provided context. If the context doesn't contain "
                    f"relevant information, you can rely on your general knowledge, but prioritize "
                    f"the context information when available. Include citations like [Document X] in your answer."
                )

                llm_logger.info(f"Enhanced prompt with {len(results)} context chunks")
                return enhanced_prompt

            except Exception as e:
                llm_logger.error(f"Error getting RAG context: {str(e)}", exc_info=True)
                return None


        def format_rag_response(response: str) -> str:
            """
            Format a response from the LLM to highlight citations.

            Args:
                response: The response from the LLM

            Returns:
                Formatted response with highlighted citations
            """
            # Simple formatting for now - in a real implementation, you might want to
            # use regex to identify citations and style them
            return response
      metadata:
        extension: .py
        size_bytes: 4823
        language: python
    src/chat/rag/torch_fix.py:
      content: |-
        """Fix for PyTorch compatibility with Streamlit."""

        import os
        import sys


        def apply_torch_fix():
            """
            Apply fixes for PyTorch compatibility with Streamlit.

            This prevents errors related to torch.classes.__path__ when using
            sentence-transformers with Streamlit.
            """
            # Set environment variable to suppress PyTorch/Streamlit compatibility issues
            os.environ["PYTORCH_JIT"] = "0"

            # Try to filter out problematic modules from sys.modules
            if "torch._classes" in sys.modules:
                del sys.modules["torch._classes"]

            # Optionally, suppress PyTorch warnings
            import warnings
            warnings.filterwarnings("ignore", category=UserWarning, module="torch")

            # Additional workaround to prevent Streamlit from examining torch.classes
            if "torch.classes" in sys.modules:
                sys.modules["torch.classes"].__path__ = []
      metadata:
        extension: .py
        size_bytes: 857
        language: python
    src/chat/rag/chat_utils.py:
      content: |-
        """Utility functions for integrating RAG with chat functionality."""

        import logging
        from typing import Dict, List, Optional, Tuple

        from .rag_service import RAGService

        logger = logging.getLogger(__name__)


        def enhance_prompt_with_context(
                prompt: str,
                rag_service: RAGService,
                project_id: int,
                max_chunks: int = 3,
                similarity_threshold: float = 0.7
        ) -> str:
            """Enhance a user prompt with context from RAG.

            Args:
                prompt: Original user prompt
                rag_service: RAG service instance
                project_id: ID of the project to search
                max_chunks: Maximum number of chunks to include
                similarity_threshold: Minimum similarity score (0.0 to 1.0)

            Returns:
                Enhanced prompt with context
            """
            context = rag_service.get_context_for_query(
                project_id=project_id,
                query=prompt,
                max_chunks=max_chunks
            )

            if not context:
                return prompt

            # Create a prompt with context
            enhanced_prompt = (
                f"I'll provide you with some context information followed by a question. "
                f"Please use this context to help answer the question accurately.\n\n"
                f"CONTEXT:\n{context}\n\n"
                f"QUESTION: {prompt}\n\n"
                f"Please answer based on the provided context. If the context doesn't contain "
                f"relevant information, you can rely on your general knowledge, but prioritize "
                f"the context information when available."
            )

            return enhanced_prompt


        def format_citation(chunk_result: Dict) -> str:
            """Format a citation for a chunk result.

            Args:
                chunk_result: Dictionary with chunk result data

            Returns:
                Formatted citation string
            """
            metadata = chunk_result.get("metadata", {})
            filename = metadata.get("filename", "Unknown source")

            return f"[Source: {filename}]"
      metadata:
        extension: .py
        size_bytes: 1868
        language: python
    src/chat/ai/provider_manager.py:
      content: |-
        """
        Provider management module for the chat application.

        This module contains functions for initializing and managing LLM providers.
        """

        from typing import Dict, Any, Optional, Tuple

        from chat.ai.open_ai import OpenAIProvider
        from chat.ai.google_gemini import GoogleGeminiProvider
        from chat.ai.perplexity import PerplexityProvider
        from chat.ai.anthropic import AnthropicProvider
        from chat.ai.ollama import OllamaProvider
        from chat.ai.bedrock import BedrockProvider
        from chat.ai.llm_provider import LLMProvider
        from chat.util.logging_util import logger as llm_logger


        # Define available providers and their default models
        PROVIDERS = {
            "OpenAI": {
                "class": OpenAIProvider,
                "models": ["gpt-4o-2024-08-06", "gpt-4.1-2025-04-14",
                           "gpt-4o", "gpt-4.1", "o4-mini", "o3", "o3-mini",
                           "chatgpt-4o-latest"]
            },
            "Google Gemini": {
                "class": GoogleGeminiProvider,
                "models": ["gemini-2.5-pro-preview-05-06",
                           "gemini-2.0-flash-001", "gemini-2.0-flash-lite-001",
                           "gemini-2.5-flash-preview-04-17",
                           "gemini-2.0-flash-live-preview-04-09"]
            },
            "Perplexity": {
                "class": PerplexityProvider,
                "models": ["sonar-pro", "sonar", "sonar-deep-research",
                           "sonar-reasoning-pro", "sonar-reasoning", "r1-1776"
                           ]
            },
            "Anthropic": {
                "class": AnthropicProvider,
                "models": ["claude-3-7-sonnet-latest",
                           "claude-3-5-haiku-latest", "claude-3-opus-latest"]
            },
            "Ollama": {
                "class": OllamaProvider,
                "models": ["gemma3:27b", "qwen3:32b", "qwen:72b", "deepseek-r1:70b", "llama3.3:latest", "llama4:scout",
                           "mistral", "mixtral", "phi4:latest"]
            },
            "AWS Bedrock": {
                "class": BedrockProvider,
                "models": [
                            # Claude models with on-demand throughput support
                            "anthropic.claude-3-7-sonnet-20250219-v1:0",
                            "mistral.mistral-7b-instruct-v0:2",
                            "mistral.mixtral-8x7b-instruct-v0:1",
                            "mistral.mistral-large-2402-v1:0",
                            "cohere.embed-multilingual-v3",
                            "amazon.titan-embed-text-v2:0"
                           ]
            }
        }


        def initialize_provider(provider_name: str, model_name: str) -> Tuple[Optional[LLMProvider], Optional[str]]:
            """
            Initialize an LLM provider with the specified model.

            Args:
                provider_name: Name of the provider to initialize
                model_name: Name of the model to use

            Returns:
                Tuple containing (provider_instance, error_message)
                If initialization is successful, error_message will be None
                If initialization fails, provider_instance will be None and error_message will contain the error
            """
            try:
                # Get the provider class from the PROVIDERS dictionary
                provider_class = PROVIDERS[provider_name]["class"]

                # Initialize the provider with the selected model
                provider_instance = provider_class(model=model_name)

                llm_logger.info(f"Provider initialized: {provider_name} with model: {model_name}")
                return provider_instance, None

            except ValueError as e:
                error_message = f"Error initializing provider: {e}"
                llm_logger.error(error_message)
                return None, error_message

            except Exception as e:
                error_message = f"An unexpected error occurred during provider initialization: {e}"
                llm_logger.error(error_message, exc_info=True)
                return None, error_message


        def get_available_providers() -> Dict[str, Dict[str, Any]]:
            """
            Get the dictionary of available providers and their models.

            Returns:
                Dictionary of providers and their configuration
            """
            return PROVIDERS
      metadata:
        extension: .py
        size_bytes: 3835
        language: python
    src/chat/ai/perplexity.py:
      content: |-
        import os
        from typing import Optional, Dict, Any, AsyncGenerator, Callable
        import asyncio

        import litellm

        from chat.ai.llm_provider import LLMProvider
        from chat.conversation.conversation import Conversation, MessageType
        from chat.util.logging_util import logger
        from chat.util.streaming_util import stream_response


        class PerplexityProvider(LLMProvider):
            """Integration with Perplexity's research-focused LLM using LiteLLM."""

            def __init__(self, api_key: Optional[str] = None, model: str = "sonar-pro"):
                self.api_key = api_key or os.getenv("PERPLEXITY_API_KEY")
                if not self.api_key:
                    raise ValueError("Perplexity API key is required. Set it in .env or as an environment variable.")

                # Use Perplexity's online models for web search capabilities
                self.model = model

                # Ensure we're using an online model for search capabilities
                if not self._is_online_model(model):
                    logger.warning(
                        f"Model {model} may not have online search capabilities. Recommended models: sonar-pro, sonar-small-online")

                # Use LiteLLM's model naming convention for Perplexity
                if not model.startswith("perplexity/"):
                    self.model = f"perplexity/{model}"

                os.environ["PERPLEXITY_API_KEY"] = self.api_key

                try:
                    self.client = litellm
                    logger.info(f"PerplexityProvider initialized with model: {self.model}")
                except ImportError:
                    logger.error("litellm package not installed. Please install it (e.g., pip install litellm)")
                    raise

            def _is_online_model(self, model_name: str) -> bool:
                """Check if the model has online search capabilities."""
                online_indicators = ["online", "search", "sonar"]
                return any(indicator in model_name.lower() for indicator in online_indicators)

            async def generate_completion(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None
            ) -> str:
                """Generate a completion from Perplexity using LiteLLM."""
                options = options or {}
                max_tokens = options.get("max_tokens", 20000)
                temperature = options.get("temperature", 0.7)
                system_prompt = options.get("system_prompt",
                                            "You are a helpful assistant specializing in providing accurate and relevant information.")

                logger.info(f"Sending request to Perplexity with model: {self.model}")

                try:
                    # Create a properly formatted message list for Perplexity
                    # Always start with a system message
                    messages = [{"role": "system", "content": system_prompt}]
                    
                    # Then ensure strict user/assistant alternation
                    if conversation and conversation.messages:
                        # Get raw messages from conversation
                        raw_messages = []
                        for msg in conversation.messages:
                            role = "user" if msg.message_type == MessageType.INPUT else "assistant"
                            raw_messages.append({"role": role, "content": msg.content})
                        
                        # Ensure strict alternation
                        formatted_messages = []
                        expected_role = "user"  # First message after system should be user
                        
                        for msg in raw_messages:
                            if not formatted_messages:
                                # First message must be from user
                                if msg["role"] == "user":
                                    formatted_messages.append(msg)
                                    expected_role = "assistant"
                                # If first message is assistant, skip it (we'll add the prompt as user message later)
                            else:
                                # For subsequent messages, ensure alternation
                                if msg["role"] == expected_role:
                                    formatted_messages.append(msg)
                                    expected_role = "user" if expected_role == "assistant" else "assistant"
                                else:
                                    # If we have consecutive messages with the same role, combine their content
                                    if formatted_messages and formatted_messages[-1]["role"] == msg["role"]:
                                        formatted_messages[-1]["content"] += "\n\n" + msg["content"]
                        
                        # Add formatted messages to our message list
                        messages.extend(formatted_messages)
                        
                        # Ensure the last message is from user with the current prompt
                        if not formatted_messages or formatted_messages[-1]["role"] == "assistant":
                            # If last message was assistant or no messages, add the prompt as user
                            messages.append({"role": "user", "content": prompt})
                        elif formatted_messages[-1]["role"] == "user" and formatted_messages[-1]["content"] != prompt:
                            # If last message was user but with different content, add an assistant response and then the new prompt
                            messages.append({"role": "assistant", "content": "I understand. Please continue."})
                            messages.append({"role": "user", "content": prompt})
                    else:
                        # No conversation history, just add a user message with the prompt
                        messages.append({"role": "user", "content": prompt})

                    # Add detailed logging of the message sequence
                    logger.info(f"Using {len(messages)} messages in conversation history")
                    logger.info(f"Detailed message sequence:")
                    for i, msg in enumerate(messages):
                        logger.info(f"  Message {i}: role={msg['role']}, content_preview=\"{msg['content'][:50]}...\"")
                    
                    # Log the full sequence of roles for easier debugging
                    role_sequence = [msg['role'] for msg in messages]
                    logger.info(f"Role sequence: {role_sequence}")

                    # Validate message sequence before sending
                    self._validate_message_sequence(messages)

                    response = await self.client.acompletion(
                        model=self.model,
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=temperature
                    )

                    output = response.choices[0].message.content
                    reason = response.choices[0].finish_reason or "unknown"
                    logger.info(
                        f"Received response from Perplexity. Finish reason: {reason}, Output length: {len(output or '')}")

                    # Update conversation if provided
                    if conversation and output:
                        conversation.add_message(output, MessageType.OUTPUT)

                    return output or ""

                except Exception as e:
                    logger.error(f"Error generating completion from Perplexity via LiteLLM: {e}", exc_info=True)
                    # Log the message sequence that caused the error
                    if 'messages' in locals():
                        logger.error(f"Error occurred with this message sequence: {[msg['role'] for msg in messages]}")
                    raise

            def _validate_message_sequence(self, messages):
                """Validate that the message sequence follows Perplexity's requirements."""
                if not messages:
                    return
                    
                # First message(s) can be system
                i = 0
                while i < len(messages) and messages[i]["role"] == "system":
                    i += 1
                    
                # After system messages, roles must alternate between user and assistant
                if i < len(messages):
                    # First non-system message must be user
                    if messages[i]["role"] != "user":
                        raise ValueError("First message after system must be from user")
                        
                    expected_role = "assistant"
                    i += 1
                    
                    # Check remaining messages
                    while i < len(messages):
                        if messages[i]["role"] != expected_role:
                            raise ValueError(f"Expected {expected_role} message at position {i}, but got {messages[i]['role']}")
                        expected_role = "user" if expected_role == "assistant" else "assistant"
                        i += 1

            async def generate_json(
                    self,
                    prompt: str,
                    schema: Dict[str, Any],
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None
            ) -> Dict[str, Any]:
                """Generate a JSON response from Perplexity using the parent class implementation."""
                return await super().generate_json(prompt, schema, options, conversation)
                
            async def generate_completion_stream(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None,
                    callback: Optional[Callable[[str], None]] = None
            ) -> AsyncGenerator[str, None]:
                """Generate a streaming completion from Perplexity using LiteLLM."""
                options = options or {}
                max_tokens = options.get("max_tokens", 20000)
                temperature = options.get("temperature", 0.7)
                system_prompt = options.get("system_prompt",
                                           "You are a helpful assistant specializing in providing accurate and relevant information.")

                logger.info(f"Starting streaming request to Perplexity with model: {self.model}")

                try:
                    # Create a properly formatted message list for Perplexity
                    # Always start with a system message
                    messages = [{"role": "system", "content": system_prompt}]
                    
                    # Then ensure strict user/assistant alternation
                    if conversation and conversation.messages:
                        # Get raw messages from conversation
                        raw_messages = []
                        for msg in conversation.messages:
                            role = "user" if msg.message_type == MessageType.INPUT else "assistant"
                            raw_messages.append({"role": role, "content": msg.content})
                        
                        # Ensure strict alternation
                        formatted_messages = []
                        expected_role = "user"  # First message after system should be user
                        
                        for msg in raw_messages:
                            if not formatted_messages:
                                # First message must be from user
                                if msg["role"] == "user":
                                    formatted_messages.append(msg)
                                    expected_role = "assistant"
                                # If first message is assistant, skip it (we'll add the prompt as user message later)
                            else:
                                # For subsequent messages, ensure alternation
                                if msg["role"] == expected_role:
                                    formatted_messages.append(msg)
                                    expected_role = "user" if expected_role == "assistant" else "assistant"
                                else:
                                    # If we have consecutive messages with the same role, combine their content
                                    if formatted_messages and formatted_messages[-1]["role"] == msg["role"]:
                                        formatted_messages[-1]["content"] += "\n\n" + msg["content"]
                        
                        # Add formatted messages to our message list
                        messages.extend(formatted_messages)
                        
                        # Ensure the last message is from user with the current prompt
                        if not formatted_messages or formatted_messages[-1]["role"] == "assistant":
                            # If last message was assistant or no messages, add the prompt as user
                            messages.append({"role": "user", "content": prompt})
                        elif formatted_messages[-1]["role"] == "user" and formatted_messages[-1]["content"] != prompt:
                            # If last message was user but with different content, add an assistant response and then the new prompt
                            messages.append({"role": "assistant", "content": "I understand. Please continue."})
                            messages.append({"role": "user", "content": prompt})
                    else:
                        # No conversation history, just add a user message with the prompt
                        messages.append({"role": "user", "content": prompt})

                    # Log the message sequence
                    logger.info(f"Using {len(messages)} messages in conversation history for streaming")
                    role_sequence = [msg['role'] for msg in messages]
                    logger.info(f"Role sequence: {role_sequence}")

                    # Validate message sequence before sending
                    self._validate_message_sequence(messages)
                    
                    # Set up streaming options
                    stream_options = {
                        "model": self.model,
                        "max_tokens": max_tokens,
                        "temperature": temperature
                    }
                    
                    full_response = ""
                    # Use the streaming utility with LiteLLM
                    async for chunk in stream_response(
                        client=self.client,
                        messages=messages,
                        stream_options=stream_options,
                        callback=callback
                    ):
                        full_response += chunk
                        yield chunk
                    
                    # Update conversation with complete response
                    if conversation and full_response:
                        conversation.add_message(full_response, MessageType.OUTPUT)
                        
                except Exception as e:
                    error_msg = f"Error generating streaming completion from Perplexity: {e}"
                    logger.error(error_msg, exc_info=True)
                    # Log the message sequence that caused the error
                    if 'messages' in locals():
                        logger.error(f"Error occurred with this message sequence: {[msg['role'] for msg in messages]}")
                    if callback:
                        callback(f"\nError: {str(e)}")
                    yield f"\nError: {str(e)}"
      metadata:
        extension: .py
        size_bytes: 14217
        language: python
    src/chat/ai/__init__.py:
      content: ''
      metadata:
        extension: .py
        size_bytes: 0
        language: python
    src/chat/ai/google_gemini.py:
      content: |
        import os
        from typing import Optional, Dict, Any, AsyncGenerator, Callable
        import asyncio

        import litellm

        from chat.ai.llm_provider import LLMProvider
        from chat.conversation.conversation import Conversation, MessageType
        from chat.util.logging_util import logger
        from chat.util.streaming_util import stream_response


        class GoogleGeminiProvider(LLMProvider):
            """Integration with Google Gemini models using LiteLLM."""

            def __init__(self, api_key: Optional[str] = None, model: str = "gemini-2-flash"):
                self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
                if not self.api_key:
                    raise ValueError("Google API key is required. Set it in .env or as an environment variable.")

                # Use LiteLLM's model naming convention for Gemini
                self.model = model
                if not self.model.startswith("gemini/"):
                    self.model = f"gemini/{model}"

                os.environ["GEMINI_API_KEY"] = self.api_key

                try:
                    self.client = litellm
                    logger.info(f"GoogleGeminiProvider initialized with model: {self.model}")
                except ImportError:
                    logger.error("litellm package not installed. Please install it (e.g., pip install litellm)")
                    raise

            async def generate_completion(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None
            ) -> str:
                """Generate a completion from Gemini using LiteLLM."""
                options = options or {}
                max_tokens = options.get("max_tokens", 64000)
                temperature = options.get("temperature", 0.7)
                system_prompt = options.get("system_prompt",
                                            "You are a helpful assistant specializing in providing accurate and relevant information.")

                logger.info(f"Sending request to Gemini with model: {self.model}")

                try:
                    # Determine message format based on conversation history
                    if conversation and conversation.messages:
                        # Add the new prompt if it's not already the last user message
                        messages = conversation.to_llm_messages()

                        # Ensure a system message exists at the beginning
                        if not any(msg["role"] == "system" for msg in messages):
                            messages.insert(0, {"role": "system", "content": system_prompt})

                        # Add the new prompt if it's not already the last user message
                        if not (messages and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                            messages.append({"role": "user", "content": prompt})
                    else:
                        # Standard message format without conversation history
                        messages = [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": prompt}
                        ]

                    logger.info(f"Using {len(messages)} messages in conversation history")

                    response = await self.client.acompletion(
                        model=self.model,
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=temperature
                    )

                    output = response.choices[0].message.content
                    reason = response.choices[0].finish_reason or "unknown"
                    logger.info(f"Received response from Gemini. Finish reason: {reason}, Output length: {len(output or '')}")

                    # Update conversation if provided
                    if conversation and output:
                        conversation.add_message(output, MessageType.OUTPUT)

                    return output or ""

                except Exception as e:
                    logger.error(f"Error generating completion from Google Gemini via LiteLLM: {e}", exc_info=True)
                    raise

            async def generate_json(
                    self,
                    prompt: str,
                    schema: Dict[str, Any],
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None
            ) -> Dict[str, Any]:
                """Generate a JSON response from Gemini using the parent class implementation."""
                return await super().generate_json(prompt, schema, options, conversation)
                
            async def generate_completion_stream(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None,
                    callback: Optional[Callable[[str], None]] = None
            ) -> AsyncGenerator[str, None]:
                """Generate a streaming completion from Gemini using LiteLLM."""
                options = options or {}
                max_tokens = options.get("max_tokens", 64000)
                temperature = options.get("temperature", 0.7)
                system_prompt = options.get("system_prompt",
                                            "You are a helpful assistant specializing in providing accurate and relevant information.")

                logger.info(f"Starting streaming request to Gemini with model: {self.model}")
                
                # Determine message format based on conversation history
                if conversation and conversation.messages:
                    # Add the new prompt if it's not already the last user message
                    messages = conversation.to_llm_messages()

                    # Ensure a system message exists at the beginning
                    if not any(msg["role"] == "system" for msg in messages):
                        messages.insert(0, {"role": "system", "content": system_prompt})

                    # Add the new prompt if it's not already the last user message
                    if not (messages and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                        messages.append({"role": "user", "content": prompt})
                else:
                    # Standard message format without conversation history
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ]

                logger.info(f"Using {len(messages)} messages in conversation history for streaming")
                
                # Set up streaming options
                stream_options = {
                    "model": self.model,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "system": system_prompt
                }
                
                try:
                    full_response = ""
                    # Use the streaming utility with LiteLLM
                    async for chunk in stream_response(
                        client=self.client,
                        messages=messages,
                        stream_options=stream_options,
                        callback=callback
                    ):
                        full_response += chunk
                        yield chunk
                    
                    # Update conversation with complete response
                    if conversation and full_response:
                        conversation.add_message(full_response, MessageType.OUTPUT)
                        
                except Exception as e:
                    error_msg = f"Error generating streaming completion from Gemini via LiteLLM: {e}"
                    logger.error(error_msg, exc_info=True)
                    if callback:
                        callback(f"\nError: {str(e)}")
                    yield f"\nError: {str(e)}"
      metadata:
        extension: .py
        size_bytes: 7236
        language: python
    src/chat/ai/anthropic.py:
      content: |-
        import os
        from typing import Optional, Dict, Any, AsyncGenerator, Callable
        import asyncio

        import litellm

        from chat.ai.llm_provider import LLMProvider
        from chat.conversation.conversation import Conversation, MessageType
        from chat.util.logging_util import logger
        from chat.util.streaming_util import stream_response


        class AnthropicProvider(LLMProvider):
            """Integration with Anthropic Claude models using LiteLLM."""

            def __init__(self, api_key: Optional[str] = None, model: str = "claude-3-7-sonnet-latest"):
                self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
                if not self.api_key:
                    raise ValueError("Anthropic API key is required. Set it in .env or as an environment variable.")

                self.model = model
                self.original_model_name = model

                # Use LiteLLM's model naming convention for Anthropic
                if not self.model.startswith("anthropic/"):
                    self.model = f"anthropic/{model}"

                os.environ["ANTHROPIC_API_KEY"] = self.api_key

                try:
                    # Initialize litellm client
                    self.client = litellm
                    logger.info(f"AnthropicProvider initialized with model: {self.model}")
                except ImportError:
                    logger.error("litellm package not installed. Please install it (e.g., pip install litellm)")
                    raise

            async def generate_completion(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None
            ) -> str:
                """Generate a completion from Claude using LiteLLM."""
                options = options or {}
                
                # Set appropriate max_tokens based on the model
                if "haiku" in self.original_model_name.lower():
                    default_max_tokens = 8000  # Slightly below the 8192 limit for safety
                elif "sonnet" in self.original_model_name.lower():
                    default_max_tokens = 32000
                else:  # opus or other models
                    default_max_tokens = 64000
                    
                max_tokens = options.get("max_tokens", default_max_tokens)
                temperature = options.get("temperature", 0.7)
                system_prompt = options.get("system_prompt",
                                            "You are a helpful assistant specializing in providing accurate and relevant information.")

                logger.info(f"Sending request to Claude with model: {self.model}, max_tokens: {max_tokens}")

                try:
                    # Determine message format based on conversation history
                    if conversation and conversation.messages:
                        # Add the new prompt if it's not already the last user message
                        # This prevents duplicating the prompt if it was already added to the conversation
                        # by a previous call to this method
                        messages = conversation.to_llm_messages()

                        # Ensure a system message exists at the beginning
                        if not any(msg["role"] == "system" for msg in messages):
                            messages.insert(0, {"role": "system", "content": system_prompt})

                        # Add the new prompt if it's not already the last user message
                        if not (messages and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                            messages.append({"role": "user", "content": prompt})
                    else:
                        # Standard message format without conversation history
                        messages = [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": prompt}
                        ]

                    logger.info(f"Using {len(messages)} messages in conversation history")

                    response = await self.client.acompletion(
                        model=self.model,
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=temperature
                    )

                    output = response.choices[0].message.content
                    reason = response.choices[0].finish_reason or "unknown"
                    logger.info(f"Received response from Claude. Finish reason: {reason}, Output length: {len(output or '')}")

                    # Update conversation if provided
                    if conversation and output:
                        conversation.add_message(output, MessageType.OUTPUT)

                    return output or ""

                except Exception as e:
                    logger.error(f"Error generating completion from Anthropic via LiteLLM: {e}", exc_info=True)
                    raise

            async def generate_json(
                    self,
                    prompt: str,
                    schema: Dict[str, Any],
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None
            ) -> Dict[str, Any]:
                """Generate a JSON response from Claude using the parent class implementation."""
                return await super().generate_json(prompt, schema, options, conversation)
                
            async def generate_completion_stream(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None,
                    callback: Optional[Callable[[str], None]] = None
            ) -> AsyncGenerator[str, None]:
                """Generate a streaming completion from Claude using LiteLLM."""
                options = options or {}
                
                # Set up appropriate parameters
                system_prompt = options.get("system_prompt",
                    "You are a helpful assistant specializing in providing accurate information.")
                
                # Set appropriate max_tokens based on the model
                if "haiku" in self.original_model_name.lower():
                    default_max_tokens = 8000  # Slightly below the 8192 limit for safety
                elif "sonnet" in self.original_model_name.lower():
                    default_max_tokens = 32000
                else:  # opus or other models
                    default_max_tokens = 64000
                    
                max_tokens = options.get("max_tokens", default_max_tokens)
                temperature = options.get("temperature", 0.7)
                
                # Determine message format based on conversation history
                if conversation and conversation.messages:
                    # Add the new prompt if it's not already the last user message
                    messages = conversation.to_llm_messages()
                    
                    # Add the new prompt if it's not already the last user message
                    if not (messages and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                        messages.append({"role": "user", "content": prompt})
                else:
                    # Standard message format without conversation history
                    messages = [{"role": "user", "content": prompt}]
                    
                logger.info(f"Using {len(messages)} messages for streaming with model {self.model}")
                
                # Set up streaming options
                stream_options = {
                    "model": self.model,  # Use LiteLLM model format
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "system": system_prompt
                }
                
                try:
                    full_response = ""
                    # Use the streaming utility with LiteLLM
                    async for chunk in stream_response(
                        client=self.client,
                        messages=messages,
                        stream_options=stream_options,
                        callback=callback
                    ):
                        full_response += chunk
                        yield chunk
                    
                    # Update conversation with complete response
                    if conversation and full_response:
                        conversation.add_message(full_response, MessageType.OUTPUT)
                        
                except Exception as e:
                    error_msg = f"Error generating streaming completion from Claude via LiteLLM: {e}"
                    logger.error(error_msg, exc_info=True)
                    if callback:
                        callback(f"\nError: {str(e)}")
                    yield f"\nError: {str(e)}"
                    
            def _prepare_messages(self, prompt: str, conversation: Optional[Conversation], system_prompt: str) -> list:
                """Prepare messages for Claude API from conversation history and prompt."""
                # Convert conversation history to messages format expected by the Anthropic API
                if conversation and conversation.messages:
                    # Start with an empty list - system prompt is handled separately
                    messages = []
                    
                    # Loop through conversation messages and convert to Anthropic format
                    for msg in conversation.messages:
                        # Map message types to API roles
                        role = "user" if msg.message_type == MessageType.INPUT else "assistant"
                        messages.append({"role": role, "content": msg.content})
                    
                    # Add the new prompt if it's not already the last user message
                    if not (messages and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                        messages.append({"role": "user", "content": prompt})
                else:
                    # Standard message format without conversation history
                    messages = [{"role": "user", "content": prompt}]
                
                logger.info(f"Prepared {len(messages)} messages for Claude API")
                return messages
      metadata:
        extension: .py
        size_bytes: 9304
        language: python
    src/chat/ai/llm_provider.py:
      content: |-
        import json
        from typing import Optional, Dict, Any, AsyncGenerator, Callable
        from abc import ABC, abstractmethod

        # Assuming jsonschema and litellm are installed
        import jsonschema

        from chat.util.json_util import JsonUtil
        from chat.conversation.conversation import Conversation
        from chat.util.logging_util import logger


        class LLMProvider(ABC):
            """Abstract base class for LLM providers."""

            @abstractmethod
            async def generate_completion(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None
            ) -> str:
                """
                Generate a completion from the LLM for the given prompt.

                Args:
                    prompt: The user's input prompt
                    output_format: The desired output format (text or json)
                    options: Additional options for the completion
                    conversation: Optional conversation history to provide context

                Returns:
                    The generated completion as a string
                """
                pass

            async def generate_completion_stream(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None,
                    callback: Optional[Callable[[str], None]] = None
            ) -> AsyncGenerator[str, None]:
                """
                Generate a streaming completion from the LLM for the given prompt.
                
                Note: This method should be implemented by providers that support streaming.
                The default implementation raises NotImplementedError.

                Args:
                    prompt: The user's input prompt
                    output_format: The desired output format (text or json)
                    options: Additional options for the completion
                    conversation: Optional conversation history to provide context
                    callback: Optional callback function to update UI with each chunk

                Yields:
                    Text chunks as they become available
                """
                raise NotImplementedError("This provider does not support streaming completions")
                
            async def generate_json(
                    self,
                    prompt: str,
                    schema: Dict[str, Any],
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None
            ) -> Dict[str, Any]:
                """
                Generate a completion from the LLM for the given prompt, expecting JSON output matching the schema.

                Args:
                    prompt: The user's input prompt
                    schema: JSON schema that the response should conform to
                    options: Additional options for the completion
                    conversation: Optional conversation history to provide context

                Returns:
                    The generated completion as a validated JSON dictionary
                """
                options = options or {}  # Ensure options is a dict
                # Store schema in options if generate_completion needs it for specific modes
                options_with_schema = {**options, "schema": schema}

                json_str = await self.generate_completion(
                    prompt,
                    output_format="json_object",
                    options=options_with_schema,
                    conversation=conversation
                )

                # Use the JsonUtil from this module
                json_dict = JsonUtil.extract_json(json_str)
                try:
                    jsonschema.validate(instance=json_dict, schema=schema)
                except jsonschema.ValidationError as e:
                    logger.error(
                        f"JSON validation error: {e.message}\nSchema: {json.dumps(schema, indent=2)}\nJSON String: {json_str[:500]}...\nParsed Dict: {json_dict}")
                    raise ValueError(f"JSON validation error: {e.message}\nJSON received: {json_str}") from e
                return json_dict
      metadata:
        extension: .py
        size_bytes: 3845
        language: python
    src/chat/ai/ollama.py:
      content: |-
        import os
        from typing import Optional, Dict, Any, List, AsyncGenerator, Callable
        import asyncio

        import litellm

        from chat.ai.llm_provider import LLMProvider
        from chat.conversation.conversation import Conversation, MessageType
        from chat.util.logging_util import logger
        from chat.util.streaming_util import stream_response


        class OllamaProvider(LLMProvider):
            """Integration with Ollama models using LiteLLM."""

            def __init__(self, api_key: Optional[str] = None, model: str = "llama3.3:latest"):
                # Ollama doesn't require an API key, but we'll keep this parameter for consistency
                self.api_key = api_key

                # LiteLLM's naming convention for Ollama models depends on the model name format
                # If the model name contains a colon (e.g., "gemma3:27b"), we need to keep that
                # Otherwise, we use the format "ollama/model_name"
                self.original_model_name = model

                if ":" in model:
                    # Models with versions/variants like gemma3:27b should be formatted
                    # as ollama/gemma3:27b for LiteLLM
                    self.model = f"ollama/{model}"
                elif not model.startswith("ollama/"):
                    self.model = f"ollama/{model}"
                else:
                    self.model = model

                # Default Ollama base URL
                self.base_url = os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
                os.environ["OLLAMA_API_BASE"] = self.base_url

                try:
                    self.client = litellm
                    logger.info(f"OllamaProvider initialized with model: {self.model} at {self.base_url}")
                except ImportError:
                    logger.error("litellm package not installed. Please install it (e.g., pip install litellm)")
                    raise

            async def generate_completion(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None
            ) -> str:
                """Generate a completion from an Ollama model using LiteLLM."""
                options = options or {}

                # Adjust parameters based on model size and type
                if "70b" in self.original_model_name or "72b" in self.original_model_name:
                    default_max_tokens = 2048  # Most conservative for the largest models
                elif "27b" in self.original_model_name or "32b" in self.original_model_name:
                    default_max_tokens = 2560  # Conservative for medium-large models
                elif "llama4:scout" in self.original_model_name:
                    default_max_tokens = 4096  # Scout should be efficient enough for this
                else:
                    default_max_tokens = 4096  # Standard value for smaller models

                max_tokens = options.get("max_tokens", default_max_tokens)
                temperature = options.get("temperature", 0.7)
                system_prompt = options.get("system_prompt",
                                            "You are a helpful assistant specializing in providing accurate and relevant information.")

                logger.info(f"Sending request to Ollama with model: {self.model}")

                try:
                    # Convert conversation history to messages format expected by the LLM
                    if conversation and conversation.messages:
                        # Add the new prompt if it's not already the last user message
                        messages = conversation.to_llm_messages()

                        # Ensure a system message exists at the beginning
                        if not any(msg["role"] == "system" for msg in messages):
                            messages.insert(0, {"role": "system", "content": system_prompt})

                        # Add the new prompt if it's not already the last user message
                        if not (messages and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                            messages.append({"role": "user", "content": prompt})
                    else:
                        # Standard message format without conversation history
                        messages = [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": prompt}
                        ]

                    logger.info(f"Using {len(messages)} messages in conversation history")

                    # For very large models, we might need to limit context size
                    if len(messages) > 10:
                        if "70b" in self.original_model_name or "72b" in self.original_model_name:
                            logger.info(f"Trimming conversation history for large model ({self.original_model_name})")
                            # Keep system message and most recent messages
                            messages = [messages[0]] + messages[-7:]  # More aggressive trimming for largest models
                        elif "27b" in self.original_model_name or "32b" in self.original_model_name:
                            logger.info(f"Trimming conversation history for medium-large model ({self.original_model_name})")
                            # Keep system message and most recent messages
                            messages = [messages[0]] + messages[-9:]  # Less aggressive trimming

                    # Set up additional parameters for the request
                    extra_params = {}
                    if "stop_sequences" in options and options["stop_sequences"]:
                        extra_params["stop"] = options["stop_sequences"]

                    # Add timeout parameters based on model size
                    if "70b" in self.original_model_name or "72b" in self.original_model_name:
                        extra_params["request_timeout"] = 180  # 3 minute timeout for largest models
                    elif "27b" in self.original_model_name or "32b" in self.original_model_name:
                        extra_params["request_timeout"] = 120  # 2 minute timeout for medium-large models
                    else:
                        extra_params["request_timeout"] = 60  # 1 minute timeout for standard models

                    # Get context window size if set in session state
                    import streamlit as st
                    if "ollama_context_size" in st.session_state and (
                            "70b" in self.original_model_name or
                            "72b" in self.original_model_name or
                            "27b" in self.original_model_name or
                            "32b" in self.original_model_name
                    ):
                        # Adjust max_tokens based on user-set context size
                        max_tokens = min(max_tokens, st.session_state.ollama_context_size)
                        logger.info(f"Using user-defined context size: {max_tokens}")

                    response = await self.client.acompletion(
                        model=self.model,
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        **extra_params
                    )

                    output = response.choices[0].message.content
                    reason = response.choices[0].finish_reason or "unknown"
                    logger.info(f"Received response from Ollama. Finish reason: {reason}, Output length: {len(output or '')}")

                    # Update conversation if provided
                    if conversation and output:
                        conversation.add_message(output, MessageType.OUTPUT)

                    return output or ""

                except Exception as e:
                    logger.error(f"Error generating completion from Ollama via LiteLLM: {e}", exc_info=True)
                    if "connection" in str(e).lower() and "refused" in str(e).lower():
                        return "Error: Could not connect to Ollama server. Please ensure Ollama is running and the base URL is correct."
                    elif "model not found" in str(e).lower():
                        return f"Error: Model '{self.original_model_name}' not found. Please make sure you've pulled this model using 'ollama pull {self.original_model_name}'."
                    elif "timeout" in str(e).lower():
                        return f"Error: Request timed out. The model '{self.original_model_name}' might be loading or require more resources than available."
                    elif "out of memory" in str(e).lower() or "oom" in str(e).lower():
                        return f"Error: Out of memory error. The model '{self.original_model_name}' requires more RAM than currently available. Try reducing the context size in the settings."
                    else:
                        raise

            async def generate_json(
                    self,
                    prompt: str,
                    schema: Dict[str, Any],
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None
            ) -> Dict[str, Any]:
                """Generate a JSON response from Ollama using the parent class implementation."""
                return await super().generate_json(prompt, schema, options, conversation)
                
            async def generate_completion_stream(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None,
                    callback: Optional[Callable[[str], None]] = None
            ) -> AsyncGenerator[str, None]:
                """Generate a streaming completion from Ollama using LiteLLM."""
                options = options or {}

                # Adjust parameters based on model size and type
                if "70b" in self.original_model_name or "72b" in self.original_model_name:
                    default_max_tokens = 2048  # Most conservative for the largest models
                elif "27b" in self.original_model_name or "32b" in self.original_model_name:
                    default_max_tokens = 2560  # Conservative for medium-large models
                elif "llama4:scout" in self.original_model_name:
                    default_max_tokens = 4096  # Scout should be efficient enough for this
                else:
                    default_max_tokens = 4096  # Standard value for smaller models

                max_tokens = options.get("max_tokens", default_max_tokens)
                temperature = options.get("temperature", 0.7)
                system_prompt = options.get("system_prompt",
                                           "You are a helpful assistant specializing in providing accurate and relevant information.")

                logger.info(f"Starting streaming request to Ollama with model: {self.model}")

                # Convert conversation history to messages format expected by the LLM
                if conversation and conversation.messages:
                    # Add the new prompt if it's not already the last user message
                    messages = conversation.to_llm_messages()

                    # Ensure a system message exists at the beginning
                    if not any(msg["role"] == "system" for msg in messages):
                        messages.insert(0, {"role": "system", "content": system_prompt})

                    # Add the new prompt if it's not already the last user message
                    if not (messages and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                        messages.append({"role": "user", "content": prompt})
                else:
                    # Standard message format without conversation history
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ]

                logger.info(f"Using {len(messages)} messages in conversation history for streaming")

                # For very large models, we might need to limit context size
                if len(messages) > 10:
                    if "70b" in self.original_model_name or "72b" in self.original_model_name:
                        logger.info(f"Trimming conversation history for large model ({self.original_model_name})")
                        # Keep system message and most recent messages
                        messages = [messages[0]] + messages[-7:]  # More aggressive trimming for largest models
                    elif "27b" in self.original_model_name or "32b" in self.original_model_name:
                        logger.info(f"Trimming conversation history for medium-large model ({self.original_model_name})")
                        # Keep system message and most recent messages
                        messages = [messages[0]] + messages[-9:]  # Less aggressive trimming

                # Set up streaming options
                stream_options = {
                    "model": self.model,
                    "max_tokens": max_tokens,
                    "temperature": temperature
                }
                
                # Add stop sequences if provided
                if "stop_sequences" in options and options["stop_sequences"]:
                    stream_options["stop"] = options["stop_sequences"]
                    
                # Get context window size if set in session state
                import streamlit as st
                if "ollama_context_size" in st.session_state and (
                        "70b" in self.original_model_name or
                        "72b" in self.original_model_name or
                        "27b" in self.original_model_name or
                        "32b" in self.original_model_name
                ):
                    # Adjust max_tokens based on user-set context size
                    stream_options["max_tokens"] = min(max_tokens, st.session_state.ollama_context_size)
                    logger.info(f"Using user-defined context size: {stream_options['max_tokens']}")
                    
                try:
                    full_response = ""
                    # Use the streaming utility with LiteLLM
                    async for chunk in stream_response(
                        client=self.client,
                        messages=messages,
                        stream_options=stream_options,
                        callback=callback
                    ):
                        full_response += chunk
                        yield chunk
                    
                    # Update conversation with complete response
                    if conversation and full_response:
                        conversation.add_message(full_response, MessageType.OUTPUT)
                        
                except Exception as e:
                    error_msg = f"Error generating streaming completion from Ollama: {e}"
                    logger.error(error_msg, exc_info=True)
                    
                    # Provide specific error messages based on the exception
                    error_response = None
                    if "connection" in str(e).lower() and "refused" in str(e).lower():
                        error_response = "Error: Could not connect to Ollama server. Please ensure Ollama is running and the base URL is correct."
                    elif "model not found" in str(e).lower():
                        error_response = f"Error: Model '{self.original_model_name}' not found. Please make sure you've pulled this model using 'ollama pull {self.original_model_name}'."
                    elif "timeout" in str(e).lower():
                        error_response = f"Error: Request timed out. The model '{self.original_model_name}' might be loading or require more resources than available."
                    elif "out of memory" in str(e).lower() or "oom" in str(e).lower():
                        error_response = f"Error: Out of memory error. The model '{self.original_model_name}' requires more RAM than currently available. Try reducing the context size in the settings."
                    else:
                        error_response = f"Error: {str(e)}"
                        
                    if callback:
                        callback(f"\n{error_response}")
                    yield f"\n{error_response}"
      metadata:
        extension: .py
        size_bytes: 14890
        language: python
    src/chat/ai/bedrock.py:
      content: |-
        # src/chat/ai/bedrock.py
        import os
        import json
        import uuid
        from typing import Optional, Dict, Any, AsyncGenerator, Callable
        import boto3
        from botocore.exceptions import ClientError
        import asyncio
        import litellm

        from chat.ai.llm_provider import LLMProvider
        from chat.conversation.conversation import Conversation, MessageType
        from chat.util.logging_util import logger
        from chat.util.streaming_util import stream_response


        class BedrockProvider(LLMProvider):
            """Integration with Amazon Bedrock foundation models using both direct API and LiteLLM."""

            # The full ARN of the inference profile from your AWS console
            CLAUDE_INFERENCE_PROFILE_ARN = "arn:aws:bedrock:us-west-2:043309360196:inference-profile/us.anthropic.claude-3-7-sonnet-20250219-v1:0"

            def __init__(self, api_key: Optional[str] = None, model: str = "anthropic.claude-3-sonnet-20240229-v1:0",
                         inference_profile: Optional[str] = None):
                # For Bedrock, we don't use the api_key parameter directly, but use AWS credentials
                self.aws_access_key_id = os.getenv("AWS_ACCESS_KEY_ID")
                self.aws_secret_access_key = os.getenv("AWS_SECRET_ACCESS_KEY")
                self.aws_session_token = os.getenv("AWS_SESSION_TOKEN")  # Optional session token for temp credentials
                self.aws_region = os.getenv("AWS_REGION", "us-west-2")  # Default to us-west-2

                if not self.aws_access_key_id or not self.aws_secret_access_key:
                    raise ValueError("AWS credentials are required. Set AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY in .env")

                # Store the original model name
                self.original_model = model

                # Initialize boto3 client for direct API calls
                bedrock_client_kwargs = {
                    "service_name": "bedrock-runtime",
                    "region_name": self.aws_region,
                    "aws_access_key_id": self.aws_access_key_id,
                    "aws_secret_access_key": self.aws_secret_access_key
                }

                # Add session token if available
                if self.aws_session_token:
                    bedrock_client_kwargs["aws_session_token"] = self.aws_session_token
                    logger.info("Using AWS session token for temporary credentials")

                self.bedrock_runtime = boto3.client(**bedrock_client_kwargs)
                logger.info(f"Direct boto3 bedrock-runtime client initialized for region {self.aws_region}")

                # Set the inference profile ARN
                self.inference_profile = inference_profile or self.CLAUDE_INFERENCE_PROFILE_ARN

                # Special handling for Claude 3.7 - we'll use direct boto3 calls
                self.use_direct_api = model == "anthropic.claude-3-7-sonnet-20250219-v1:0"

                if self.use_direct_api:
                    logger.info(f"Using direct boto3 API for Claude 3.7 model with inference profile: {self.inference_profile}")
                    self.model = model  # Store original model ID for direct API
                else:
                    # For non-Claude 3.7 models, set up LiteLLM
                    if not model.startswith("bedrock/"):
                        self.model = f"bedrock/{model}"
                    else:
                        self.model = model

                    # Set environment variables for LiteLLM to use
                    os.environ["AWS_ACCESS_KEY_ID"] = self.aws_access_key_id
                    os.environ["AWS_SECRET_ACCESS_KEY"] = self.aws_secret_access_key
                    os.environ["AWS_REGION_NAME"] = self.aws_region

                    if self.aws_session_token:
                        os.environ["AWS_SESSION_TOKEN"] = self.aws_session_token

                    # Initialize LiteLLM client for non-Claude 3.7 models
                    self.client = litellm
                    logger.info(f"LiteLLM initialized for regular model: {self.model}")

            async def generate_completion(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None
            ) -> str:
                """Generate a completion from AWS Bedrock models."""
                options = options or {}

                # Add the new user message to the conversation if provided
                if conversation:
                    conversation.add_message(prompt, MessageType.INPUT)

                # Default system prompt
                system_prompt = options.get("system_prompt",
                                            "You are a helpful assistant specializing in technical writing and software engineering.")

                # Determine max tokens based on model
                if "claude" in self.model.lower() or "claude" in self.original_model.lower():
                    if "claude-3-7" in self.model.lower() or "claude-3-7" in self.original_model.lower():
                        max_tokens = options.get("max_tokens", 32000)  # Claude 3.7 has larger context
                    elif "opus" in self.model.lower():
                        max_tokens = options.get("max_tokens", 16000)
                    elif "sonnet" in self.model.lower():
                        max_tokens = options.get("max_tokens", 8000)
                    elif "haiku" in self.model.lower():
                        max_tokens = options.get("max_tokens", 4000)
                    else:
                        max_tokens = options.get("max_tokens", 4000)  # Default for older Claude models
                elif "llama" in self.model.lower():
                    max_tokens = options.get("max_tokens", 4096)  # Llama models
                else:
                    max_tokens = options.get("max_tokens", 1024)  # Default for other models

                # Get temperature parameter with a reasonable default
                temperature = options.get("temperature", 0.7)

                # Prepare the messages
                if conversation and conversation.messages:
                    # Convert conversation history to messages format
                    messages = conversation.to_llm_messages()

                    # Add the new prompt as a user message if not already present
                    if not (messages and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                        messages.append({"role": "user", "content": prompt})

                    # Ensure the system prompt is set
                    if not any(msg["role"] == "system" for msg in messages):
                        messages.insert(0, {"role": "system", "content": system_prompt})
                else:
                    # Standard message format without conversation history
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ]

                # Decide which API to use
                if self.use_direct_api:
                    return await self._generate_with_boto3(
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        output_format=output_format,
                        conversation=conversation
                    )
                else:
                    return await self._generate_with_litellm(
                        messages=messages,
                        max_tokens=max_tokens,
                        temperature=temperature,
                        output_format=output_format,
                        conversation=conversation
                    )

            async def _generate_with_litellm(
                    self,
                    messages: list,
                    max_tokens: int,
                    temperature: float,
                    output_format: str,
                    conversation: Optional[Conversation] = None
            ) -> str:
                """Generate completion using LiteLLM (for regular models)."""
                llm_params = {
                    "model": self.model,
                    "messages": messages,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "aws_region_name": self.aws_region,
                }

                # Handle response format for models that support it
                if output_format.lower() == "json" and "claude" in self.model.lower():
                    llm_params["response_format"] = {"type": "json_object"}
                    logger.info("Using JSON response format")

                logger.info(f"Sending request to AWS Bedrock ({self.model}) with LiteLLM")
                logger.info(f"Using {len(messages)} messages in conversation history")

                # Make the API call
                response = await self.client.acompletion(**llm_params)

                output = response.choices[0].message.content
                reason = response.choices[0].finish_reason or "unknown"
                logger.info(f"Received response from Bedrock. Finish reason: {reason}, "
                            f"Output length: {len(output or '')}")

                # Update conversation if provided
                if conversation and output:
                    conversation.add_message(output, MessageType.OUTPUT)

                return output or ""

            async def _generate_with_boto3(
                    self,
                    messages: list,
                    max_tokens: int,
                    temperature: float,
                    output_format: str,
                    conversation: Optional[Conversation] = None
            ) -> str:
                """Generate completion using direct boto3 API for Claude 3.7."""
                # Extract system message
                system_content = None
                anthropic_messages = []

                for msg in messages:
                    if msg["role"] == "system":
                        system_content = msg["content"]
                    else:
                        anthropic_messages.append({
                            "role": msg["role"],
                            "content": [{"type": "text", "text": msg["content"]}]
                        })

                # Create request payload
                request_body = {
                    "anthropic_version": "bedrock-2023-05-31",
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "messages": anthropic_messages
                }

                # Add system message if present
                if system_content:
                    request_body["system"] = system_content

                # Add response format for JSON if needed
                if output_format.lower() == "json":
                    request_body["response_format"] = {"type": "json_object"}
                    logger.info("Using JSON response format for direct API call")

                # Convert to JSON and encode
                body_bytes = json.dumps(request_body).encode('utf-8')

                logger.info(f"Sending direct API request to Bedrock for Claude 3.7")
                logger.info(
                    f"Request includes {len(anthropic_messages)} messages and system prompt: {system_content is not None}")

                try:
                    # Make the API call with the body containing our whole request
                    # Note: No inferenceProfileArn parameter here - we'll directly use the inference profile
                    # as the modelId according to AWS documentation
                    response = self.bedrock_runtime.invoke_model(
                        modelId=self.inference_profile,  # Use the inference profile ARN as the modelId
                        body=body_bytes,
                        accept="application/json",
                        contentType="application/json"
                    )

                    # Parse the response body
                    response_body = json.loads(response['body'].read())

                    # Extract the text from the response
                    if 'content' in response_body and len(response_body['content']) > 0:
                        output = response_body['content'][0]['text']
                        logger.info(f"Received response from direct Bedrock API, length: {len(output)}")

                        # Update conversation if provided
                        if conversation and output:
                            conversation.add_message(output, MessageType.OUTPUT)

                        return output
                    else:
                        logger.warning(f"Unexpected response format: {response_body}")
                        return "Error: Unexpected response format from AWS Bedrock"

                except (ClientError, Exception) as e:
                    # Log the error details
                    logger.error(f"Bedrock error: {str(e)}")

                    # Check for specific error types
                    if hasattr(e, 'response') and 'Error' in getattr(e, 'response', {}):
                        error_code = e.response['Error'].get('Code', 'Unknown')
                        error_message = e.response['Error'].get('Message', str(e))
                        logger.error(f"AWS Error {error_code}: {error_message}")

                        if "AccessDenied" in error_code:
                            raise ValueError(f"Access denied. Please check your AWS credentials and permissions.")
                        elif "ValidationException" in error_code:
                            raise ValueError(f"Validation error: {error_message}")
                        elif "ResourceNotFoundException" in error_code:
                            raise ValueError(f"Resource not found: {error_message}")

                    # Generic error
                    raise ValueError(f"AWS Bedrock error: {str(e)}")

            async def generate_completion_stream(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None,
                    callback: Optional[Callable[[str], None]] = None
            ) -> AsyncGenerator[str, None]:
                """Generate a streaming completion from AWS Bedrock models."""
                # For Claude 3.7, we'll use non-streaming approach and return in one go
                if self.use_direct_api:
                    logger.info(f"Using non-streaming approach for Claude 3.7 model with direct boto3")

                    try:
                        # Generate the full response
                        full_response = await self.generate_completion(
                            prompt=prompt,
                            output_format=output_format,
                            options=options,
                            conversation=conversation
                        )

                        # If we have a callback, call it with the entire output
                        if callback:
                            callback(full_response)

                        # Yield the entire output
                        yield full_response

                        return

                    except Exception as e:
                        error_message = str(e)
                        logger.error(f"Error in non-streaming boto3 approach: {error_message}")

                        if callback:
                            callback(f"\nError: {error_message}")
                        yield f"\nError: {error_message}"

                        return

                # For other models, use standard LiteLLM streaming
                options = options or {}

                # Add the new user message to the conversation if provided
                if conversation:
                    conversation.add_message(prompt, MessageType.INPUT)

                # Default system prompt
                system_prompt = options.get("system_prompt",
                                            "You are a helpful assistant specializing in technical writing and software engineering.")

                # Determine max tokens based on model
                if "claude" in self.model.lower() or "claude" in self.original_model.lower():
                    if "claude-3-7" in self.model.lower() or "claude-3-7" in self.original_model.lower():
                        max_tokens = options.get("max_tokens", 32000)  # Claude 3.7 has larger context
                    elif "opus" in self.model.lower():
                        max_tokens = options.get("max_tokens", 16000)
                    elif "sonnet" in self.model.lower():
                        max_tokens = options.get("max_tokens", 8000)
                    elif "haiku" in self.model.lower():
                        max_tokens = options.get("max_tokens", 4000)
                    else:
                        max_tokens = options.get("max_tokens", 4000)  # Default for older Claude models
                elif "llama" in self.model.lower():
                    max_tokens = options.get("max_tokens", 4096)  # Llama models
                else:
                    max_tokens = options.get("max_tokens", 1024)  # Default for other models

                # Get temperature parameter with a reasonable default
                temperature = options.get("temperature", 0.7)

                # Prepare messages
                if conversation and conversation.messages:
                    # Convert conversation history to messages format
                    messages = conversation.to_llm_messages()

                    # Add the new prompt as a user message if not already present
                    if not (messages and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                        messages.append({"role": "user", "content": prompt})

                    # Ensure the system prompt is set
                    if not any(msg["role"] == "system" for msg in messages):
                        messages.insert(0, {"role": "system", "content": system_prompt})
                else:
                    # Standard message format without conversation history
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ]

                # Set up streaming options
                stream_options = {
                    "model": self.model,
                    "max_tokens": max_tokens,
                    "temperature": temperature,
                    "aws_region_name": self.aws_region,
                }

                # Handle response format for models that support it
                if output_format.lower() == "json" and "claude" in self.model.lower():
                    stream_options["response_format"] = {"type": "json_object"}
                    logger.info("Using JSON response format for streaming")

                logger.info(f"Starting streaming request to AWS Bedrock with model: {self.model}")
                logger.info(f"Using {len(messages)} messages in conversation history for streaming")

                try:
                    full_response = ""
                    # Use the streaming utility with LiteLLM
                    async for chunk in stream_response(
                            client=self.client,
                            messages=messages,
                            stream_options=stream_options,
                            callback=callback
                    ):
                        full_response += chunk
                        yield chunk

                    # Update conversation with complete response
                    if conversation and full_response:
                        conversation.add_message(full_response, MessageType.OUTPUT)

                    logger.info("Streaming completed successfully")

                except Exception as e:
                    error_message = str(e)
                    logger.error(f"Error in streaming: {error_message}")

                    # Create a user-friendly error message
                    if "You don't have access to the model" in error_message:
                        error_msg = f"Access denied to model: {self.model}. Check your AWS credentials and permissions."
                    elif "Streaming not supported for" in error_message:
                        error_msg = f"Streaming is not supported for this model. Try using non-streaming mode instead."
                    elif "failed to authenticate request" in error_message.lower():
                        error_msg = "Authentication failed. Please check your AWS credentials and region configuration."
                    else:
                        error_msg = f"Error during streaming: {e}"

                    if callback:
                        callback(f"\n{error_msg}")
                    yield f"\n{error_msg}"
      metadata:
        extension: .py
        size_bytes: 18572
        language: python
    src/chat/ai/open_ai.py:
      content: |-
        import os
        from typing import Optional, Dict, Any, Literal, AsyncGenerator, Callable
        import asyncio

        import litellm

        from chat.ai.llm_provider import LLMProvider
        from chat.conversation.conversation import Conversation, MessageType
        from chat.util.logging_util import logger
        from chat.util.streaming_util import stream_response


        class OpenAIProvider(LLMProvider):
            """Integration with OpenAI GPT models using LiteLLM."""

            def __init__(self, api_key: Optional[str] = None, model: str = "gpt-4o-2024-08-06"):
                self.api_key = api_key or os.getenv("OPENAI_API_KEY")
                if not self.api_key:
                    raise ValueError("OpenAI API key is required. Set it in .env or as an environment variable.")

                self.model = model
                os.environ["OPENAI_API_KEY"] = self.api_key  # LiteLLM expects this

                try:
                    self.client = litellm
                    logger.info(f"OpenAIProvider initialized with model: {self.model}")
                except ImportError:
                    logger.error("litellm package not installed. Please install it (e.g., pip install litellm)")
                    raise

            async def _generate_completion_gpt4_series(
                    self,
                    prompt: str,
                    response_format: Dict[str, Any],
                    options: Optional[Dict[str, Any]] = None,  # Made options optional
                    conversation: Optional[Conversation] = None
            ) -> str:
                options = options or {}

                if self.model.startswith("gpt-4.1"):  # Matches "gpt-4.1-2025-04-14"
                    max_tokens = options.get("max_tokens", 32768)
                elif self.model.startswith("gpt-4o"):
                    max_tokens = options.get("max_tokens", 4096)
                else:
                    max_tokens = options.get("max_tokens", 16384)  # Default for other gpt-4

                temperature = options.get("temperature", 0.5)
                reasoning_effort = self.get_reasoning_effort(options)
                system_prompt = options.get("system_prompt",
                                            "You are a helpful assistant specializing in technical writing and software engineering.")

                try:
                    # Prepare the messages - either from conversation history or create new
                    if conversation and conversation.messages:
                        # Convert conversation history to messages format
                        messages = conversation.to_llm_messages()

                        # Add the new prompt as a user message if not already present
                        if not (messages and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                            messages.append({"role": "user", "content": prompt})

                        # Ensure the system prompt is set
                        if not any(msg["role"] == "system" for msg in messages):
                            messages.insert(0, {"role": "system", "content": system_prompt})
                    else:
                        # Standard message format without conversation history
                        messages = [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": prompt}
                        ]

                    llm_params: Dict[str, Any] = {  # Explicitly type llm_params
                        "model": self.model,
                        "response_format": response_format,
                        "max_tokens": max_tokens,
                        "temperature": temperature,
                        "messages": messages
                    }

                    llm_params["allowed_openai_params"] = llm_params.get("allowed_openai_params", [])

                    if self._supports_reasoning_effort(self.model) and not self.model.startswith("gpt-4o"):
                        llm_params["reasoning_effort"] = reasoning_effort
                        if "reasoning_effort" not in llm_params["allowed_openai_params"]:
                            llm_params["allowed_openai_params"].append("reasoning_effort")

                    # Log params without messages for brevity
                    loggable_params = {k: v for k, v in llm_params.items() if k != "messages"}
                    logger.info(f"Sending request to LiteLLM (GPT-4 series) with params: {loggable_params}")
                    logger.info(f"Using {len(messages)} messages in conversation history")

                    response = await self.client.acompletion(**llm_params)

                    output = response.choices[0].message.content
                    reason = response.choices[0].finish_reason or "unknown"
                    logger.info(
                        f"Received response (GPT-4 series). Finish reason: {reason}, Output length: {len(output or '')}")

                    # Update conversation if provided
                    if conversation and output:
                        conversation.add_message(output, MessageType.OUTPUT)

                    if reason == "stop":
                        return output or ""  # Ensure string return
                    elif reason == "length":
                        logger.info("Output truncated due to length (GPT-4 series). Attempting to continue.")
                        return await self._generate_continue(prompt, output or "", options, llm_params,
                                                             conversation=conversation)
                    else:  # Includes other reasons like 'tool_calls', 'content_filter', etc.
                        logger.warning(f"Unexpected finish_reason (GPT-4 series): {reason}. Returning output as is.")
                        return output or ""

                except Exception as e:
                    logger.error(f"Error generating completion from OpenAI {self.model} (GPT-4 series) via LiteLLM: {e}",
                                 exc_info=True)
                    raise

            async def _generate_completion_o_series(
                    self,
                    prompt: str,
                    response_format: Dict[str, Any],
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None
            ) -> str:
                options = options or {}
                # Your class uses 'max_completion_tokens' for o-series
                # LiteLLM typically uses 'max_tokens' for OpenAI models.
                # We'll use 'max_tokens' in the call to LiteLLM but source it from 'max_completion_tokens' if present.
                if self.model.startswith("gpt-4o"):
                    max_completion_tokens = options.get("max_completion_tokens", options.get("max_tokens", 16384))
                else:
                    max_completion_tokens = options.get("max_completion_tokens", options.get("max_tokens", 100000))

                reasoning_effort = self.get_reasoning_effort(options)
                system_prompt = options.get("system_prompt",
                                            "You are a helpful assistant specializing in technical writing and software engineering.")

                try:
                    # Prepare the messages - either from conversation history or create new
                    if conversation and conversation.messages:
                        # Convert conversation history to messages format
                        messages = conversation.to_llm_messages()

                        # Add the new prompt as a user message if not already present
                        if not (messages and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                            messages.append({"role": "user", "content": prompt})

                        # Ensure the system prompt is set
                        if not any(msg["role"] == "system" for msg in messages):
                            messages.insert(0, {"role": "system", "content": system_prompt})
                    else:
                        # Standard message format without conversation history
                        messages = [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": prompt}
                        ]

                    llm_params: Dict[str, Any] = {  # Explicitly type llm_params
                        "model": self.model,
                        "response_format": response_format,
                        "max_tokens": max_completion_tokens,  # This is what LiteLLM's acompletion expects for OpenAI
                        "messages": messages
                    }

                    # Handle temperature for O-series models
                    is_o_series = "o" in self.model.lower()
                    if is_o_series:
                        # O-series models only support temperature=1.0
                        llm_params["temperature"] = 1.0
                        logger.info("Using temperature=1.0 for O-series model (only supported value)")
                    elif "temperature" in options:
                        # For non-O-series models, use the provided temperature
                        llm_params["temperature"] = options.get("temperature", 0.5)

                    llm_params["allowed_openai_params"] = llm_params.get("allowed_openai_params", [])

                    if self._supports_reasoning_effort(self.model) and not self.model.startswith("gpt-4o"):
                        llm_params["reasoning_effort"] = reasoning_effort
                        if "reasoning_effort" not in llm_params["allowed_openai_params"]:
                            llm_params["allowed_openai_params"].append("reasoning_effort")

                    loggable_params = {k: v for k, v in llm_params.items() if k != "messages"}
                    logger.info(f"Sending request to LiteLLM (o-series) with params: {loggable_params}")
                    logger.info(f"Using {len(messages)} messages in conversation history")

                    response = await self.client.acompletion(**llm_params)

                    output = response.choices[0].message.content
                    reason = response.choices[0].finish_reason or "unknown"
                    logger.info(f"Received response (o-series). Finish reason: {reason}, Output length: {len(output or '')}")

                    # Update conversation if provided
                    if conversation and output:
                        conversation.add_message(output, MessageType.OUTPUT)

                    if reason == "stop":
                        return output or ""
                    elif reason == "length":
                        logger.info("Output truncated due to length (o-series). Attempting to continue.")
                        return await self._generate_continue(prompt, output or "", options, llm_params,
                                                             conversation=conversation)
                    else:
                        logger.warning(f"Unexpected finish_reason (o-series): {reason}. Returning output as is.")
                        return output or ""

                except Exception as e:
                    logger.error(f"Error generating completion from OpenAI {self.model} (o-series) via LiteLLM: {e}",
                                 exc_info=True)
                    raise

            @staticmethod
            def _supports_reasoning_effort(model: str) -> bool:
                return model.startswith("gpt-4o") or model.startswith("o3")

            async def generate_completion(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None
            ) -> str:
                options = options or {}
                response_format = await self._create_response_format(options, output_format)

                # Add the new user message to the conversation if provided
                if conversation:
                    conversation.add_message(prompt, MessageType.INPUT)

                if self.model.startswith("o") or self.model.startswith("gpt-4o"):  # Matching gpt-4o to o-series logic
                    logger.info(f"Routing to o-series completion for model {self.model}")
                    return await self._generate_completion_o_series(prompt, response_format, options, conversation)
                else:  # Handles gpt-4, gpt-4.1, etc.
                    logger.info(f"Routing to GPT-4 series completion for model {self.model}")
                    return await self._generate_completion_gpt4_series(prompt, response_format, options, conversation)

            @staticmethod
            async def _create_response_format(options: Dict[str, Any], output_format: str) -> Dict[str, Any]:
                normalized_format = "text"  # Default for chat
                if output_format.lower() == "json_object" or (
                        output_format.lower() == "json" and "schema" in options):  # More specific for json_object
                    normalized_format = "json_object"
                elif output_format.lower() == "json":  # Simple JSON, could be text if schema not present
                    normalized_format = "json_object"  # Assume intent is JSON mode

                response_format_dict = {"type": normalized_format}

                logger.info(f"Response format created: {response_format_dict} for input format '{output_format}'")
                return response_format_dict

            # generate_json is inherited from LLMProvider ABC and uses the above generate_completion

            async def _generate_continue(
                    self,
                    original_prompt: str,
                    current_output: str,
                    options: Dict[str, Any],
                    llm_params: Dict[str, Any],  # Original params from the first call
                    max_calls: int = 3,
                    conversation: Optional[Conversation] = None
            ) -> str:
                accumulated_output = current_output
                continuation_instruction = "Continue exactly where you left off, providing the next part of the response. Do not repeat any part of the previous response. Start directly with the new content."

                # Start with a copy of the original messages from the first call
                messages_history = list(llm_params.get("messages", []))
                # Add the assistant's partial response so far
                messages_history.append({"role": "assistant", "content": current_output})
                # Add the user's instruction to continue
                messages_history.append({"role": "user", "content": continuation_instruction})

                # Determine max_tokens for continuation based on original params
                # This should be the max_tokens for *each continuation chunk*, not the total.
                # The original llm_params['max_tokens'] or llm_params['max_completion_tokens']
                # was for the first chunk. We can reuse it for subsequent chunks.
                continue_max_tokens = llm_params.get("max_tokens") or llm_params.get("max_completion_tokens")

                for attempt in range(1, max_calls + 1):
                    try:
                        logger.info(
                            f"Continuation attempt {attempt}/{max_calls}. Current accumulated length: {len(accumulated_output)}")

                        continuation_api_params = {
                            "model": self.model,
                            "messages": messages_history,
                            "max_tokens": continue_max_tokens,  # Max tokens for this specific continuation call
                            "temperature": llm_params.get("temperature"),
                            "response_format": llm_params.get("response_format"),  # Preserve original response format
                            "allowed_openai_params": llm_params.get("allowed_openai_params", [])
                        }
                        if "reasoning_effort" in llm_params and self._supports_reasoning_effort(self.model):
                            continuation_api_params["reasoning_effort"] = llm_params["reasoning_effort"]

                        response = await self.client.acompletion(**continuation_api_params)

                        new_chunk = response.choices[0].message.content or ""  # Ensure it's a string
                        finish_reason = response.choices[0].finish_reason or "unknown"
                        logger.info(
                            f"Continuation attempt {attempt} - Finish reason: {finish_reason}, New chunk length: {len(new_chunk)}")

                        accumulated_output += new_chunk

                        # Note: we don't add continuation chunks to the conversation history
                        # as they're part of the same logical message

                        if finish_reason == "stop":
                            logger.info("Continuation successful, finish_reason is 'stop'.")
                            return accumulated_output
                        elif finish_reason == "length":
                            if not new_chunk:  # Safety break if it returns empty but says length
                                logger.warning(f"Attempt {attempt}: finish_reason 'length' but no new content. Stopping.")
                                return accumulated_output
                            logger.warning(
                                f"Attempt {attempt}: output cut off again during continuation. Preparing next attempt...")
                            messages_history.append({"role": "assistant", "content": new_chunk})
                            messages_history.append({"role": "user", "content": continuation_instruction})
                        else:
                            logger.warning(
                                f"Attempt {attempt}: Unexpected finish_reason during continuation: {finish_reason}. Stopping.")
                            return accumulated_output

                    except Exception as e:
                        logger.error(f"Error continuing generation at attempt {attempt}: {e}", exc_info=True)
                        return accumulated_output  # Return what we have so far on error

                logger.warning("Maximum continuation attempts reached. Returning accumulated output.")
                return accumulated_output

            @staticmethod
            def get_reasoning_effort(options: Dict[str, Any]) -> Literal["low", "medium", "high"]:
                reasoning_effort_value = options.get("reasoning_effort", "high")
                if reasoning_effort_value not in ["low", "medium", "high"]:
                    logger.warning(f"Invalid reasoning_effort value '{reasoning_effort_value}'. Defaulting to 'high'.")
                    reasoning_effort_value = "high"
                return reasoning_effort_value  # type: ignore
            
            async def generate_completion_stream(
                    self,
                    prompt: str,
                    output_format: str = "text",
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None,
                    callback: Optional[Callable[[str], None]] = None
            ) -> AsyncGenerator[str, None]:
                """Generate a streaming completion from OpenAI using LiteLLM."""
                options = options or {}
                response_format = await self._create_response_format(options, output_format)
                
                # Add the new user message to the conversation if provided
                if conversation:
                    conversation.add_message(prompt, MessageType.INPUT)
                
                # Prepare streaming based on model type and set up async generator properly
                if self.model.startswith("o") or self.model.startswith("gpt-4o"):
                    # Use async for to properly yield from the inner generator
                    generator = self._generate_completion_stream_o_series(prompt, response_format, options, conversation, callback)
                    async for chunk in generator:
                        yield chunk
                else:
                    # Use async for to properly yield from the inner generator
                    generator = self._generate_completion_stream_gpt4_series(prompt, response_format, options, conversation, callback)
                    async for chunk in generator:
                        yield chunk
            
            async def _generate_completion_stream_gpt4_series(
                    self,
                    prompt: str,
                    response_format: Dict[str, Any],
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None,
                    callback: Optional[Callable[[str], None]] = None
            ) -> AsyncGenerator[str, None]:
                """Stream completions for GPT-4 series models."""
                options = options or {}
                
                if self.model.startswith("gpt-4.1"):  # Matches "gpt-4.1-2025-04-14"
                    max_tokens = options.get("max_tokens", 32768)
                elif self.model.startswith("gpt-4o"):
                    max_tokens = options.get("max_tokens", 4096)
                else:
                    max_tokens = options.get("max_tokens", 16384)  # Default for other gpt-4
                    
                temperature = options.get("temperature", 0.5)
                reasoning_effort = self.get_reasoning_effort(options)
                system_prompt = options.get("system_prompt",
                                           "You are a helpful assistant specializing in technical writing and software engineering.")
                
                logger.info(f"Starting streaming request to GPT-4 series model: {self.model}")
                
                # Prepare messages
                if conversation and conversation.messages:
                    # Convert conversation history to messages format
                    messages = conversation.to_llm_messages()
                    
                    # Add the new prompt as a user message if not already present
                    if not (messages and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                        messages.append({"role": "user", "content": prompt})
                    
                    # Ensure the system prompt is set
                    if not any(msg["role"] == "system" for msg in messages):
                        messages.insert(0, {"role": "system", "content": system_prompt})
                else:
                    # Standard message format without conversation history
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ]
                    
                # Set up streaming options - don't include response_format directly
                stream_options = {
                    "model": self.model,
                    "max_tokens": max_tokens,
                    "temperature": temperature
                }
                
                # Handle response format differently for streaming
                if response_format:
                    if response_format.get("type") == "json_object":
                        stream_options["response_format"] = {"type": "json_object"}
                
                # Add reasoning effort if supported
                if self._supports_reasoning_effort(self.model) and not self.model.startswith("gpt-4o"):
                    stream_options["reasoning_effort"] = reasoning_effort
                
                logger.info(f"Using {len(messages)} messages in conversation history for streaming")
                
                try:
                    full_response = ""
                    # Use the streaming utility with LiteLLM
                    async for chunk in stream_response(
                        client=self.client,
                        messages=messages,
                        stream_options=stream_options,
                        callback=callback
                    ):
                        full_response += chunk
                        yield chunk
                    
                    # Update conversation with complete response
                    if conversation and full_response:
                        conversation.add_message(full_response, MessageType.OUTPUT)
                        
                except Exception as e:
                    error_msg = f"Error generating streaming completion from OpenAI GPT-4 series: {e}"
                    logger.error(error_msg, exc_info=True)
                    if callback:
                        callback(f"\nError: {str(e)}")
                    yield f"\nError: {str(e)}"
            
            async def _generate_completion_stream_o_series(
                    self,
                    prompt: str,
                    response_format: Dict[str, Any],
                    options: Optional[Dict[str, Any]] = None,
                    conversation: Optional[Conversation] = None,
                    callback: Optional[Callable[[str], None]] = None
            ) -> AsyncGenerator[str, None]:
                """Stream completions for O-series models."""
                options = options or {}
                
                if self.model.startswith("gpt-4o"):
                    max_completion_tokens = options.get("max_completion_tokens", options.get("max_tokens", 16384))
                else:
                    max_completion_tokens = options.get("max_completion_tokens", options.get("max_tokens", 100000))
                    
                reasoning_effort = self.get_reasoning_effort(options)
                system_prompt = options.get("system_prompt",
                                            "You are a helpful assistant specializing in technical writing and software engineering.")
                
                logger.info(f"Starting streaming request to O-series model: {self.model}")
                
                # Prepare messages
                if conversation and conversation.messages:
                    # Convert conversation history to messages format
                    messages = conversation.to_llm_messages()
                    
                    # Add the new prompt as a user message if not already present
                    if not (messages and messages[-1]["role"] == "user" and messages[-1]["content"] == prompt):
                        messages.append({"role": "user", "content": prompt})
                    
                    # Ensure the system prompt is set
                    if not any(msg["role"] == "system" for msg in messages):
                        messages.insert(0, {"role": "system", "content": system_prompt})
                else:
                    # Standard message format without conversation history
                    messages = [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ]
                    
                # Set up streaming options - don't include response_format directly
                stream_options = {
                    "model": self.model,
                    "max_tokens": max_completion_tokens,
                    "temperature": 1.0  # O-series only supports temperature=1.0
                }
                
                # Handle response format differently for streaming
                if response_format:
                    if response_format.get("type") == "json_object":
                        stream_options["response_format"] = {"type": "json_object"}
                
                # Add reasoning effort if supported (note: not supported for streaming with gpt-4o)
                if self._supports_reasoning_effort(self.model) and not self.model.startswith("gpt-4o"):
                    stream_options["reasoning_effort"] = reasoning_effort
                elif self._supports_reasoning_effort(self.model):
                    logger.info(f"Skipping reasoning_effort parameter for {self.model} as it's not supported in streaming mode")
                    
                logger.info(f"Using {len(messages)} messages in conversation history for streaming")
                logger.info(f"Using temperature=1.0 for O-series model (only supported value)")
                
                try:
                    full_response = ""
                    # Use the streaming utility with LiteLLM
                    async for chunk in stream_response(
                        client=self.client,
                        messages=messages,
                        stream_options=stream_options,
                        callback=callback
                    ):
                        full_response += chunk
                        yield chunk
                    
                    # Update conversation with complete response
                    if conversation and full_response:
                        conversation.add_message(full_response, MessageType.OUTPUT)
                        
                except Exception as e:
                    error_msg = f"Error generating streaming completion from OpenAI O-series: {e}"
                    logger.error(error_msg, exc_info=True)
                    if callback:
                        callback(f"\nError: {str(e)}")
                    yield f"\nError: {str(e)}"
      metadata:
        extension: .py
        size_bytes: 26310
        language: python
    src/chat/conversation/conversation.py:
      content: |-
        from datetime import datetime
        from enum import Enum
        from typing import List, Optional
        from pydantic import BaseModel, Field


        class MessageType(str, Enum):
            """Enum for message types."""
            INPUT = "input"
            OUTPUT = "output"


        class Message(BaseModel):
            """A model for individual messages in a conversation."""
            timestamp: datetime = Field(default_factory=datetime.now)
            message_type: MessageType
            content: str
            role: str = Field(default="user")  # Default role for backward compatibility with LLM APIs

            def to_llm_message(self) -> dict:
                """Convert to a format suitable for LLM API calls."""
                role = "user" if self.message_type == MessageType.INPUT else "assistant"
                return {
                    "role": role,
                    "content": self.content
                }


        class Conversation(BaseModel):
            """A model for storing conversation history."""
            id: str
            title: Optional[str] = None
            messages: List[Message] = Field(default_factory=list)
            created_at: datetime = Field(default_factory=datetime.now)
            updated_at: datetime = Field(default_factory=datetime.now)

            def add_message(self, content: str, message_type: MessageType, role: Optional[str] = None) -> Message:
                """Add a new message to the conversation."""
                message = Message(
                    timestamp=datetime.now(),
                    message_type=message_type,
                    content=content,
                    role=role or ("user" if message_type == MessageType.INPUT else "assistant")
                )
                self.messages.append(message)
                self.updated_at = datetime.now()
                return message

            def to_llm_messages(self) -> List[dict]:
                """Convert conversation history to a format suitable for LLM APIs."""
                return [msg.to_llm_message() for msg in self.messages]

            def ensure_alternating_messages(self) -> List[dict]:
                """Ensure that messages alternate between user and assistant roles."""
                result = []
                last_role = None

                for msg in self.messages:
                    current_role = "user" if msg.message_type == MessageType.INPUT else "assistant"

                    # If this role is the same as the last one, combine them
                    if current_role == last_role and result:
                        result[-1]["content"] += "\n\n" + msg.content
                    else:
                        # Otherwise add as a new message
                        result.append({
                            "role": current_role,
                            "content": msg.content
                        })

                    last_role = current_role

                return result

            class Config:
                arbitrary_types_allowed = True
      metadata:
        extension: .py
        size_bytes: 2605
        language: python
    src/chat/conversation/__init__.py:
      content: ''
      metadata:
        extension: .py
        size_bytes: 0
        language: python
    src/chat/conversation/conversation_storage.py:
      content: |-
        import json
        import os
        from datetime import datetime
        from pathlib import Path
        from typing import Dict, List, Optional, Union

        from chat.conversation.conversation import Conversation, MessageType
        from chat.util.logging_util import logger


        class ConversationStorage:
            """Utility class for storing and retrieving conversations."""

            def __init__(self, storage_dir: Union[str, Path] = "conversations"):
                """
                Initialize the conversation storage.

                Args:
                    storage_dir: Directory to store conversation files
                """
                self.storage_dir = Path(storage_dir)
                self.storage_dir.mkdir(parents=True, exist_ok=True)
                logger.info(f"Initialized ConversationStorage in directory: {self.storage_dir}")

            def save_conversation(self, conversation: Conversation) -> bool:
                """
                Save a conversation to a JSON file.

                Args:
                    conversation: The conversation to save

                Returns:
                    True if successful, False otherwise
                """
                try:
                    # Ensure directory exists
                    self.storage_dir.mkdir(parents=True, exist_ok=True)

                    # Prepare file path
                    file_path = self.storage_dir / f"{conversation.id}.json"

                    # Convert to serializable format
                    conversation_dict = conversation.dict()

                    # Convert datetime objects to strings for JSON serialization
                    conversation_dict['created_at'] = conversation_dict['created_at'].isoformat()
                    conversation_dict['updated_at'] = conversation_dict['updated_at'].isoformat()

                    for msg in conversation_dict['messages']:
                        msg['timestamp'] = msg['timestamp'].isoformat()

                    # Write to file
                    with open(file_path, 'w', encoding='utf-8') as f:
                        json.dump(conversation_dict, f, indent=2, ensure_ascii=False)

                    logger.info(f"Saved conversation {conversation.id} to {file_path}")
                    return True

                except Exception as e:
                    logger.error(f"Failed to save conversation {conversation.id}: {e}", exc_info=True)
                    return False

            def load_conversation(self, conversation_id: str) -> Optional[Conversation]:
                """
                Load a conversation from a JSON file.

                Args:
                    conversation_id: The ID of the conversation to load

                Returns:
                    The loaded conversation or None if not found
                """
                try:
                    file_path = self.storage_dir / f"{conversation_id}.json"

                    if not file_path.exists():
                        logger.warning(f"Conversation file not found: {file_path}")
                        return None

                    with open(file_path, 'r', encoding='utf-8') as f:
                        conversation_dict = json.load(f)

                    # Convert string timestamps back to datetime objects
                    conversation_dict['created_at'] = datetime.fromisoformat(conversation_dict['created_at'])
                    conversation_dict['updated_at'] = datetime.fromisoformat(conversation_dict['updated_at'])

                    # Convert message timestamps
                    for msg in conversation_dict['messages']:
                        msg['timestamp'] = datetime.fromisoformat(msg['timestamp'])

                    # Recreate the Conversation object
                    conversation = Conversation(**conversation_dict)
                    logger.info(f"Loaded conversation {conversation_id} from {file_path}")

                    return conversation

                except Exception as e:
                    logger.error(f"Error loading conversation {conversation_id}: {e}", exc_info=True)
                    return None

            def delete_conversation(self, conversation_id: str) -> bool:
                """
                Delete a conversation file.

                Args:
                    conversation_id: The ID of the conversation to delete

                Returns:
                    True if successful, False otherwise
                """
                try:
                    file_path = self.storage_dir / f"{conversation_id}.json"

                    if not file_path.exists():
                        logger.warning(f"Cannot delete: Conversation file not found: {file_path}")
                        return False

                    os.remove(file_path)
                    logger.info(f"Deleted conversation {conversation_id} from {file_path}")
                    return True

                except Exception as e:
                    logger.error(f"Error deleting conversation {conversation_id}: {e}", exc_info=True)
                    return False

            def list_conversations(self) -> List[Dict]:
                """
                List all available conversations with metadata.

                Returns:
                    List of conversation metadata dictionaries
                """
                try:
                    conversations = []

                    # Check if directory exists
                    if not self.storage_dir.exists():
                        logger.warning(f"Storage directory {self.storage_dir} does not exist")
                        # Create it
                        self.storage_dir.mkdir(parents=True, exist_ok=True)
                        return []

                    for file_path in self.storage_dir.glob("*.json"):
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                data = json.load(f)

                            # Extract basic metadata
                            conversations.append({
                                'id': data['id'],
                                'title': data.get('title', 'Untitled Conversation'),
                                'created_at': data['created_at'],
                                'updated_at': data['updated_at'],
                                'message_count': len(data['messages'])
                            })
                        except Exception as e:
                            logger.error(f"Error reading conversation file {file_path}: {e}")

                    # Sort by updated_at (most recent first)
                    conversations.sort(key=lambda x: x['updated_at'], reverse=True)

                    logger.info(f"Listed {len(conversations)} conversations")
                    return conversations

                except Exception as e:
                    logger.error(f"Error listing conversations: {e}", exc_info=True)
                    return []

            def generate_conversation_title(self, conversation: Conversation, max_length: int = 50) -> str:
                """
                Generate a title for a conversation based on its content.

                Args:
                    conversation: The conversation to generate a title for
                    max_length: Maximum length of the title

                Returns:
                    A generated title
                """
                # Find the first user message to use as the title
                for msg in conversation.messages:
                    if msg.message_type == MessageType.INPUT:
                        # Get the first line of the message
                        first_line = msg.content.split('\n')[0].strip()

                        # Truncate if needed
                        if len(first_line) > max_length:
                            title = first_line[:max_length - 3] + "..."
                        else:
                            title = first_line

                        return title

                # If no user messages found, use a default title
                return f"Conversation {conversation.id[:8]}"

            def update_conversation_title(self, conversation_id: str, new_title: str) -> bool:
                """
                Update the title of a conversation.

                Args:
                    conversation_id: The ID of the conversation to update
                    new_title: The new title

                Returns:
                    True if successful, False otherwise
                """
                try:
                    # Load the conversation
                    conversation = self.load_conversation(conversation_id)

                    if not conversation:
                        logger.warning(f"Cannot update title: Conversation {conversation_id} not found")
                        return False

                    # Update the title
                    conversation.title = new_title

                    # Save the conversation
                    return self.save_conversation(conversation)

                except Exception as e:
                    logger.error(f"Error updating title for conversation {conversation_id}: {e}", exc_info=True)
                    return False
      metadata:
        extension: .py
        size_bytes: 7981
        language: python
    sql/identify_large_chunks.sql:
      content: |-
        -- Identify chunks that exceed the tsvector limit
        -- Run this BEFORE applying the FTS migration to see what will be affected

        -- Show exact chunks that exceed the limit
        SELECT 
            c.id as chunk_id,
            c.file_id,
            f.name as file_name,
            c.chunk_index,
            LENGTH(c.content) as content_bytes,
            LENGTH(c.content) - 1048575 as bytes_over_limit,
            ROUND((LENGTH(c.content) / 1048575.0) * 100, 2) as percent_of_limit,
            LEFT(c.content, 200) || '...' as content_preview,
            c.chunk_metadata->>'chunk_strategy' as chunk_strategy
        FROM chunks c
        JOIN files f ON c.file_id = f.id  
        WHERE LENGTH(c.content) > 1048575
        ORDER BY LENGTH(c.content) DESC;

        -- Summary by chunking strategy (if available in metadata)
        SELECT 
            c.chunk_metadata->>'chunk_strategy' as chunk_strategy,
            COUNT(*) as total_chunks,
            COUNT(CASE WHEN LENGTH(c.content) > 1048575 THEN 1 END) as oversized_chunks,
            MAX(LENGTH(c.content)) as max_size,
            AVG(LENGTH(c.content))::INTEGER as avg_size
        FROM chunks c
        GROUP BY c.chunk_metadata->>'chunk_strategy'
        ORDER BY oversized_chunks DESC;
      metadata:
        extension: .sql
        size_bytes: 1068
        language: sql
    sql/add_fts_with_truncation.sql:
      content: |-
        -- Add full-text search support with handling for oversized chunks
        -- This script truncates content that's too large for tsvector

        -- First, add the column without the GENERATED clause
        ALTER TABLE chunks 
        ADD COLUMN IF NOT EXISTS content_tsv tsvector;

        -- Create a function to safely convert text to tsvector with truncation
        CREATE OR REPLACE FUNCTION safe_to_tsvector(input_text text) 
        RETURNS tsvector AS $$
        DECLARE
            max_length INTEGER := 1000000; -- Leave some buffer (actual max is 1048575)
            truncated_text TEXT;
        BEGIN
            -- If text is too long, truncate it
            IF LENGTH(input_text) > max_length THEN
                truncated_text := LEFT(input_text, max_length);
                -- Try to truncate at a word boundary
                truncated_text := REGEXP_REPLACE(truncated_text, '\s+\S*$', '');
                RETURN to_tsvector('english', truncated_text);
            ELSE
                RETURN to_tsvector('english', input_text);
            END IF;
        EXCEPTION
            WHEN OTHERS THEN
                -- If any error occurs, return an empty tsvector
                RETURN to_tsvector('english', '');
        END;
        $$ LANGUAGE plpgsql IMMUTABLE;

        -- Update existing chunks with the safe tsvector
        UPDATE chunks 
        SET content_tsv = safe_to_tsvector(content)
        WHERE content_tsv IS NULL;

        -- Create a trigger to automatically update content_tsv on insert/update
        CREATE OR REPLACE FUNCTION update_content_tsv() 
        RETURNS TRIGGER AS $$
        BEGIN
            NEW.content_tsv := safe_to_tsvector(NEW.content);
            RETURN NEW;
        END;
        $$ LANGUAGE plpgsql;

        -- Drop existing trigger if it exists
        DROP TRIGGER IF EXISTS chunks_content_tsv_trigger ON chunks;

        -- Create the trigger
        CREATE TRIGGER chunks_content_tsv_trigger
        BEFORE INSERT OR UPDATE OF content ON chunks
        FOR EACH ROW
        EXECUTE FUNCTION update_content_tsv();

        -- Create GIN index for efficient full-text search
        CREATE INDEX IF NOT EXISTS idx_chunks_content_tsv 
        ON chunks USING GIN (content_tsv);

        -- Show statistics about the chunks that were truncated
        SELECT 
            COUNT(*) as total_chunks,
            COUNT(CASE WHEN LENGTH(content) > 1000000 THEN 1 END) as truncated_chunks,
            MAX(LENGTH(content)) as max_content_length
        FROM chunks;

        -- Analyze table to update statistics for query planner
        ANALYZE chunks;
      metadata:
        extension: .sql
        size_bytes: 2171
        language: sql
    sql/check_chunk_sizes.sql:
      content: |-
        -- Check for chunks that are too large for tsvector
        -- PostgreSQL tsvector has a maximum size of 1048575 bytes

        -- First, let's see how many chunks exceed the limit
        SELECT COUNT(*) as total_chunks,
               COUNT(CASE WHEN LENGTH(content) > 1048575 THEN 1 END) as chunks_too_large,
               COUNT(CASE WHEN LENGTH(content) > 900000 THEN 1 END) as chunks_near_limit
        FROM chunks;

        -- Show the largest chunks with their details
        SELECT 
            c.id,
            c.chunk_index,
            f.name as file_name,
            LENGTH(c.content) as content_length,
            LEFT(c.content, 100) as content_preview,
            c.chunk_metadata
        FROM chunks c
        JOIN files f ON c.file_id = f.id
        WHERE LENGTH(c.content) > 900000
        ORDER BY LENGTH(c.content) DESC
        LIMIT 20;

        -- Group by file to see which files have oversized chunks
        SELECT 
            f.id as file_id,
            f.name as file_name,
            COUNT(*) as total_chunks,
            COUNT(CASE WHEN LENGTH(c.content) > 1048575 THEN 1 END) as oversized_chunks,
            MAX(LENGTH(c.content)) as max_chunk_size,
            AVG(LENGTH(c.content))::INTEGER as avg_chunk_size
        FROM files f
        JOIN chunks c ON f.id = c.file_id
        GROUP BY f.id, f.name
        HAVING COUNT(CASE WHEN LENGTH(c.content) > 1048575 THEN 1 END) > 0
        ORDER BY oversized_chunks DESC;
      metadata:
        extension: .sql
        size_bytes: 1203
        language: sql
