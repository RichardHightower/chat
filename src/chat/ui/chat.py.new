"""
Chat UI components for the chat application.

This module contains functions for rendering the chat UI components
including message display, input handling, and response generation.
"""

import streamlit as st
import asyncio
from typing import Dict, Optional, List, Union, Any

from chat.conversation.conversation import Conversation, MessageType
from chat.conversation.conversation_storage import ConversationStorage
from chat.ai.llm_provider import LLMProvider
from chat.util.logging_util import logger as llm_logger
from chat.ui.chat_utils import handle_message_with_streaming


def display_chat_messages(messages: List[Dict[str, str]]) -> None:
    """
    Display the chat message history.
    
    Args:
        messages: List of message dictionaries with 'role' and 'content' keys
    """
    for message in messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])


def handle_user_input(
    llm_provider: Optional[LLMProvider],
    conversation: Optional[Conversation],
    conversation_storage: ConversationStorage,
    selected_provider: str,
    selected_model: str,
    temperature: float,
    system_prompt: str = "You are a helpful and concise chat assistant designed for providing accurate and relevant information.",
    use_streaming: bool = False
) -> None:
    """
    Handle user input, generate responses, and update the conversation.
    
    Args:
        llm_provider: The initialized LLM provider
        conversation: The current conversation object
        conversation_storage: Instance of ConversationStorage
        selected_provider: Currently selected provider name
        selected_model: Currently selected model name
        temperature: Temperature setting for response generation
        system_prompt: System prompt to use for the LLM
        use_streaming: Whether to use streaming mode for response generation
    """
    # Get user input from chat input
    if prompt := st.chat_input("Your message..."):
        # Add user message to chat history and display it
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # Generate and display assistant's response
        if not llm_provider:
            with st.chat_message("assistant"):
                st.error("LLM Provider is not available. Cannot process messages.")
            if not any(m["role"] == "assistant" and "Provider is not available" in m["content"] for m in
                       st.session_state.messages):
                st.session_state.messages.append({"role": "assistant",
                                                 "content": "LLM Provider is not available due to an initialization error. Please check the logs."})
            return

        # Check if we should use streaming mode
        should_stream = (use_streaming and 
                        hasattr(llm_provider, 'generate_completion_stream') and 
                        selected_provider == "Anthropic")
        
        # Generate options for completion
        generation_options = {
            "system_prompt": system_prompt,
            "temperature": temperature,
        }
        
        # Decide whether to use conversation context
        maintain_context = st.session_state.get("maintain_context", True)
        active_conversation = conversation if maintain_context else None

        llm_logger.info(f"User prompt for {selected_provider}: {prompt}")
        
        if should_stream:
            # Use streaming mode
            llm_logger.info(f"Using streaming mode for {selected_provider}")
            
            # Directly handle streaming (which creates its own chat message)
            handle_message_with_streaming(
                provider=llm_provider,
                prompt=prompt,
                conversation=active_conversation,
                conversation_storage=conversation_storage,
                options=generation_options
            )
            
            # The streaming function handles everything, so we're done
            return

        # Non-streaming path
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("â³ Thinking...")
            
            try:
                # Use standard non-streaming approach
                full_response_content = asyncio.run(
                    llm_provider.generate_completion(
                        prompt=prompt,
                        output_format="text",
                        options=generation_options,
                        conversation=active_conversation
                    )
                )
                
                # Update UI after completion
                llm_logger.info(f"{selected_provider} response received, length: {len(full_response_content or '')}")
                
                if not full_response_content:
                    full_response_content = "I received an empty response. Could you try rephrasing?"
                    llm_logger.warning("LLM returned an empty response.")
                
                # Display the response
                message_placeholder.markdown(full_response_content)
                
                # Add assistant's response to session state
                st.session_state.messages.append({"role": "assistant", "content": full_response_content})
                
                # If not using the conversation object for context (which would have been updated automatically),
                # we need to manually add the response to the conversation
                if not maintain_context and conversation:
                    conversation.add_message(full_response_content, MessageType.OUTPUT)
                
                # Auto-save the conversation
                try:
                    if conversation_storage.save_conversation(conversation):
                        llm_logger.info(f"Auto-saved conversation {conversation.id}")
                    else:
                        llm_logger.warning(f"Failed to auto-save conversation {conversation.id}")
                except Exception as e:
                    llm_logger.error(f"Error during auto-save of conversation: {e}", exc_info=True)
                    
            except ValueError as ve:
                error_msg = f"Input/Output Error: {ve}"
                llm_logger.error(error_msg, exc_info=True)
                message_placeholder.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": f"I encountered an issue with the data format: {ve}"})
                
            except ImportError as ie:
                error_msg = f"Import error: {ie}. Ensure all dependencies for the LLM provider are installed."
                llm_logger.error(error_msg, exc_info=True)
                message_placeholder.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": "There's a configuration problem with the LLM provider."})
                
            except Exception as e:
                error_msg = f"Sorry, an unexpected error occurred: {type(e).__name__} - {e}"
                llm_logger.error(error_msg, exc_info=True)
                message_placeholder.error(error_msg)
                st.session_state.messages.append({"role": "assistant", "content": "I ran into an unexpected problem trying to respond."})


def initialize_chat_history(selected_provider: str, selected_model: str) -> None:
    """
    Initialize the chat history in session state if it doesn't exist.
    Update provider information if the provider has changed.
    
    Args:
        selected_provider: Currently selected provider name
        selected_model: Currently selected model name
    """
    # Initialize chat history if it doesn't exist
    if "messages" not in st.session_state:
        st.session_state.messages = [
            {"role": "assistant",
             "content": f"Hello! I'm using {selected_provider}'s {selected_model}. How can I help you today?"}
        ]
    # If provider changed, add a system message
    elif "current_provider" in st.session_state and st.session_state.current_provider != selected_provider:
        st.session_state.messages.append(
            {"role": "assistant",
             "content": f"I've switched to {selected_provider}'s {selected_model}. How can I help you?"}
        )
    
    # Update current provider
    st.session_state.current_provider = selected_provider